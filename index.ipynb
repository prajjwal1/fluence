{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluence\n",
    "Fluence is a deep learning library based on Pytorch for adaptive computation approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "`pip install fluence`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library contains implementation for the following approaches (many more to come):\n",
    "- [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799)\n",
    "- [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015)\n",
    "- [Reducing Transformer Depth on Demand with Structured Dropout](https://arxiv.org/abs/1909.11556)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "### Using Adaptive Attention Spans in a transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import torch\n",
    "from fluence.adaptive.adaptive_span import AdaptiveSpan\n",
    "config = {'attn_span': 1024, 'adapt_span_loss_coeff': 0.000005, 'adapt_span_ramp': 32,\n",
    "                      'adapt_span_init': 0.002, 'adapt_span_cache': True, 'nb_heads': 12,'bs': 128,\n",
    "                      'mask_size': [20,36]}\n",
    "adaptive_span = AdaptiveSpan(**config)\n",
    "adaptive_span.get_current_avg_span() # Returns average span\n",
    "adaptive_span.get_current_max_span() # Returns maximum span\n",
    "adaptive_span.get_trim_len() # Returns length that can be trimmed\n",
    "adaptive_span.clamp_param() # Clamps values of parameter to stay between [0,1]\n",
    "\n",
    "attention_scores_0 = torch.randn(128,12,26,36) # These scores come from softmax\n",
    "attention_scores_1 = torch.randn(128,12,26,20) # These scores come from softmax\n",
    "adaptive_span(attention_scores_0).shape # Soft masking function is multiplied\n",
    "adaptive_span(attention_scores_1).shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the following in __init__ of BertAttention\n",
    "1. Set it as an attribute\n",
    "    ```\n",
    "    if self.adapt_span_bool:\n",
    "        self.adaptive_span = AdaptiveSpan(**config)\n",
    "    ```\n",
    "2. Use the adapt_span_loss with the current loss function\n",
    "```\n",
    "adapt_span_loss = 0.\n",
    "for l in self.model.layer: # Should be a nn.ModuleList to iterate\n",
    "        adapt_span_loss += l.attention.adaptive_span.get_loss() #attention is the BertAttention class\n",
    "```\n",
    "3. Perform clamping\n",
    "```\n",
    "for l in self.model.layer:\n",
    "        l.attention.self.adaptive_span.clamp_param()\n",
    "```\n",
    "4. Get attention span\n",
    "```\n",
    "for layer_idx, i in enumerate(self.model.layer):\n",
    "        l = i.attention.adaptive_span.get_current_avg_span()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
