{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/prajjwal1/fluence/workflows/CI/badge.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluence\n",
    "\n",
    ">Fluence is a deep learning library based on Pytorch for attention based approaches. It's a toolkit which I use for my own research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install fluence`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library contains implementation for the following approaches (many more to come):\n",
    "- [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799)\n",
    "- [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015)\n",
    "- [Reducing Transformer Depth on Demand with Structured Dropout](https://arxiv.org/abs/1909.11556)\n",
    "- Optimizers: Lamb, Lookahead (Pytorch doesn't have it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Structure\n",
    "```\n",
    "fluence\n",
    "    - adaptive     # Implements Adaptive Modules\n",
    "    - models       # Models\n",
    "    - optimizers   # optimizers \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation \n",
    "Please head to this [link](prajjwal1.github.io/fluence) to learn how you can integrate fluence with your workflow. Since it's an early release, there might be bugs here and there. Please file an issue if you encounter one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "Right now, it consists of major adaptive computation approaches which have been tested with transformers. Fluence is easy to use. Here are some of the examples\n",
    "\n",
    "\n",
    "### Using Adaptive Attention Span\n",
    "```\n",
    "import torch\n",
    "from fluence.adaptive.adaptive_span import AdaptiveSpan\n",
    "config = {'attn_span': 1024, 'adapt_span_loss_coeff': 0.000005, 'adapt_span_ramp': 32,\n",
    "                      'adapt_span_init': 0.002, 'adapt_span_cache': True, 'nb_heads': 12,'bs': 128,\n",
    "                      'mask_size': [20,36]}\n",
    "adaptive_span = AdaptiveSpan(**config)\n",
    "adaptive_span.get_current_avg_span() # Returns average span\n",
    "adaptive_span.get_current_max_span() # Returns maximum span\n",
    "adaptive_span.get_trim_len() # Returns length that can be trimmed\n",
    "adaptive_span.clamp_param() # Clamps values of parameter to stay between [0,1]\n",
    "\n",
    "attention_scores_0 = torch.randn(128,12,26,36) # These scores come from softmax\n",
    "attention_scores_1 = torch.randn(128,12,26,20) # These scores come from softmax\n",
    "adaptive_span(attention_scores_0).shape # Soft masking function is multiplied\n",
    "adaptive_span(attention_scores_1).shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Entmax as a replacement for softmax with learnable alpha values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from fluence.adaptive.entmax import *\n",
    "num_attention_heads = 12\n",
    "entmax_alpha = EntmaxAlpha(num_attention_heads)\n",
    "attention_scores = entmax_alpha(att_scores=torch.rand(128,12,26,36)) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Layerdrop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from fluence.adaptive.layerdrop import LayerDrop\n",
    "from torch import nn\n",
    "net = nn.ModuleList([nn.Linear(2, 2) for i in range(3)])\n",
    "layers_to_drop = 2\n",
    "layerdrop = LayerDrop(net, layers_to_drop)\n",
    "output = layerdrop(torch.rand(10,2))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fluence.optimizer\n",
    "```\n",
    "from fluence.optimizers.lamb import Lamb\n",
    "from fluence.optimizers.lookahead import Lookahead\n",
    "\n",
    "model = torchvision.models.AlexNet()                        # Can be a transformer\n",
    "base_optim = Lamb(params=model.parameters(),lr=1e-5, weight_decay=1.2e-6, min_trust=0.25)\n",
    "optim = Lookahead(base_optimizer=base_optim, k=5, alpha=0.8)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgements\n",
    "- [Hugging face Transformer](https://github.com/huggingface/transformers/)\n",
    "- [Adaptive Attention Span for Transformers](https://github.com/facebookresearch/adaptive-span)\n",
    "- [entmax](https://github.com/deep-spin/entmax)\n",
    "- [LXMERT](https://github.com/airsplay/lxmert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Prajjwal Bhargava ([@prajjwal_1](https://twitter.com/prajjwal_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch7",
   "language": "python",
   "name": "torch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
