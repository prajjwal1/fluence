{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/prajjwal1/fluence/workflows/CI/badge.svg)\n",
    "[![PyPI version](https://badge.fury.io/py/fluence.svg)](https://badge.fury.io/py/fluence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluence\n",
    "\n",
    ">Fluence is a Pytorch based deep learning library focussed on providing computationally efficient, low resource methods and algorithms. Although the main focus is to provide support with transformers, it can be extended with other architectures as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install fluence`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library contains implementation for the following approaches (many more to come):\n",
    "- Adaptive Methods\n",
    "    - [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799)\n",
    "    - [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015)\n",
    "    - [Reducing Transformer Depth on Demand with Structured Dropout](https://arxiv.org/abs/1909.11556)\n",
    "\n",
    "- Optimizers: \n",
    "    - Lamb\n",
    "    - Lookahead\n",
    "    \n",
    "- Importance Sampling:\n",
    "    - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation \n",
    "Please head to this [link](prajjwal1.github.io/fluence) to learn how you can integrate fluence with your workflow. Since it's an early release, there might be bugs here and there. Please file an issue if you encounter one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Examples\n",
    "Right now, it consists of major adaptive computation approaches which have been tested with transformers. Fluence is easy to use. Here are some of the examples\n",
    "\n",
    "\n",
    "### Using Adaptive Attention Span\n",
    "```\n",
    "import torch\n",
    "from fluence.adaptive.adaptive_span import AdaptiveSpan\n",
    "config = {'attn_span': 1024, 'adapt_span_loss_coeff': 0.000005, 'adapt_span_ramp': 32,\n",
    "                      'adapt_span_init': 0.002, 'adapt_span_cache': True, 'nb_heads': 12,'bs': 128,\n",
    "                      'mask_size': [20,36]}\n",
    "adaptive_span = AdaptiveSpan(**config)\n",
    "\n",
    "attention_scores_0 = torch.randn(128,12,26,36) # These scores come from softmax\n",
    "attention_scores_1 = torch.randn(128,12,26,20) # These scores come from softmax\n",
    "adaptive_span(attention_scores_0).shape # Soft masking function is multiplied\n",
    "adaptive_span(attention_scores_1).shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Entmax as a replacement for softmax with learnable alpha values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from fluence.adaptive.entmax import *\n",
    "num_attention_heads = 12\n",
    "entmax_alpha = EntmaxAlpha(num_attention_heads)\n",
    "attention_scores = entmax_alpha(att_scores=torch.rand(128,12,26,36)) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Layerdrop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from fluence.adaptive.layerdrop import Layerdrop\n",
    "from torch import nn\n",
    "net = nn.ModuleList([nn.Linear(2, 2) for i in range(3)])\n",
    "layers_to_drop = 2\n",
    "layerdrop = Layerdrop(net, layers_to_drop)\n",
    "output = layerdrop(torch.rand(10,2))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fluence.optim\n",
    "```\n",
    "from fluence.optim.lamb import Lamb\n",
    "from fluence.optim.lookahead import Lookahead\n",
    "\n",
    "model = torchvision.models.AlexNet()                        # Can be a transformer\n",
    "base_optim = Lamb(params=model.parameters(),lr=1e-5, weight_decay=1.2e-6, min_trust=0.25)\n",
    "optim = Lookahead(base_optimizer=base_optim, k=5, alpha=0.8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook2script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Prajjwal Bhargava ([@prajjwal_1](https://twitter.com/prajjwal_1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
