{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp sampling.clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance sampling with clustering\n",
    "> Perform Importance sampling with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class Clustering_Arguments:\n",
    "    batch_size: int = field(metadata={\"help\": \"Batch size to use for MiniBatchKMeans\"})\n",
    "    num_clusters: int = field(metadata={\"help\": \"number of clusters to obtain\"})\n",
    "    embedding_path: str = field(\n",
    "        metadata={\"help\": \"Path from where embeddings will be loaded\"}\n",
    "    )\n",
    "    data_pct: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"specifies how much data will be used\"}\n",
    "    )\n",
    "    num_clusters_elements: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"specifies the number of clusters that will be used. If this\"\n",
    "                \" is enabled, `data_pct` should be set to None\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    cluster_output_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path where embedding will be stored\"}\n",
    "    )\n",
    "    cluster_only: bool = field(default=False, metadata={\"help\": \"Run only clustering\"})\n",
    "    random_state: int = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"for producing deterministic results with MiniBatchKMeans\"},\n",
    "    )\n",
    "    cluster_input_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path from there clustering labels will be loaded\"},\n",
    "    )\n",
    "    cluster_n_jobs: Optional[int] = field(\n",
    "        default=-1,\n",
    "        metadata={\"help\": \"Number of parallel processes to run for clustering\"},\n",
    "    )\n",
    "    centroid_elements_only: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Specify to use cluster centroid elements for training\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Clustering_Processor:\n",
    "    \"\"\"\n",
    "    A processor class that makes it easy to obtain indices from clusters with\n",
    "    various methods\n",
    "    \"\"\"\n",
    "\n",
    "    labels: np.array\n",
    "    data_pct: float\n",
    "    num_clusters: int\n",
    "    cluster_num: int\n",
    "\n",
    "    def __init__(self, cluster):\n",
    "        self.labels = cluster[\"labels_\"]\n",
    "        self.kmeans_cluster_centers = cluster[\"cluster_centers_\"]\n",
    "\n",
    "    def get_cluster_indices(self, cluster_num: int):\n",
    "        return np.where(self.labels == cluster_num)[0]\n",
    "\n",
    "    def get_cluster_indices_by_pct(self, data_pct: float, original_len: int) -> List:\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            data_pct: specify how many elements are required from clusters\n",
    "            original_len: length of the dataset\n",
    "        Output:\n",
    "            cluster_indices: cluster indices\n",
    "\n",
    "        This method return concatenated cluster indices whose propotion equals\n",
    "        len(dataset)*data_percentage\n",
    "        \"\"\"\n",
    "        current_len, cluster_indices = 0, []\n",
    "        for i in set(self.labels):\n",
    "            curr_cluster_indices = self.get_cluster_indices(i)\n",
    "            current_len += len(curr_cluster_indices)\n",
    "            if current_len < int(original_len * data_pct):\n",
    "                cluster_indices.extend(curr_cluster_indices)\n",
    "            else:\n",
    "                return cluster_indices\n",
    "\n",
    "    def get_cluster_indices_by_num(self, num_clusters: int) -> List:\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            num_clusters: specify how many clusters to return\n",
    "        Output:\n",
    "            cluster_indices: cluster indices\n",
    "\n",
    "        This method returns concatenated cluster indices whose propotion equals\n",
    "        to that of number of elements in specified number of cluster\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        for i in range(num_clusters):\n",
    "            indices.extend(self.get_cluster_indices(i))\n",
    "        return indices\n",
    "\n",
    "    def get_cluster_indices_from_centroid(self, embeddings: torch.tensor) -> np.array:\n",
    "        return pairwise_distances_argmin_min(self.kmeans_cluster_centers, embeddings)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Load the embeddings, if you don't have, download it from the following URL:\n",
    "# https://s3.amazonaws.com/models.huggingface.co/bert/prajjwal1/albert-base-v2-mnli/cls_embeddings_mnli.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def get_embeddings(embedding_path):\n",
    "    embeddings = torch.load(embedding_path)\n",
    "    embeddings = np.concatenate(embeddings)  # (392702, 768)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def get_clustering_obj(embeddings):\n",
    "    clustering = MiniBatchKMeans(n_clusters=512, batch_size=256,).fit(embeddings)\n",
    "    return clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def test_data_pct(clustering_obj):\n",
    "    clustering_args = Clustering_Arguments(\n",
    "        batch_size=32,\n",
    "        num_clusters=32,\n",
    "        embedding_path=\"/home/nlp/experiments/cls_embeddings_mnli.pth\",\n",
    "        data_pct=0.2,\n",
    "        cluster_output_path=\"/home/nlp/experiments/tmp/c.pth\",\n",
    "    )\n",
    "    clustering_proc = Clustering_Processor(vars(clustering_obj))\n",
    "    cluster_indices = clustering_proc.get_cluster_indices_by_pct(\n",
    "        clustering_args.data_pct, embeddings.shape[0]\n",
    "    )\n",
    "    assert len(cluster_indices) > 70000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def test_cluster_indices(clustering_obj):\n",
    "    clustering_args = Clustering_Arguments(\n",
    "        batch_size=32,\n",
    "        num_clusters_elements=32,\n",
    "        embedding_path=\"/home/nlp/experiments/cls_embeddings_mnli.pth\",\n",
    "        num_clusters=8,\n",
    "        cluster_output_path=\"/home/nlp/experiments/tmp/c.pth\",\n",
    "    )\n",
    "    clustering_proc = Clustering_Processor(vars(clustering_obj))\n",
    "    cluster_indices = clustering_proc.get_cluster_indices_by_num(\n",
    "        clustering_args.num_clusters_elements\n",
    "    )\n",
    "    assert len(cluster_indices) > 10000\n",
    "    return cluster_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def test_cluster_centroids(clustering_obj):\n",
    "    clustering_args = Clustering_Arguments(\n",
    "        batch_size=32,\n",
    "        num_clusters_elements=32,\n",
    "        embedding_path=\"/home/nlp/experiments/cls_embeddings_mnli.pth\",\n",
    "        num_clusters=8,\n",
    "        cluster_output_path=\"/home/nlp/experiments/tmp/c.pth\",\n",
    "    )\n",
    "    clustering_proc = Clustering_Processor(vars(clustering_obj))\n",
    "    cluster_indices = clustering_proc.get_cluster_indices_from_centroid(embeddings)\n",
    "    assert len(cluster_indices) == 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_torch_hf_dataset(cluster_indices):\n",
    "    from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "    from transformers import AutoTokenizer, GlueDataset\n",
    "\n",
    "    data_args = DataTrainingArguments(\n",
    "        task_name=\"MNLI\", data_dir=\"/home/nlp/data/glue_data/MNLI\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "    train_dataset = GlueDataset(data_args, tokenizer)\n",
    "    train_dataset = torch.utils.data.Subset(train_dataset, cluster_indices)\n",
    "    assert len(train_dataset[0].input_ids) == 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "embeddings = get_embeddings(\"/home/nlp/experiments/cls_embeddings_mnli.pth\")\n",
    "clustering_obj = get_clustering_obj(embeddings)\n",
    "test_data_pct(clustering_obj)\n",
    "cluster_indices = test_cluster_indices(clustering_obj)\n",
    "test_cluster_centroids(clustering_obj)\n",
    "test_torch_hf_dataset(cluster_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
