{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.lxmert.lxmert_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0219 20:27:27.257721 140477671249728 file_utils.py:35] PyTorch version 1.4.0+cpu available.\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#export \n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import tempfile\n",
    "import sys\n",
    "from io import open\n",
    "import torch\n",
    "from torch import nn\n",
    "from fluence.models.lxmert.file_utils import cached_path\n",
    "\n",
    "from transformers import BertConfig\n",
    "from transformers.modeling_bert import BertLayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lxmert_utils\n",
    "> Utitilies for lxmert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
    "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n",
    "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n",
    "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n",
    "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n",
    "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n",
    "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n",
    "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n",
    "}\n",
    "CONFIG_NAME = 'bert_config.json'\n",
    "WEIGHTS_NAME = 'pytorch_model.bin'\n",
    "TF_WEIGHTS_NAME = 'model.ckpt'\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_visual_config(params):\n",
    "    \"\"\"\n",
    "    Stores the parameters as attributes\n",
    "    \"\"\"\n",
    "    VISUAL_CONFIG.l_layers = params['layer_sizes']['lang']\n",
    "    VISUAL_CONFIG.x_layers = params['layer_sizes']['cross']\n",
    "    VISUAL_CONFIG.r_layers = params['layer_sizes']['vision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VisualConfig(object):\n",
    "    VISUAL_LOSSES = ['obj', 'attr', 'feat']\n",
    "    def __init__(self,\n",
    "                 l_layers=12,\n",
    "                 x_layers=5,\n",
    "                 r_layers=0):\n",
    "        self.l_layers = l_layers\n",
    "        self.x_layers = x_layers\n",
    "        self.r_layers = r_layers\n",
    "\n",
    "        self.visual_feat_dim = 2048\n",
    "        self.visual_pos_dim = 4\n",
    "\n",
    "        self.obj_id_num = 1600\n",
    "        self.attr_id_num = 400\n",
    "\n",
    "        self.visual_losses = self.VISUAL_LOSSES\n",
    "        self.visual_loss_config = {\n",
    "            'obj': (self.obj_id_num, 'ce', (-1,), 1/0.15),\n",
    "            'attr': (self.attr_id_num, 'ce', (-1,), 1/0.15),\n",
    "            'feat': (2048, 'l2', (-1, 2048), 1/0.15),\n",
    "        }\n",
    "\n",
    "    def set_visual_dims(self, feat_dim, pos_dim):\n",
    "        self.visual_feat_dim = feat_dim\n",
    "        self.visual_pos_dim = pos_dim\n",
    "        \n",
    "VISUAL_CONFIG = VisualConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InputFeatures(object):\n",
    "    \"\"\"\n",
    "    A class to store features of data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convert_sents_to_features(sents, max_seq_length, tokenizer):\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of `InputBatch`s.\n",
    "    \"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (i, sent) in enumerate(sents):\n",
    "        tokens_a = tokenizer.tokenize(sent.strip())\n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "        \n",
    "        # Keep segment id which allows loading BERT-weights.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BertPreTrainedModel(nn.Module):\n",
    "    \"\"\" An abstract class to handle weights initialization and\n",
    "        a simple interface for dowloading and loading pretrained models.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(BertPreTrainedModel, self).__init__()\n",
    "        if not isinstance(config, BertConfig):\n",
    "            raise ValueError(\n",
    "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
    "                \"To create a model from a Google pretrained model use \"\n",
    "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
    "                    self.__class__.__name__, self.__class__.__name__\n",
    "                ))\n",
    "        self.config = config\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None,\n",
    "                        from_tf=False,*inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n",
    "        Download and cache the pre-trained model file if needed.\n",
    "\n",
    "        Params:\n",
    "            pretrained_model_name_or_path: either:\n",
    "                - a str with the name of a pre-trained model to load selected in the list of:\n",
    "                    . `bert-base-uncased`\n",
    "                    . `bert-large-uncased`\n",
    "                    . `bert-base-cased`\n",
    "                    . `bert-large-cased`\n",
    "                    . `bert-base-multilingual-uncased`\n",
    "                    . `bert-base-multilingual-cased`\n",
    "                    . `bert-base-chinese`\n",
    "                - a path or url to a pretrained model archive containing:\n",
    "                    . `bert_config.json` a configuration file for the model\n",
    "                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n",
    "                - a path or url to a pretrained model archive containing:\n",
    "                    . `bert_config.json` a configuration file for the model\n",
    "                    . `model.chkpt` a TensorFlow checkpoint\n",
    "            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n",
    "            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n",
    "            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n",
    "            *inputs, **kwargs: additional input for the specific Bert class\n",
    "                (ex: num_labels for BertForSequenceClassification)\n",
    "        \"\"\"\n",
    "        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
    "            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n",
    "        else:\n",
    "            archive_file = pretrained_model_name_or_path\n",
    "        # redirect to the cache, if necessary\n",
    "        try:\n",
    "            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n",
    "        except EnvironmentError:\n",
    "            if pretrained_model_name_or_path == 'bert-base-uncased':\n",
    "                try:\n",
    "                    print(\"The BERT-weight-downloading query to AWS was time-out;\" \n",
    "                          \"trying to download from UNC servers\")\n",
    "                    archive_file = \"https://nlp.cs.unc.edu/data/bert/bert-base-uncased.tar.gz\"\n",
    "                    resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n",
    "                except EnvironmentError:\n",
    "                    print(\"The weight-downloading still crashed with link: %s, \"\n",
    "                          \"please check your network connection\" % archive_file)\n",
    "                    return None\n",
    "            else:\n",
    "                logger.error(\n",
    "                        \"Model name '{}' was not found in model name list ({}). \"\n",
    "                        \"We assumed '{}' was a path or url but couldn't find any file \"\n",
    "                        \"associated to this path or url.\".format(\n",
    "                            pretrained_model_name_or_path,\n",
    "                            ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n",
    "                            archive_file))\n",
    "        if resolved_archive_file == archive_file:\n",
    "            logger.info(\"loading archive file {}\".format(archive_file))\n",
    "        else:\n",
    "            logger.info(\"loading archive file {} from cache at {}\".format(\n",
    "                archive_file, resolved_archive_file))\n",
    "        tempdir = None\n",
    "        if os.path.isdir(resolved_archive_file) or from_tf:\n",
    "            serialization_dir = resolved_archive_file\n",
    "        else:\n",
    "            # Extract archive to temp dir\n",
    "            tempdir = tempfile.mkdtemp()\n",
    "            logger.info(\"extracting archive file {} to temp dir {}\".format(\n",
    "                resolved_archive_file, tempdir))\n",
    "            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
    "                archive.extractall(tempdir)\n",
    "            serialization_dir = tempdir\n",
    "        # Load config\n",
    "        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
    "        config = BertConfig.from_json_file(config_file)\n",
    "        logger.info(\"Model config {}\".format(config))\n",
    "        # Instantiate model.\n",
    "        model = cls(config, *inputs, **kwargs)\n",
    "        if state_dict is None and not from_tf:\n",
    "            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n",
    "            state_dict = torch.load(weights_path, map_location='cpu' if not torch.cuda.is_available() else None)\n",
    "        if tempdir:\n",
    "            # Clean up temp dir\n",
    "            shutil.rmtree(tempdir)\n",
    "        if from_tf:\n",
    "            # Directly load from a TensorFlow checkpoint\n",
    "            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\n",
    "            return load_tf_weights_in_bert(model, weights_path)\n",
    "        # Load from a PyTorch state_dict\n",
    "        old_keys = []\n",
    "        new_keys = []\n",
    "        for key in state_dict.keys():\n",
    "            new_key = None\n",
    "            if 'gamma' in key:\n",
    "                new_key = key.replace('gamma', 'weight')\n",
    "            if 'beta' in key:\n",
    "                new_key = key.replace('beta', 'bias')\n",
    "            if new_key:\n",
    "                old_keys.append(key)\n",
    "                new_keys.append(new_key)\n",
    "        for old_key, new_key in zip(old_keys, new_keys):\n",
    "            state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        missing_keys = []\n",
    "        unexpected_keys = []\n",
    "        error_msgs = []\n",
    "        # copy state_dict so _load_from_state_dict can modify it\n",
    "        metadata = getattr(state_dict, '_metadata', None)\n",
    "        state_dict = state_dict.copy()\n",
    "        if metadata is not None:\n",
    "            state_dict._metadata = metadata\n",
    "\n",
    "        def load(module, prefix=''):\n",
    "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "            module._load_from_state_dict(\n",
    "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "            for name, child in module._modules.items():\n",
    "                if child is not None:\n",
    "                    load(child, prefix + name + '.')\n",
    "        start_prefix = ''\n",
    "        if not hasattr(model, 'bert') and any(s.startswith('bert.') for s in state_dict.keys()):\n",
    "            start_prefix = 'bert.'\n",
    "        load(model, prefix=start_prefix)\n",
    "\n",
    "        if len(error_msgs) > 0:\n",
    "            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "                               model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "cwd = os.getcwd()+'/pretrain/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AnswerTable:\n",
    "    ANS_CONVERT = {\n",
    "        \"a man\": \"man\",\n",
    "        \"the man\": \"man\",\n",
    "        \"a woman\": \"woman\",\n",
    "        \"the woman\": \"woman\",\n",
    "        'one': '1',\n",
    "        'two': '2',\n",
    "        'three': '3',\n",
    "        'four': '4',\n",
    "        'five': '5',\n",
    "        'six': '6',\n",
    "        'seven': '7',\n",
    "        'eight': '8',\n",
    "        'nine': '9',\n",
    "        'ten': '10',\n",
    "        'grey': 'gray',\n",
    "    }\n",
    "\n",
    "    def __init__(self, dsets=None):\n",
    "        self.all_ans = json.load(open(cwd+\"all_ans.json\"))\n",
    "        if dsets is not None:\n",
    "            dsets = set(dsets)\n",
    "            # If the answer is used in the dsets\n",
    "            self.anss = [ans['ans'] for ans in self.all_ans if\n",
    "                         len(set(ans['dsets']) & dsets) > 0]\n",
    "        else:\n",
    "            self.anss = [ans['ans'] for ans in self.all_ans]\n",
    "        self.ans_set = set(self.anss)\n",
    "\n",
    "        self._id2ans_map = self.anss\n",
    "        self._ans2id_map = {ans: ans_id for ans_id, ans in enumerate(self.anss)}\n",
    "\n",
    "        assert len(self._id2ans_map) == len(self._ans2id_map)\n",
    "        for ans_id, ans in enumerate(self._id2ans_map):\n",
    "            assert self._ans2id_map[ans] == ans_id\n",
    "\n",
    "    def convert_ans(self, ans):\n",
    "        if len(ans) == 0:\n",
    "            return \"\"\n",
    "        ans = ans.lower()\n",
    "        if ans[-1] == '.':\n",
    "            ans = ans[:-1].strip()\n",
    "        if ans.startswith(\"a \"):\n",
    "            ans = ans[2:].strip()\n",
    "        if ans.startswith(\"an \"):\n",
    "            ans = ans[3:].strip()\n",
    "        if ans.startswith(\"the \"):\n",
    "            ans = ans[4:].strip()\n",
    "        if ans in self.ANS_CONVERT:\n",
    "            ans = self.ANS_CONVERT[ans]\n",
    "        return ans\n",
    "\n",
    "    def ans2id(self, ans):\n",
    "        return self._ans2id_map[ans]\n",
    "\n",
    "    def id2ans(self, ans_id):\n",
    "        return self._id2ans_map[ans_id]\n",
    "\n",
    "    def ans2id_map(self):\n",
    "        return self._ans2id_map.copy()\n",
    "\n",
    "    def id2ans_map(self):\n",
    "        return self._id2ans_map.copy()\n",
    "\n",
    "    def used(self, ans):\n",
    "        return ans in self.ans_set\n",
    "\n",
    "    def all_answers(self):\n",
    "        return self.anss.copy()\n",
    "\n",
    "    @property\n",
    "    def num_answers(self):\n",
    "        return len(self.anss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_lxmert_qa(path, model, label2ans):\n",
    "    \"\"\"\n",
    "    Load model weights from LXMERT pre-training.\n",
    "    The answers in the fine-tuned QA task (indicated by label2ans)\n",
    "        would also be properly initialized with LXMERT pre-trained\n",
    "        QA heads.\n",
    "\n",
    "    :param path: Path to LXMERT snapshot.\n",
    "    :param model: LXRT model instance.\n",
    "    :param label2ans: The label2ans dict of fine-tuned QA datasets, like\n",
    "        {0: 'cat', 1: 'dog', ...}\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"Load QA pre-trained LXMERT from %s \" % path)\n",
    "    loaded_state_dict = torch.load(\"%s_LXRT.pth\" % path, map_location=lambda storage, loc: storage)\n",
    "    model_state_dict = model.state_dict()\n",
    "\n",
    "    # Handle Multi-GPU pre-training --> Single GPU fine-tuning\n",
    "    for key in list(loaded_state_dict.keys()):\n",
    "        loaded_state_dict[key.replace(\"module.\", '')] = loaded_state_dict.pop(key)\n",
    "\n",
    "    # Isolate bert model\n",
    "    bert_state_dict = {}\n",
    "    for key, value in loaded_state_dict.items():\n",
    "        if key.startswith('bert.'):\n",
    "            bert_state_dict[key] = value\n",
    "\n",
    "    # Isolate answer head\n",
    "    answer_state_dict = {}\n",
    "    for key, value in loaded_state_dict.items():\n",
    "        if key.startswith(\"answer_head.\"):\n",
    "            answer_state_dict[key.replace('answer_head.', '')] = value\n",
    "\n",
    "    # Do surgery on answer state dict\n",
    "    ans_weight = answer_state_dict['logit_fc.3.weight']\n",
    "    ans_bias = answer_state_dict['logit_fc.3.bias']\n",
    "    import copy\n",
    "    new_answer_weight = copy.deepcopy(model_state_dict['logit_fc.3.weight'])\n",
    "    new_answer_bias = copy.deepcopy(model_state_dict['logit_fc.3.bias'])\n",
    "    answer_table = AnswerTable()\n",
    "    loaded = 0\n",
    "    unload = 0\n",
    "    if type(label2ans) is list:\n",
    "        label2ans = {label: ans for label, ans in enumerate(label2ans)}\n",
    "    for label, ans in label2ans.items():\n",
    "        new_ans = answer_table.convert_ans(ans)\n",
    "        if answer_table.used(new_ans):\n",
    "            ans_id_9500 = answer_table.ans2id(new_ans)\n",
    "            new_answer_weight[label] = ans_weight[ans_id_9500]\n",
    "            new_answer_bias[label] = ans_bias[ans_id_9500]\n",
    "            loaded += 1\n",
    "        else:\n",
    "            new_answer_weight[label] = 0.\n",
    "            new_answer_bias[label] = 0.\n",
    "            unload += 1\n",
    "    print(\"Loaded %d answers from LXRTQA pre-training and %d not\" % (loaded, unload))\n",
    "    print()\n",
    "    answer_state_dict['logit_fc.3.weight'] = new_answer_weight\n",
    "    answer_state_dict['logit_fc.3.bias'] = new_answer_bias\n",
    "\n",
    "    # Load Bert Weights\n",
    "    bert_model_keys = set(model.lxrt_encoder.model.state_dict().keys())\n",
    "    bert_loaded_keys = set(bert_state_dict.keys())\n",
    "    if len(bert_model_keys - bert_loaded_keys) != 0:\n",
    "        print(\"Keys don't exactly match\")\n",
    "    model.lxrt_encoder.model.load_state_dict(bert_state_dict, strict=False)\n",
    "\n",
    "    # Load Answer Logic FC Weights\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    ans_loaded_keys = set(answer_state_dict.keys())\n",
    "    assert len(ans_loaded_keys - model_keys) == 0\n",
    "\n",
    "    model.load_state_dict(answer_state_dict, strict=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
