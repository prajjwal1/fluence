{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp adaptive.adaptive_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext pycodestyle_magic\\n%pycodestyle_on\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext pycodestyle_magic\\n%pycodestyle_on\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# export\\nimport math\\nimport torch\\nimport torch.nn as nn\";\n",
       "                var nbb_formatted_code = \"# export\\nimport math\\nimport torch\\nimport torch.nn as nn\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fluence.adaptive.adaptive_span\n",
    "> Implements Adaptive Attention Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\nclass AdaptiveSpan(nn.Module):\\n    \\\"\\\"\\\"\\n    Implements `Adaptive Attention Span in Transformers`\\n                [Paper](https://arxiv.org/abs/1905.07799)\\n\\n    Arguments:\\n        attn_span (int): specifies the maximum attention span\\n        adapt_span_loss_coeff (float): regulates the initial value of\\n                                        adapt_span_loss\\n        adapt_span_ramp (int): offset value\\n        adapt_span_init (float): initial additive value for the\\n                                 main parameter\\n        adapt_span_cache (bool): determines working of caching\\n        nb_heads (int): number of attention heads\\n        bs (int): batch size\\n        mask_size (list): a list containing last dimension of possible\\n                        attention scores\\n\\n    Example::\\n        >>> config = {'attn_span': 1024,\\n                     'adapt_span_loss_coeff': 0.000005, 'adapt_span_ramp': 32,\\n                     'adapt_span_init': 0.002, 'adapt_span_cache': True,\\n                     'nb_heads': 12,'bs': 128, 'mask_size': [20,36]}\\n        >>> adaptive_span = AdaptiveSpan(**config)\\n        >>> adaptive_span(torch.randn(128,12,26,36)).shape\\n        >>> adaptive_span(torch.randn(128,12,26,20)).shape\\n        >>> adaptive_span.get_current_avg_span()\\n        >>> adaptive_span.get_current_max_span()\\n        >>> adaptive_span.get_trim_len()\\n        >>> adaptive_span.clamp_param()\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        attn_span,\\n        adapt_span_loss_coeff,\\n        adapt_span_ramp,\\n        adapt_span_init,\\n        adapt_span_cache,\\n        nb_heads,\\n        bs,\\n        mask_size,\\n    ):\\n\\n        super(AdaptiveSpan, self).__init__()\\n        self.attn_span = attn_span  # [attn_span]\\n        self.ramp_size = adapt_span_ramp\\n        self.bs = bs\\n        self.nb_heads = nb_heads\\n        self.init_val = adapt_span_init\\n        self.adapt_cache = adapt_span_cache\\n        self.loss_coeff = adapt_span_loss_coeff\\n        self.shape = (self.bs, self.nb_heads, 1, 1)\\n\\n        self.current_val = nn.Parameter(\\n            torch.nn.init.kaiming_normal_(torch.empty(*self.shape)) +\\n            self.init_val\\n        )  # [bs,nb_heads,1,1]\\n        self.mask_size = mask_size\\n\\n        mask_template_0 = torch.linspace(\\n            1 - self.mask_size[0], 0, steps=self.mask_size[0]\\n        )  # [attn_span]\\n        self.register_buffer(\\\"mask_template_0\\\", mask_template_0)\\n\\n        if len(self.mask_size) > 1:\\n            mask_template_1 = torch.linspace(\\n                1 - self.mask_size[1], 0, steps=self.mask_size[1]\\n            )\\n            self.register_buffer(\\\"mask_template_1\\\", mask_template_1)\\n\\n    def mask_forward(self, x):\\n        \\\"\\\"\\\"\\n        Computes the mask and performs the multiplication operation\\n        with attention weights\\n        \\\"\\\"\\\"\\n        mask_size = x.size(3)\\n        if mask_size == self.mask_size[0]:\\n            mask = self.mask_template_0 + self.current_val * mask_size\\n        else:\\n            mask = self.mask_template_1 + self.current_val * mask_size\\n        mask = mask / self.ramp_size + 1\\n        mask = mask.clamp(0, 1)\\n        if x.size(0) == mask.size(0):\\n            x = x * mask  # [bs, nb_heads, 36, 64]) [bs, nb_heads, 1, 64]\\n            return x\\n        else:\\n            return x\\n\\n    def get_current_avg_span(self, include_ramp=True):\\n        \\\"\\\"\\\"\\n        Outputs average span\\n        \\\"\\\"\\\"\\n        current_size = math.ceil(self.current_val.mean().item() *\\n                                 self.attn_span)\\n        if include_ramp:\\n            current_size += self.ramp_size\\n        current_size = max(0, min(self.attn_span, current_size))\\n        return current_size\\n\\n    def get_current_max_span(self, include_ramp=True):\\n        \\\"\\\"\\\"\\n        Determines maximum span\\n        \\\"\\\"\\\"\\n        current_size = math.ceil(self.current_val.max().item() *\\n                                 self.attn_span)\\n        if include_ramp:\\n            current_size += self.ramp_size\\n        current_size = max(0, min(self.attn_span, current_size))\\n        return current_size\\n\\n    def clamp_param(self):\\n        \\\"\\\"\\\"\\n        Clamps the values of parameter to stay between 0 and 1\\n        \\\"\\\"\\\"\\n        self.current_val.data.clamp_(0, 1)\\n\\n    def get_trim_len(self):\\n        \\\"\\\"\\\"\\n        Outputs length to be trimmed\\n        \\\"\\\"\\\"\\n        L = self.attn_span\\n        trim_len = min(L - 1, L - self.get_current_max_span())\\n        trim_len = math.floor(trim_len / 64) * 64\\n        return trim_len\\n\\n    def get_cache_size(self):\\n        \\\"\\\"\\\"\\n        Determine how long the cache should be\\n        \\\"\\\"\\\"\\n        if self.adapt_cache:\\n            trim_len = self.get_trim_len()\\n            return min(self.attn_span, self.attn_span - trim_len + 64)\\n        else:\\n            return self.attn_span\\n\\n    def get_loss(self):\\n        \\\"\\\"\\\"\\n        A loss term for regularizing the span length\\n        \\\"\\\"\\\"\\n        return self.loss_coeff * self.attn_span * self.current_val.mean()\\n\\n    def forward(self, attn):\\n        attn = self.mask_forward(attn)\\n        attn = attn / (attn.sum(-1, keepdim=True) + 1e-8)\\n        return attn\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\nclass AdaptiveSpan(nn.Module):\\n    \\\"\\\"\\\"\\n    Implements `Adaptive Attention Span in Transformers`\\n                [Paper](https://arxiv.org/abs/1905.07799)\\n\\n    Arguments:\\n        attn_span (int): specifies the maximum attention span\\n        adapt_span_loss_coeff (float): regulates the initial value of\\n                                        adapt_span_loss\\n        adapt_span_ramp (int): offset value\\n        adapt_span_init (float): initial additive value for the\\n                                 main parameter\\n        adapt_span_cache (bool): determines working of caching\\n        nb_heads (int): number of attention heads\\n        bs (int): batch size\\n        mask_size (list): a list containing last dimension of possible\\n                        attention scores\\n\\n    Example::\\n        >>> config = {'attn_span': 1024,\\n                     'adapt_span_loss_coeff': 0.000005, 'adapt_span_ramp': 32,\\n                     'adapt_span_init': 0.002, 'adapt_span_cache': True,\\n                     'nb_heads': 12,'bs': 128, 'mask_size': [20,36]}\\n        >>> adaptive_span = AdaptiveSpan(**config)\\n        >>> adaptive_span(torch.randn(128,12,26,36)).shape\\n        >>> adaptive_span(torch.randn(128,12,26,20)).shape\\n        >>> adaptive_span.get_current_avg_span()\\n        >>> adaptive_span.get_current_max_span()\\n        >>> adaptive_span.get_trim_len()\\n        >>> adaptive_span.clamp_param()\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        attn_span,\\n        adapt_span_loss_coeff,\\n        adapt_span_ramp,\\n        adapt_span_init,\\n        adapt_span_cache,\\n        nb_heads,\\n        bs,\\n        mask_size,\\n    ):\\n\\n        super(AdaptiveSpan, self).__init__()\\n        self.attn_span = attn_span  # [attn_span]\\n        self.ramp_size = adapt_span_ramp\\n        self.bs = bs\\n        self.nb_heads = nb_heads\\n        self.init_val = adapt_span_init\\n        self.adapt_cache = adapt_span_cache\\n        self.loss_coeff = adapt_span_loss_coeff\\n        self.shape = (self.bs, self.nb_heads, 1, 1)\\n\\n        self.current_val = nn.Parameter(\\n            torch.nn.init.kaiming_normal_(torch.empty(*self.shape)) + self.init_val\\n        )  # [bs,nb_heads,1,1]\\n        self.mask_size = mask_size\\n\\n        mask_template_0 = torch.linspace(\\n            1 - self.mask_size[0], 0, steps=self.mask_size[0]\\n        )  # [attn_span]\\n        self.register_buffer(\\\"mask_template_0\\\", mask_template_0)\\n\\n        if len(self.mask_size) > 1:\\n            mask_template_1 = torch.linspace(\\n                1 - self.mask_size[1], 0, steps=self.mask_size[1]\\n            )\\n            self.register_buffer(\\\"mask_template_1\\\", mask_template_1)\\n\\n    def mask_forward(self, x):\\n        \\\"\\\"\\\"\\n        Computes the mask and performs the multiplication operation\\n        with attention weights\\n        \\\"\\\"\\\"\\n        mask_size = x.size(3)\\n        if mask_size == self.mask_size[0]:\\n            mask = self.mask_template_0 + self.current_val * mask_size\\n        else:\\n            mask = self.mask_template_1 + self.current_val * mask_size\\n        mask = mask / self.ramp_size + 1\\n        mask = mask.clamp(0, 1)\\n        if x.size(0) == mask.size(0):\\n            x = x * mask  # [bs, nb_heads, 36, 64]) [bs, nb_heads, 1, 64]\\n            return x\\n        else:\\n            return x\\n\\n    def get_current_avg_span(self, include_ramp=True):\\n        \\\"\\\"\\\"\\n        Outputs average span\\n        \\\"\\\"\\\"\\n        current_size = math.ceil(self.current_val.mean().item() * self.attn_span)\\n        if include_ramp:\\n            current_size += self.ramp_size\\n        current_size = max(0, min(self.attn_span, current_size))\\n        return current_size\\n\\n    def get_current_max_span(self, include_ramp=True):\\n        \\\"\\\"\\\"\\n        Determines maximum span\\n        \\\"\\\"\\\"\\n        current_size = math.ceil(self.current_val.max().item() * self.attn_span)\\n        if include_ramp:\\n            current_size += self.ramp_size\\n        current_size = max(0, min(self.attn_span, current_size))\\n        return current_size\\n\\n    def clamp_param(self):\\n        \\\"\\\"\\\"\\n        Clamps the values of parameter to stay between 0 and 1\\n        \\\"\\\"\\\"\\n        self.current_val.data.clamp_(0, 1)\\n\\n    def get_trim_len(self):\\n        \\\"\\\"\\\"\\n        Outputs length to be trimmed\\n        \\\"\\\"\\\"\\n        L = self.attn_span\\n        trim_len = min(L - 1, L - self.get_current_max_span())\\n        trim_len = math.floor(trim_len / 64) * 64\\n        return trim_len\\n\\n    def get_cache_size(self):\\n        \\\"\\\"\\\"\\n        Determine how long the cache should be\\n        \\\"\\\"\\\"\\n        if self.adapt_cache:\\n            trim_len = self.get_trim_len()\\n            return min(self.attn_span, self.attn_span - trim_len + 64)\\n        else:\\n            return self.attn_span\\n\\n    def get_loss(self):\\n        \\\"\\\"\\\"\\n        A loss term for regularizing the span length\\n        \\\"\\\"\\\"\\n        return self.loss_coeff * self.attn_span * self.current_val.mean()\\n\\n    def forward(self, attn):\\n        attn = self.mask_forward(attn)\\n        attn = attn / (attn.sum(-1, keepdim=True) + 1e-8)\\n        return attn\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class AdaptiveSpan(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements `Adaptive Attention Span in Transformers`\n",
    "                [Paper](https://arxiv.org/abs/1905.07799)\n",
    "\n",
    "    Arguments:\n",
    "        attn_span (int): specifies the maximum attention span\n",
    "        adapt_span_loss_coeff (float): regulates the initial value of\n",
    "                                        adapt_span_loss\n",
    "        adapt_span_ramp (int): offset value\n",
    "        adapt_span_init (float): initial additive value for the\n",
    "                                 main parameter\n",
    "        adapt_span_cache (bool): determines working of caching\n",
    "        nb_heads (int): number of attention heads\n",
    "        bs (int): batch size\n",
    "        mask_size (list): a list containing last dimension of possible\n",
    "                        attention scores\n",
    "\n",
    "    Example::\n",
    "        >>> config = {'attn_span': 1024,\n",
    "                     'adapt_span_loss_coeff': 0.000005, 'adapt_span_ramp': 32,\n",
    "                     'adapt_span_init': 0.002, 'adapt_span_cache': True,\n",
    "                     'nb_heads': 12,'bs': 128, 'mask_size': [20,36]}\n",
    "        >>> adaptive_span = AdaptiveSpan(**config)\n",
    "        >>> adaptive_span(torch.randn(128,12,26,36)).shape\n",
    "        >>> adaptive_span(torch.randn(128,12,26,20)).shape\n",
    "        >>> adaptive_span.get_current_avg_span()\n",
    "        >>> adaptive_span.get_current_max_span()\n",
    "        >>> adaptive_span.get_trim_len()\n",
    "        >>> adaptive_span.clamp_param()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_span,\n",
    "        adapt_span_loss_coeff,\n",
    "        adapt_span_ramp,\n",
    "        adapt_span_init,\n",
    "        adapt_span_cache,\n",
    "        nb_heads,\n",
    "        bs,\n",
    "        mask_size,\n",
    "    ):\n",
    "\n",
    "        super(AdaptiveSpan, self).__init__()\n",
    "        self.attn_span = attn_span  # [attn_span]\n",
    "        self.ramp_size = adapt_span_ramp\n",
    "        self.bs = bs\n",
    "        self.nb_heads = nb_heads\n",
    "        self.init_val = adapt_span_init\n",
    "        self.adapt_cache = adapt_span_cache\n",
    "        self.loss_coeff = adapt_span_loss_coeff\n",
    "        self.shape = (self.bs, self.nb_heads, 1, 1)\n",
    "\n",
    "        self.current_val = nn.Parameter(\n",
    "            torch.nn.init.kaiming_normal_(torch.empty(*self.shape)) +\n",
    "            self.init_val\n",
    "        )  # [bs,nb_heads,1,1]\n",
    "        self.mask_size = mask_size\n",
    "\n",
    "        mask_template_0 = torch.linspace(\n",
    "            1 - self.mask_size[0], 0, steps=self.mask_size[0]\n",
    "        )  # [attn_span]\n",
    "        self.register_buffer(\"mask_template_0\", mask_template_0)\n",
    "\n",
    "        if len(self.mask_size) > 1:\n",
    "            mask_template_1 = torch.linspace(\n",
    "                1 - self.mask_size[1], 0, steps=self.mask_size[1]\n",
    "            )\n",
    "            self.register_buffer(\"mask_template_1\", mask_template_1)\n",
    "\n",
    "    def mask_forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes the mask and performs the multiplication operation\n",
    "        with attention weights\n",
    "        \"\"\"\n",
    "        mask_size = x.size(3)\n",
    "        if mask_size == self.mask_size[0]:\n",
    "            mask = self.mask_template_0 + self.current_val * mask_size\n",
    "        else:\n",
    "            mask = self.mask_template_1 + self.current_val * mask_size\n",
    "        mask = mask / self.ramp_size + 1\n",
    "        mask = mask.clamp(0, 1)\n",
    "        if x.size(0) == mask.size(0):\n",
    "            x = x * mask  # [bs, nb_heads, 36, 64]) [bs, nb_heads, 1, 64]\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def get_current_avg_span(self, include_ramp=True):\n",
    "        \"\"\"\n",
    "        Outputs average span\n",
    "        \"\"\"\n",
    "        current_size = math.ceil(self.current_val.mean().item() *\n",
    "                                 self.attn_span)\n",
    "        if include_ramp:\n",
    "            current_size += self.ramp_size\n",
    "        current_size = max(0, min(self.attn_span, current_size))\n",
    "        return current_size\n",
    "\n",
    "    def get_current_max_span(self, include_ramp=True):\n",
    "        \"\"\"\n",
    "        Determines maximum span\n",
    "        \"\"\"\n",
    "        current_size = math.ceil(self.current_val.max().item() *\n",
    "                                 self.attn_span)\n",
    "        if include_ramp:\n",
    "            current_size += self.ramp_size\n",
    "        current_size = max(0, min(self.attn_span, current_size))\n",
    "        return current_size\n",
    "\n",
    "    def clamp_param(self):\n",
    "        \"\"\"\n",
    "        Clamps the values of parameter to stay between 0 and 1\n",
    "        \"\"\"\n",
    "        self.current_val.data.clamp_(0, 1)\n",
    "\n",
    "    def get_trim_len(self):\n",
    "        \"\"\"\n",
    "        Outputs length to be trimmed\n",
    "        \"\"\"\n",
    "        L = self.attn_span\n",
    "        trim_len = min(L - 1, L - self.get_current_max_span())\n",
    "        trim_len = math.floor(trim_len / 64) * 64\n",
    "        return trim_len\n",
    "\n",
    "    def get_cache_size(self):\n",
    "        \"\"\"\n",
    "        Determine how long the cache should be\n",
    "        \"\"\"\n",
    "        if self.adapt_cache:\n",
    "            trim_len = self.get_trim_len()\n",
    "            return min(self.attn_span, self.attn_span - trim_len + 64)\n",
    "        else:\n",
    "            return self.attn_span\n",
    "\n",
    "    def get_loss(self):\n",
    "        \"\"\"\n",
    "        A loss term for regularizing the span length\n",
    "        \"\"\"\n",
    "        return self.loss_coeff * self.attn_span * self.current_val.mean()\n",
    "\n",
    "    def forward(self, attn):\n",
    "        attn = self.mask_forward(attn)\n",
    "        attn = attn / (attn.sum(-1, keepdim=True) + 1e-8)\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"config = {\\n    \\\"attn_span\\\": 1024,\\n    \\\"adapt_span_loss_coeff\\\": 0.000005,\\n    \\\"adapt_span_ramp\\\": 32,\\n    \\\"adapt_span_init\\\": 0.002,\\n    \\\"adapt_span_cache\\\": True,\\n    \\\"nb_heads\\\": 12,\\n    \\\"bs\\\": 128,\\n    \\\"mask_size\\\": [20, 36],\\n}\";\n",
       "                var nbb_formatted_code = \"config = {\\n    \\\"attn_span\\\": 1024,\\n    \\\"adapt_span_loss_coeff\\\": 0.000005,\\n    \\\"adapt_span_ramp\\\": 32,\\n    \\\"adapt_span_init\\\": 0.002,\\n    \\\"adapt_span_cache\\\": True,\\n    \\\"nb_heads\\\": 12,\\n    \\\"bs\\\": 128,\\n    \\\"mask_size\\\": [20, 36],\\n}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    \"attn_span\": 1024,\n",
    "    \"adapt_span_loss_coeff\": 0.000005,\n",
    "    \"adapt_span_ramp\": 32,\n",
    "    \"adapt_span_init\": 0.002,\n",
    "    \"adapt_span_cache\": True,\n",
    "    \"nb_heads\": 12,\n",
    "    \"bs\": 128,\n",
    "    \"mask_size\": [20, 36],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "1024\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"adaptive_span = AdaptiveSpan(**config)\\nprint(adaptive_span.get_current_avg_span())\\nprint(adaptive_span.get_current_max_span())\\nadaptive_span.clamp_param()\\nprint(adaptive_span.get_trim_len())\";\n",
       "                var nbb_formatted_code = \"adaptive_span = AdaptiveSpan(**config)\\nprint(adaptive_span.get_current_avg_span())\\nprint(adaptive_span.get_current_max_span())\\nadaptive_span.clamp_param()\\nprint(adaptive_span.get_trim_len())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adaptive_span = AdaptiveSpan(**config)\n",
    "print(adaptive_span.get_current_avg_span())\n",
    "print(adaptive_span.get_current_max_span())\n",
    "adaptive_span.clamp_param()\n",
    "print(adaptive_span.get_trim_len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12, 26, 36])\n",
      "torch.Size([128, 12, 26, 20])\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"print(adaptive_span(torch.randn(128, 12, 26, 36)).shape)\\nprint(adaptive_span(torch.randn(128, 12, 26, 20)).shape)\";\n",
       "                var nbb_formatted_code = \"print(adaptive_span(torch.randn(128, 12, 26, 36)).shape)\\nprint(adaptive_span(torch.randn(128, 12, 26, 20)).shape)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(adaptive_span(torch.randn(128, 12, 26, 36)).shape)\n",
    "print(adaptive_span(torch.randn(128, 12, 26, 20)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch7",
   "language": "python",
   "name": "torch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
