{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp adaptive.adaptive_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fluence.adaptive.adaptive_span\n",
    "> Implements Adaptive Attention Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "class AdaptiveSpan(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements `Adaptive Attention Span in Transformers` [Paper](https://arxiv.org/abs/1905.07799)\n",
    "\n",
    "    Arguments:\n",
    "        attn_span (int): specifies the maximum attention span\n",
    "        adapt_span_loss_coeff (float): regulates the initial value of adapt_span_loss\n",
    "        adapt_span_ramp (int): offset value\n",
    "        adapt_span_init (float): initial additive value for the\n",
    "                                 main parameter\n",
    "        adapt_span_cache (bool): determines working of caching\n",
    "        nb_heads (int): number of attention heads\n",
    "        bs (int): batch size\n",
    "        mask_size (list): a list containing last dimension of possible attention scores\n",
    "\n",
    "    Example::\n",
    "        >>> config = {'attn_span': 1024, 'adapt_span_loss_coeff': 0.000005, \n",
    "                      'adapt_span_ramp': 32, 'adapt_span_init': 0.002, 'adapt_span_cache': True,\n",
    "                      'nb_heads': 12,'bs': 128, 'mask_size': [20,36]}\n",
    "        >>> adaptive_span = AdaptiveSpan(**config)\n",
    "        >>> adaptive_span(torch.randn(128,12,26,36)).shape\n",
    "        >>> adaptive_span(torch.randn(128,12,26,20)).shape\n",
    "        >>> adaptive_span.get_current_avg_span()\n",
    "        >>> adaptive_span.get_current_max_span()\n",
    "        >>> adaptive_span.get_trim_len()\n",
    "        >>> adaptive_span.clamp_param()\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_span, adapt_span_loss_coeff, adapt_span_ramp, adapt_span_init,\n",
    "                 adapt_span_cache, nb_heads, bs, mask_size):\n",
    "\n",
    "        super(AdaptiveSpan, self).__init__()\n",
    "        self.attn_span = attn_span    # [attn_span]\n",
    "        self.ramp_size = adapt_span_ramp\n",
    "        self.bs = bs\n",
    "        self.nb_heads = nb_heads\n",
    "        self.init_val = adapt_span_init\n",
    "        self.adapt_cache = adapt_span_cache\n",
    "        self.loss_coeff = adapt_span_loss_coeff\n",
    "        self.shape = (self.bs, self.nb_heads, 1, 1)\n",
    "\n",
    "        self.current_val = nn.Parameter(torch.nn.init.kaiming_normal_(torch.empty(*self.shape)) + self.init_val)  # [bs,nb_heads,1,1]\n",
    "        self.mask_size = mask_size\n",
    "\n",
    "        mask_template_0 = torch.linspace(1 - self.mask_size[0], 0, steps=self.mask_size[0]) # [attn_span]\n",
    "        self.register_buffer('mask_template_0', mask_template_0)\n",
    "\n",
    "        if len(self.mask_size)>1:\n",
    "            mask_template_1 = torch.linspace(1 - self.mask_size[1], 0, steps=self.mask_size[1])\n",
    "            self.register_buffer('mask_template_1', mask_template_1)\n",
    "\n",
    "    def mask_forward(self,x):\n",
    "        \"\"\"\n",
    "        Computes the mask and performs the multiplication operation with attention weights\n",
    "        \"\"\"\n",
    "        mask_size = x.size(3)\n",
    "        if mask_size==self.mask_size[0]:\n",
    "            mask = self.mask_template_0 + self.current_val*mask_size\n",
    "        else:\n",
    "            mask = self.mask_template_1 + self.current_val*mask_size\n",
    "        mask = mask / self.ramp_size + 1\n",
    "        mask = mask.clamp(0, 1)\n",
    "        if x.size(0)==mask.size(0):\n",
    "            x = x * mask   # [bs, nb_heads, 36, 64]) [bs, nb_heads, 1, 64]\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def get_current_avg_span(self,include_ramp=True):\n",
    "        \"\"\"\n",
    "        Outputs average span\n",
    "        \"\"\"\n",
    "        current_size = math.ceil(self.current_val.mean().item() * self.attn_span)\n",
    "        if include_ramp:\n",
    "            current_size += self.ramp_size\n",
    "        current_size = max(0, min(self.attn_span, current_size))\n",
    "        return current_size\n",
    "\n",
    "    def get_current_max_span(self,include_ramp=True):\n",
    "        \"\"\"\n",
    "        Determines maximum span\n",
    "        \"\"\"\n",
    "        current_size = math.ceil(self.current_val.max().item() * self.attn_span)\n",
    "        if include_ramp:\n",
    "            current_size += self.ramp_size\n",
    "        current_size = max(0, min(self.attn_span, current_size))\n",
    "        return current_size\n",
    "\n",
    "    def clamp_param(self):\n",
    "        \"\"\"\n",
    "        Clamps the values of parameter to stay between 0 and 1\n",
    "        \"\"\"\n",
    "        self.current_val.data.clamp_(0, 1)\n",
    "\n",
    "    def get_trim_len(self):\n",
    "        \"\"\"\n",
    "        Outputs length to be trimmed\n",
    "        \"\"\"\n",
    "        L = self.attn_span\n",
    "        trim_len = min(L - 1, L - self.get_current_max_span())\n",
    "        trim_len = math.floor(trim_len / 64) * 64\n",
    "        return trim_len\n",
    "\n",
    "    def get_cache_size(self):\n",
    "        \"\"\"\n",
    "        Determine how long the cache should be\n",
    "        \"\"\"\n",
    "        if self.adapt_cache:\n",
    "            trim_len = self.get_trim_len()\n",
    "            return min(self.attn_span, self.attn_span - trim_len + 64)\n",
    "        else:\n",
    "            return self.attn_span\n",
    "\n",
    "    def get_loss(self):\n",
    "        \"\"\"\n",
    "        A loss term for regularizing the span length\n",
    "        \"\"\"\n",
    "        return self.loss_coeff * self.attn_span * self.current_val.mean()\n",
    "\n",
    "    def forward(self,attn):\n",
    "        attn = self.mask_forward(attn)\n",
    "        attn = attn/(attn.sum(-1, keepdim=True)+ 1e-8)\n",
    "        return attn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "config = {'attn_span': 1024, 'adapt_span_loss_coeff': 0.000005, \n",
    "          'adapt_span_ramp': 32, 'adapt_span_init': 0.002, 'adapt_span_cache': True, 'nb_heads': 12,\n",
    "          'bs': 128, 'mask_size': [20,36]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "1024\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "adaptive_span = AdaptiveSpan(**config)\n",
    "print(adaptive_span.get_current_avg_span())\n",
    "print(adaptive_span.get_current_max_span())\n",
    "adaptive_span.clamp_param()\n",
    "print(adaptive_span.get_trim_len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 12, 26, 36]), torch.Size([128, 12, 26, 20]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "adaptive_span(torch.randn(128,12,26,36)).shape, adaptive_span(torch.randn(128,12,26,20)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import torch\n",
    "from fluence.adaptive.adaptive_span import AdaptiveSpan\n",
    "config = {'attn_span': 1024, 'adapt_span_loss_coeff': 0.000005, 'adapt_span_ramp': 32,\n",
    "                      'adapt_span_init': 0.002, 'adapt_span_cache': True, 'nb_heads': 12,'bs': 128,\n",
    "                      'mask_size': [20,36]}\n",
    "adaptive_span = AdaptiveSpan(**config)\n",
    "adaptive_span.get_current_avg_span() # Returns average span\n",
    "adaptive_span.get_current_max_span() # Returns maximum span\n",
    "adaptive_span.get_trim_len() # Returns length that can be trimmed\n",
    "adaptive_span.clamp_param() # Clamps values of parameter to stay between [0,1]\n",
    "\n",
    "attention_scores_0 = torch.randn(128,12,26,36) # These scores come from softmax\n",
    "attention_scores_1 = torch.randn(128,12,26,20) # These scores come from softmax\n",
    "adaptive_span(attention_scores_0).shape # Soft masking function is multiplied\n",
    "adaptive_span(attention_scores_1).shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the following in __init__ of BertAttention\n",
    "1. Set it as an attribute\n",
    "    ```\n",
    "    if self.adapt_span_bool:\n",
    "        self.adaptive_span = AdaptiveSpan(**config)\n",
    "    ```\n",
    "2. Use the adapt_span_loss with the current loss function\n",
    "```\n",
    "adapt_span_loss = 0.\n",
    "for l in self.model.layer: # Should be a nn.ModuleList to iterate\n",
    "        adapt_span_loss += l.attention.adaptive_span.get_loss() #attention is the BertAttention class\n",
    "```\n",
    "3. Perform clamping\n",
    "```\n",
    "for l in self.model.layer:\n",
    "        l.attention.self.adaptive_span.clamp_param()\n",
    "```\n",
    "4. Get attention span\n",
    "```\n",
    "for layer_idx, i in enumerate(self.model.layer):\n",
    "        l = i.attention.adaptive_span.get_current_avg_span()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
