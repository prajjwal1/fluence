{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pycodestyle_magic extension is already loaded. To reload it, use:\n",
      "  %reload_ext pycodestyle_magic\n",
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# default_exp optimizers.lookahead\\n%load_ext pycodestyle_magic\\n%pycodestyle_on\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# default_exp optimizers.lookahead\\n%load_ext pycodestyle_magic\\n%pycodestyle_on\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp optimizers.lookahead\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# export\\nimport torch\\nfrom torch.optim.optimizer import Optimizer\\nfrom collections import defaultdict\";\n",
       "                var nbb_formatted_code = \"# export\\nimport torch\\nfrom torch.optim.optimizer import Optimizer\\nfrom collections import defaultdict\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fluence.optimizers.lookahead\n",
    "> Implements Lookahead optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:1: E302 expected 2 blank lines, found 0\n",
      "78:1: W391 blank line at end of file\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# export\\nclass Lookahead(Optimizer):\\n    def __init__(self, base_optimizer, alpha=0.5, k=6):\\n        if not 0.0 <= alpha <= 1.0:\\n            raise ValueError(f\\\"Slow update rate is not valid: {alpha}\\\")\\n        if not 1 <= k:\\n            raise ValueError(f\\\"Invalid lookahead steps: {k}\\\")\\n        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\\n        self.base_optimizer = base_optimizer\\n        self.param_groups = self.base_optimizer.param_groups\\n        self.defaults = base_optimizer.defaults\\n        self.defaults.update(defaults)\\n        self.state = defaultdict(dict)\\n        for name, default in defaults.items():\\n            for group in self.param_groups:\\n                group.setdefault(name, default)\\n\\n    def update_slow(self, group):\\n        for fast_p in group[\\\"params\\\"]:\\n            if fast_p.grad is None:\\n                continue\\n            param_state = self.state[fast_p]\\n            if \\\"slow_buffer\\\" not in param_state:\\n                param_state[\\\"slow_buffer\\\"] = torch.empty_like(fast_p.data)\\n                param_state[\\\"slow_buffer\\\"].copy_(fast_p.data)\\n            slow = param_state[\\\"slow_buffer\\\"]\\n            slow.add_(group[\\\"lookahead_alpha\\\"], fast_p.data - slow)\\n            fast_p.data.copy_(slow)\\n\\n    def sync_lookahead(self):\\n        for group in self.param_groups:\\n            self.update_slow(group)\\n\\n    def step(self, closure=None):\\n        loss = self.base_optimizer.step(closure)\\n        for group in self.param_groups:\\n            group[\\\"lookahead_step\\\"] += 1\\n            if group[\\\"lookahead_step\\\"] % group[\\\"lookahead_k\\\"] == 0:\\n                self.update_slow(group)\\n        return loss\\n\\n    def state_dict(self):\\n        fast_state_dict = self.base_optimizer.state_dict()\\n        slow_state = {\\n            (id(k) if isinstance(k, torch.Tensor) else k): v\\n            for k, v in self.state.items()\\n        }\\n        fast_state = fast_state_dict[\\\"state\\\"]\\n        param_groups = fast_state_dict[\\\"param_groups\\\"]\\n        return {\\n            \\\"state\\\": fast_state,\\n            \\\"slow_state\\\": slow_state,\\n            \\\"param_groups\\\": param_groups,\\n        }\\n\\n    def load_state_dict(self, state_dict):\\n        fast_state_dict = {\\n            \\\"state\\\": state_dict[\\\"state\\\"],\\n            \\\"param_groups\\\": state_dict[\\\"param_groups\\\"],\\n        }\\n        self.base_optimizer.load_state_dict(fast_state_dict)\\n        slow_state_new = False\\n        if \\\"slow_state\\\" not in state_dict:\\n            print(\\\"Loading state_dict from optimizer without \\\\\\n                     Lookahead applied.\\\")\\n            state_dict[\\\"slow_state\\\"] = defaultdict(dict)\\n            slow_state_new = True\\n        slow_state_dict = {\\n            \\\"state\\\": state_dict[\\\"slow_state\\\"],\\n            \\\"param_groups\\\": state_dict[\\\"param_groups\\\"],\\n        }\\n        super(Lookahead, self).load_state_dict(slow_state_dict)\\n        self.param_groups = self.base_optimizer.param_groups\\n        if slow_state_new:\\n            for name, default in self.defaults.items():\\n                for group in self.param_groups:\\n                    group.setdefault(name, default)\";\n",
       "                var nbb_formatted_code = \"# export\\nclass Lookahead(Optimizer):\\n    def __init__(self, base_optimizer, alpha=0.5, k=6):\\n        if not 0.0 <= alpha <= 1.0:\\n            raise ValueError(f\\\"Slow update rate is not valid: {alpha}\\\")\\n        if not 1 <= k:\\n            raise ValueError(f\\\"Invalid lookahead steps: {k}\\\")\\n        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\\n        self.base_optimizer = base_optimizer\\n        self.param_groups = self.base_optimizer.param_groups\\n        self.defaults = base_optimizer.defaults\\n        self.defaults.update(defaults)\\n        self.state = defaultdict(dict)\\n        for name, default in defaults.items():\\n            for group in self.param_groups:\\n                group.setdefault(name, default)\\n\\n    def update_slow(self, group):\\n        for fast_p in group[\\\"params\\\"]:\\n            if fast_p.grad is None:\\n                continue\\n            param_state = self.state[fast_p]\\n            if \\\"slow_buffer\\\" not in param_state:\\n                param_state[\\\"slow_buffer\\\"] = torch.empty_like(fast_p.data)\\n                param_state[\\\"slow_buffer\\\"].copy_(fast_p.data)\\n            slow = param_state[\\\"slow_buffer\\\"]\\n            slow.add_(group[\\\"lookahead_alpha\\\"], fast_p.data - slow)\\n            fast_p.data.copy_(slow)\\n\\n    def sync_lookahead(self):\\n        for group in self.param_groups:\\n            self.update_slow(group)\\n\\n    def step(self, closure=None):\\n        loss = self.base_optimizer.step(closure)\\n        for group in self.param_groups:\\n            group[\\\"lookahead_step\\\"] += 1\\n            if group[\\\"lookahead_step\\\"] % group[\\\"lookahead_k\\\"] == 0:\\n                self.update_slow(group)\\n        return loss\\n\\n    def state_dict(self):\\n        fast_state_dict = self.base_optimizer.state_dict()\\n        slow_state = {\\n            (id(k) if isinstance(k, torch.Tensor) else k): v\\n            for k, v in self.state.items()\\n        }\\n        fast_state = fast_state_dict[\\\"state\\\"]\\n        param_groups = fast_state_dict[\\\"param_groups\\\"]\\n        return {\\n            \\\"state\\\": fast_state,\\n            \\\"slow_state\\\": slow_state,\\n            \\\"param_groups\\\": param_groups,\\n        }\\n\\n    def load_state_dict(self, state_dict):\\n        fast_state_dict = {\\n            \\\"state\\\": state_dict[\\\"state\\\"],\\n            \\\"param_groups\\\": state_dict[\\\"param_groups\\\"],\\n        }\\n        self.base_optimizer.load_state_dict(fast_state_dict)\\n        slow_state_new = False\\n        if \\\"slow_state\\\" not in state_dict:\\n            print(\\n                \\\"Loading state_dict from optimizer without \\\\\\n                     Lookahead applied.\\\"\\n            )\\n            state_dict[\\\"slow_state\\\"] = defaultdict(dict)\\n            slow_state_new = True\\n        slow_state_dict = {\\n            \\\"state\\\": state_dict[\\\"slow_state\\\"],\\n            \\\"param_groups\\\": state_dict[\\\"param_groups\\\"],\\n        }\\n        super(Lookahead, self).load_state_dict(slow_state_dict)\\n        self.param_groups = self.base_optimizer.param_groups\\n        if slow_state_new:\\n            for name, default in self.defaults.items():\\n                for group in self.param_groups:\\n                    group.setdefault(name, default)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:1: E302 expected 2 blank lines, found 0\n",
      "78:1: W391 blank line at end of file\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, base_optimizer, alpha=0.5, k=6):\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f\"Slow update rate is not valid: {alpha}\")\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f\"Invalid lookahead steps: {k}\")\n",
    "        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults = base_optimizer.defaults\n",
    "        self.defaults.update(defaults)\n",
    "        self.state = defaultdict(dict)\n",
    "        for name, default in defaults.items():\n",
    "            for group in self.param_groups:\n",
    "                group.setdefault(name, default)\n",
    "\n",
    "    def update_slow(self, group):\n",
    "        for fast_p in group[\"params\"]:\n",
    "            if fast_p.grad is None:\n",
    "                continue\n",
    "            param_state = self.state[fast_p]\n",
    "            if \"slow_buffer\" not in param_state:\n",
    "                param_state[\"slow_buffer\"] = torch.empty_like(fast_p.data)\n",
    "                param_state[\"slow_buffer\"].copy_(fast_p.data)\n",
    "            slow = param_state[\"slow_buffer\"]\n",
    "            slow.add_(group[\"lookahead_alpha\"], fast_p.data - slow)\n",
    "            fast_p.data.copy_(slow)\n",
    "\n",
    "    def sync_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update_slow(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.base_optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            group[\"lookahead_step\"] += 1\n",
    "            if group[\"lookahead_step\"] % group[\"lookahead_k\"] == 0:\n",
    "                self.update_slow(group)\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.base_optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        self.base_optimizer.load_state_dict(fast_state_dict)\n",
    "        slow_state_new = False\n",
    "        if \"slow_state\" not in state_dict:\n",
    "            print(\"Loading state_dict from optimizer without \\\n",
    "                     Lookahead applied.\")\n",
    "            state_dict[\"slow_state\"] = defaultdict(dict)\n",
    "            slow_state_new = True\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        if slow_state_new:\n",
    "            for name, default in self.defaults.items():\n",
    "                for group in self.param_groups:\n",
    "                    group.setdefault(name, default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch7",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
