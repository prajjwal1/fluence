{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp optim.lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lookahead Optimizer\n",
    "> Implements Lookahead optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, base_optimizer, alpha=0.5, k=6):\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f\"Slow update rate is not valid: {alpha}\")\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f\"Invalid lookahead steps: {k}\")\n",
    "        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults = base_optimizer.defaults\n",
    "        self.defaults.update(defaults)\n",
    "        self.state = defaultdict(dict)\n",
    "        for name, default in defaults.items():\n",
    "            for group in self.param_groups:\n",
    "                group.setdefault(name, default)\n",
    "\n",
    "    def update_slow(self, group):\n",
    "        for fast_p in group[\"params\"]:\n",
    "            if fast_p.grad is None:\n",
    "                continue\n",
    "            param_state = self.state[fast_p]\n",
    "            if \"slow_buffer\" not in param_state:\n",
    "                param_state[\"slow_buffer\"] = torch.empty_like(fast_p.data)\n",
    "                param_state[\"slow_buffer\"].copy_(fast_p.data)\n",
    "            slow = param_state[\"slow_buffer\"]\n",
    "            slow.add_(group[\"lookahead_alpha\"], fast_p.data - slow)\n",
    "            fast_p.data.copy_(slow)\n",
    "\n",
    "    def sync_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update_slow(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.base_optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            group[\"lookahead_step\"] += 1\n",
    "            if group[\"lookahead_step\"] % group[\"lookahead_k\"] == 0:\n",
    "                self.update_slow(group)\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.base_optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        self.base_optimizer.load_state_dict(fast_state_dict)\n",
    "        slow_state_new = False\n",
    "        if \"slow_state\" not in state_dict:\n",
    "            print(\n",
    "                \"Loading state_dict from optimizer without                     \"\n",
    "                \" Lookahead applied.\"\n",
    "            )\n",
    "            state_dict[\"slow_state\"] = defaultdict(dict)\n",
    "            slow_state_new = True\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        if slow_state_new:\n",
    "            for name, default in self.defaults.items():\n",
    "                for group in self.param_groups:\n",
    "                    group.setdefault(name, default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def test_lookahead():\n",
    "    import torchvision\n",
    "\n",
    "    model = torchvision.models.AlexNet()\n",
    "    base_optim = torch.optim.Adam(model.parameters())\n",
    "    optim = Lookahead(base_optim, k=5, alpha=0.8)\n",
    "    output = model(torch.rand(128, 3, 64, 64))\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "test_lookahead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
