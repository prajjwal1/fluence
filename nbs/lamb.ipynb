{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp optim.lamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext pycodestyle_magic\\n%pycodestyle_on\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext pycodestyle_magic\\n%pycodestyle_on\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# export\\nimport torch\\nfrom torch.optim import Optimizer\";\n",
       "                var nbb_formatted_code = \"# export\\nimport torch\\nfrom torch.optim import Optimizer\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import torch\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fluence.optim.lamb\n",
    "> Implements Lamb optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\nclass Lamb(Optimizer):\\n    r\\\"\\\"\\\"Implements Lamb algorithm.\\n    It has been proposed in `Large Batch Optimization for Deep Learning:\\n    Training BERT in 76 minutes`.\\n    Arguments:\\n        params (iterable): iterable of parameters to optimize or dicts defining\\n            parameter groups\\n        lr (float, optional): learning rate (default: 1e-3)\\n        betas (Tuple[float, float], optional): coefficients used for computing\\n            running averages of gradient and its square (default: (0.9, 0.999))\\n        eps (float, optional): term added to the denominator to improve\\n            numerical stability (default: 1e-8)\\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\\n        adam (bool, optional): always use trust ratio = 1,\\n        which turns this into Adam. Useful for comparison purposes.\\n    .. _Large Batch Optimization for Deep Learning: Training BERT in\\n        76 minutes:\\n        https://arxiv.org/abs/1904.00962\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        params,\\n        lr=1e-3,\\n        betas=(0.9, 0.999),\\n        eps=1e-6,\\n        weight_decay=0,\\n        adam=False,\\n        min_trust=None,\\n    ):\\n        if not 0.0 <= lr:\\n            raise ValueError(\\\"Invalid learning rate: {}\\\".format(lr))\\n        if not 0.0 <= eps:\\n            raise ValueError(\\\"Invalid epsilon value: {}\\\".format(eps))\\n        if not 0.0 <= betas[0] < 1.0:\\n            raise ValueError(\\\"Invalid beta parameter at \\\\\\n                              index 0: {}\\\".format(betas[0]))\\n        if not 0.0 <= betas[1] < 1.0:\\n            raise ValueError(\\\"Invalid beta parameter at \\\\\\n                              index 1: {}\\\".format(betas[1]))\\n        if min_trust and not 0.0 <= min_trust < 1.0:\\n            raise ValueError(\\\"Minimum trust range from 0 to \\\\\\n                              1: {}\\\".format(min_trust))\\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\\n        self.adam = adam\\n        self.min_trust = min_trust\\n        super(Lamb, self).__init__(params, defaults)\\n\\n    def step(self, closure=None):\\n        \\\"\\\"\\\"Performs a single optimization step.\\n        Arguments:\\n            closure (callable, optional): A closure that reevaluates the model\\n                and returns the loss.\\n        \\\"\\\"\\\"\\n        loss = None\\n        if closure is not None:\\n            loss = closure()\\n\\n        for group in self.param_groups:\\n            for p in group[\\\"params\\\"]:\\n                if p.grad is None:\\n                    continue\\n                grad = p.grad.data\\n                if grad.is_sparse:\\n                    raise RuntimeError(\\n                        \\\"Lamb does not support sparse gradients, \\\\\\n                         consider SparseAdam instad.\\\"\\n                    )\\n\\n                state = self.state[p]\\n\\n                # State initialization\\n                if len(state) == 0:\\n                    state[\\\"step\\\"] = 0\\n                    # Exponential moving average of gradient values\\n                    state[\\\"exp_avg\\\"] = torch.zeros_like(p.data)\\n                    # Exponential moving average of squared gradient values\\n                    state[\\\"exp_avg_sq\\\"] = torch.zeros_like(p.data)\\n\\n                exp_avg, exp_avg_sq = state[\\\"exp_avg\\\"], state[\\\"exp_avg_sq\\\"]\\n                beta1, beta2 = group[\\\"betas\\\"]\\n\\n                state[\\\"step\\\"] += 1\\n\\n                # Decay the first and second moment running average coefficient\\n                # m_t\\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\\n                # v_t\\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\\n                step_size = group[\\\"lr\\\"]\\n\\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\\n\\n                adam_step = exp_avg / exp_avg_sq.sqrt().add(group[\\\"eps\\\"])\\n                if group[\\\"weight_decay\\\"] != 0:\\n                    adam_step.add_(group[\\\"weight_decay\\\"], p.data)\\n\\n                adam_norm = adam_step.pow(2).sum().sqrt()\\n                if weight_norm == 0 or adam_norm == 0:\\n                    trust_ratio = 1\\n                else:\\n                    trust_ratio = weight_norm / adam_norm\\n                if self.min_trust:\\n                    trust_ratio = max(trust_ratio, self.min_trust)\\n                state[\\\"weight_norm\\\"] = weight_norm\\n                state[\\\"adam_norm\\\"] = adam_norm\\n                state[\\\"trust_ratio\\\"] = trust_ratio\\n                if self.adam:\\n                    trust_ratio = 1\\n\\n                p.data.add_(-step_size * trust_ratio, adam_step)\\n\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\nclass Lamb(Optimizer):\\n    r\\\"\\\"\\\"Implements Lamb algorithm.\\n    It has been proposed in `Large Batch Optimization for Deep Learning:\\n    Training BERT in 76 minutes`.\\n    Arguments:\\n        params (iterable): iterable of parameters to optimize or dicts defining\\n            parameter groups\\n        lr (float, optional): learning rate (default: 1e-3)\\n        betas (Tuple[float, float], optional): coefficients used for computing\\n            running averages of gradient and its square (default: (0.9, 0.999))\\n        eps (float, optional): term added to the denominator to improve\\n            numerical stability (default: 1e-8)\\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\\n        adam (bool, optional): always use trust ratio = 1,\\n        which turns this into Adam. Useful for comparison purposes.\\n    .. _Large Batch Optimization for Deep Learning: Training BERT in\\n        76 minutes:\\n        https://arxiv.org/abs/1904.00962\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        params,\\n        lr=1e-3,\\n        betas=(0.9, 0.999),\\n        eps=1e-6,\\n        weight_decay=0,\\n        adam=False,\\n        min_trust=None,\\n    ):\\n        if not 0.0 <= lr:\\n            raise ValueError(\\\"Invalid learning rate: {}\\\".format(lr))\\n        if not 0.0 <= eps:\\n            raise ValueError(\\\"Invalid epsilon value: {}\\\".format(eps))\\n        if not 0.0 <= betas[0] < 1.0:\\n            raise ValueError(\\n                \\\"Invalid beta parameter at \\\\\\n                              index 0: {}\\\".format(\\n                    betas[0]\\n                )\\n            )\\n        if not 0.0 <= betas[1] < 1.0:\\n            raise ValueError(\\n                \\\"Invalid beta parameter at \\\\\\n                              index 1: {}\\\".format(\\n                    betas[1]\\n                )\\n            )\\n        if min_trust and not 0.0 <= min_trust < 1.0:\\n            raise ValueError(\\n                \\\"Minimum trust range from 0 to \\\\\\n                              1: {}\\\".format(\\n                    min_trust\\n                )\\n            )\\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\\n        self.adam = adam\\n        self.min_trust = min_trust\\n        super(Lamb, self).__init__(params, defaults)\\n\\n    def step(self, closure=None):\\n        \\\"\\\"\\\"Performs a single optimization step.\\n        Arguments:\\n            closure (callable, optional): A closure that reevaluates the model\\n                and returns the loss.\\n        \\\"\\\"\\\"\\n        loss = None\\n        if closure is not None:\\n            loss = closure()\\n\\n        for group in self.param_groups:\\n            for p in group[\\\"params\\\"]:\\n                if p.grad is None:\\n                    continue\\n                grad = p.grad.data\\n                if grad.is_sparse:\\n                    raise RuntimeError(\\n                        \\\"Lamb does not support sparse gradients, \\\\\\n                         consider SparseAdam instad.\\\"\\n                    )\\n\\n                state = self.state[p]\\n\\n                # State initialization\\n                if len(state) == 0:\\n                    state[\\\"step\\\"] = 0\\n                    # Exponential moving average of gradient values\\n                    state[\\\"exp_avg\\\"] = torch.zeros_like(p.data)\\n                    # Exponential moving average of squared gradient values\\n                    state[\\\"exp_avg_sq\\\"] = torch.zeros_like(p.data)\\n\\n                exp_avg, exp_avg_sq = state[\\\"exp_avg\\\"], state[\\\"exp_avg_sq\\\"]\\n                beta1, beta2 = group[\\\"betas\\\"]\\n\\n                state[\\\"step\\\"] += 1\\n\\n                # Decay the first and second moment running average coefficient\\n                # m_t\\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\\n                # v_t\\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\\n                step_size = group[\\\"lr\\\"]\\n\\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\\n\\n                adam_step = exp_avg / exp_avg_sq.sqrt().add(group[\\\"eps\\\"])\\n                if group[\\\"weight_decay\\\"] != 0:\\n                    adam_step.add_(group[\\\"weight_decay\\\"], p.data)\\n\\n                adam_norm = adam_step.pow(2).sum().sqrt()\\n                if weight_norm == 0 or adam_norm == 0:\\n                    trust_ratio = 1\\n                else:\\n                    trust_ratio = weight_norm / adam_norm\\n                if self.min_trust:\\n                    trust_ratio = max(trust_ratio, self.min_trust)\\n                state[\\\"weight_norm\\\"] = weight_norm\\n                state[\\\"adam_norm\\\"] = adam_norm\\n                state[\\\"trust_ratio\\\"] = trust_ratio\\n                if self.adam:\\n                    trust_ratio = 1\\n\\n                p.data.add_(-step_size * trust_ratio, adam_step)\\n\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Lamb(Optimizer):\n",
    "    r\"\"\"Implements Lamb algorithm.\n",
    "    It has been proposed in `Large Batch Optimization for Deep Learning:\n",
    "    Training BERT in 76 minutes`.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        adam (bool, optional): always use trust ratio = 1,\n",
    "        which turns this into Adam. Useful for comparison purposes.\n",
    "    .. _Large Batch Optimization for Deep Learning: Training BERT in\n",
    "        76 minutes:\n",
    "        https://arxiv.org/abs/1904.00962\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-6,\n",
    "        weight_decay=0,\n",
    "        adam=False,\n",
    "        min_trust=None,\n",
    "    ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at \\\n",
    "                              index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at \\\n",
    "                              index 1: {}\".format(betas[1]))\n",
    "        if min_trust and not 0.0 <= min_trust < 1.0:\n",
    "            raise ValueError(\"Minimum trust range from 0 to \\\n",
    "                              1: {}\".format(min_trust))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.adam = adam\n",
    "        self.min_trust = min_trust\n",
    "        super(Lamb, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        \"Lamb does not support sparse gradients, \\\n",
    "                         consider SparseAdam instad.\"\n",
    "                    )\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # m_t\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                # v_t\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                step_size = group[\"lr\"]\n",
    "\n",
    "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
    "\n",
    "                adam_step = exp_avg / exp_avg_sq.sqrt().add(group[\"eps\"])\n",
    "                if group[\"weight_decay\"] != 0:\n",
    "                    adam_step.add_(group[\"weight_decay\"], p.data)\n",
    "\n",
    "                adam_norm = adam_step.pow(2).sum().sqrt()\n",
    "                if weight_norm == 0 or adam_norm == 0:\n",
    "                    trust_ratio = 1\n",
    "                else:\n",
    "                    trust_ratio = weight_norm / adam_norm\n",
    "                if self.min_trust:\n",
    "                    trust_ratio = max(trust_ratio, self.min_trust)\n",
    "                state[\"weight_norm\"] = weight_norm\n",
    "                state[\"adam_norm\"] = adam_norm\n",
    "                state[\"trust_ratio\"] = trust_ratio\n",
    "                if self.adam:\n",
    "                    trust_ratio = 1\n",
    "\n",
    "                p.data.add_(-step_size * trust_ratio, adam_step)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"import torchvision\\nmodel = torchvision.models.AlexNet()\\noptim = Lamb(model.parameters())\\noutput = model(torch.rand(128, 3, 64, 64))\\noptim.step()\";\n",
       "                var nbb_formatted_code = \"import torchvision\\n\\nmodel = torchvision.models.AlexNet()\\noptim = Lamb(model.parameters())\\noutput = model(torch.rand(128, 3, 64, 64))\\noptim.step()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.AlexNet()\n",
    "optim = Lamb(model.parameters())\n",
    "output = model(torch.rand(128, 3, 64, 64))\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch7",
   "language": "python",
   "name": "torch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
