{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp adaptive.entmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext pycodestyle_magic\\n%pycodestyle_on\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext pycodestyle_magic\\n%pycodestyle_on\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "#%load_ext pycodestyle_magic\n",
    "#%pycodestyle_on\n",
    "#%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# export\\n# Implementation of Entmax Bisect functions has been adapted from\\n# https://github.com/deep-spin/entmax/\\nimport torch\\nfrom torch import nn\\nfrom torch.autograd import Function\";\n",
       "                var nbb_formatted_code = \"# export\\n# Implementation of Entmax Bisect functions has been adapted from\\n# https://github.com/deep-spin/entmax/\\nimport torch\\nfrom torch import nn\\nfrom torch.autograd import Function\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "# Implementation of Entmax Bisect functions has been adapted from\n",
    "# https://github.com/deep-spin/entmax/\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fluence.adaptive.entmax\n",
    "> Entmax as a replacement for softmax to get sparse attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\nclass AlphaChooser(torch.nn.Module):\\n\\n    def __init__(self, head_count):\\n        super(AlphaChooser, self).__init__()\\n        self.pre_alpha = nn.Parameter(torch.randn(head_count))\\n\\n    def forward(self):\\n        alpha = 1 + torch.sigmoid(self.pre_alpha)\\n        return torch.clamp(alpha, min=1.01, max=2)\\n\\n\\nclass EntmaxAlpha(nn.Module):\\n\\n    def __init__(self, head_count, dim=0):\\n        super(EntmaxAlpha, self).__init__()\\n        self.dim = dim\\n        self.alpha_chooser = nn.Parameter(AlphaChooser(head_count)())\\n        self.alpha = self.alpha_chooser\\n\\n    def forward(self, att_scores):\\n        batch_size, head_count, query_len, key_len = att_scores.size()\\n\\n        expanded_alpha = self.alpha.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\\n        # [1,nb_heads,1,1]\\n        expanded_alpha = expanded_alpha.expand((batch_size, -1, query_len, 1))\\n        # [bs, nb_heads, query_len,1]\\n        p_star = entmax_bisect(att_scores, expanded_alpha)\\n        return p_star\\n\\n\\nclass EntmaxBisectFunction(Function):\\n    @classmethod\\n    def _gp(cls, x, alpha):\\n        return x ** (alpha - 1)\\n\\n    @classmethod\\n    def _gp_inv(cls, y, alpha):\\n        return y ** (1 / (alpha - 1))\\n\\n    @classmethod\\n    def _p(cls, X, alpha):\\n        return cls._gp_inv(torch.clamp(X, min=0), alpha)\\n\\n    @classmethod\\n    def forward(cls,\\n                ctx,\\n                X,\\n                alpha=1.5,\\n                dim=-1,\\n                n_iter=50,\\n                ensure_sum_one=True):\\n\\n        if not isinstance(alpha, torch.Tensor):\\n            alpha = torch.tensor(alpha, dtype=X.dtype, device=X.device)\\n\\n        alpha_shape = list(X.shape)\\n        alpha_shape[dim] = 1\\n        alpha = alpha.expand(*alpha_shape)\\n\\n        ctx.alpha = alpha\\n        ctx.dim = dim\\n        d = X.shape[dim]\\n\\n        X = X * (alpha - 1)\\n\\n        max_val, _ = X.max(dim=dim, keepdim=True)\\n\\n        tau_lo = max_val - cls._gp(1, alpha)\\n        tau_hi = max_val - cls._gp(1 / d, alpha)\\n\\n        f_lo = cls._p(X - tau_lo, alpha).sum(dim) - 1\\n\\n        dm = tau_hi - tau_lo\\n\\n        for it in range(n_iter):\\n\\n            dm /= 2\\n            tau_m = tau_lo + dm\\n            p_m = cls._p(X - tau_m, alpha)\\n            f_m = p_m.sum(dim) - 1\\n\\n            mask = (f_m * f_lo >= 0).unsqueeze(dim)\\n            tau_lo = torch.where(mask, tau_m, tau_lo)\\n\\n        if ensure_sum_one:\\n            p_m /= p_m.sum(dim=dim).unsqueeze(dim=dim)\\n\\n        ctx.save_for_backward(p_m)\\n\\n        return p_m\\n\\n    @classmethod\\n    def backward(cls, ctx, dY):\\n        Y, = ctx.saved_tensors\\n\\n        gppr = torch.where(Y > 0, Y ** (2 - ctx.alpha), Y.new_zeros(1))\\n\\n        dX = dY * gppr\\n        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\\n        q = q.unsqueeze(ctx.dim)\\n        dX -= q * gppr\\n\\n        d_alpha = None\\n        if ctx.needs_input_grad[1]:\\n\\n            # alpha gradient computation\\n            # d_alpha = (partial_y / partial_alpha) * dY\\n            # NOTE: ensure alpha is not close to 1\\n            # since there is an indetermination\\n            # batch_size, _ = dY.shape\\n\\n            # shannon terms\\n            S = torch.where(Y > 0, Y * torch.log(Y), Y.new_zeros(1))\\n            # shannon entropy\\n            ent = S.sum(ctx.dim).unsqueeze(ctx.dim)\\n            Y_skewed = gppr / gppr.sum(ctx.dim).unsqueeze(ctx.dim)\\n\\n            d_alpha = dY * (Y - Y_skewed) / ((ctx.alpha - 1) ** 2)\\n            d_alpha -= dY * (S - Y_skewed * ent) / (ctx.alpha - 1)\\n            d_alpha = d_alpha.sum(ctx.dim).unsqueeze(ctx.dim)\\n\\n        return dX, d_alpha, None, None, None\\n\\n\\ndef entmax_bisect(X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):\\n    \\\"\\\"\\\"alpha-entmax: normalizing sparse transform (a la softmax).\\n    Solves the optimization problem:\\n        max_p <x, p> - H_a(p)    s.t.    p >= 0, sum(p) == 1.\\n    where H_a(p) is the Tsallis alpha-entropy with custom alpha >= 1,\\n    using a bisection (root finding, binary search) algorithm.\\n    This function is differentiable with respect to both X and alpha.\\n    Parameters\\n    ----------\\n    X : torch.Tensor\\n        The input tensor.\\n    alpha : float or torch.Tensor\\n        Tensor of alpha parameters (> 1) to use. If scalar\\n        or python float, the same value is used for all rows, otherwise,\\n        it must have shape (or be expandable to)\\n        alpha.shape[j] == (X.shape[j] if j != dim else 1)\\n        A value of alpha=2 corresponds to sparsemax, and alpha=1 corresponds to\\n        softmax (but computing it this way is likely unstable).\\n    dim : int\\n        The dimension along which to apply alpha-entmax.\\n    n_iter : int\\n        Number of bisection iterations. For float32, 24 iterations should\\n        suffice for machine precision.\\n    ensure_sum_one : bool,\\n        Whether to divide the result by its sum. If false, the result might\\n        sum to close but not exactly 1, which might cause downstream problems.\\n    Returns\\n    -------\\n    P : torch tensor, same shape as X\\n        The projection result, such that P.sum(dim=dim) == 1 elementwise.\\n    \\\"\\\"\\\"\\n    return EntmaxBisectFunction.apply(X, alpha, dim, n_iter, ensure_sum_one)\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\nclass AlphaChooser(torch.nn.Module):\\n    def __init__(self, head_count):\\n        super(AlphaChooser, self).__init__()\\n        self.pre_alpha = nn.Parameter(torch.randn(head_count))\\n\\n    def forward(self):\\n        alpha = 1 + torch.sigmoid(self.pre_alpha)\\n        return torch.clamp(alpha, min=1.01, max=2)\\n\\n\\nclass EntmaxAlpha(nn.Module):\\n    def __init__(self, head_count, dim=0):\\n        super(EntmaxAlpha, self).__init__()\\n        self.dim = dim\\n        self.alpha_chooser = nn.Parameter(AlphaChooser(head_count)())\\n        self.alpha = self.alpha_chooser\\n\\n    def forward(self, att_scores):\\n        batch_size, head_count, query_len, key_len = att_scores.size()\\n\\n        expanded_alpha = self.alpha.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\\n        # [1,nb_heads,1,1]\\n        expanded_alpha = expanded_alpha.expand((batch_size, -1, query_len, 1))\\n        # [bs, nb_heads, query_len,1]\\n        p_star = entmax_bisect(att_scores, expanded_alpha)\\n        return p_star\\n\\n\\nclass EntmaxBisectFunction(Function):\\n    @classmethod\\n    def _gp(cls, x, alpha):\\n        return x ** (alpha - 1)\\n\\n    @classmethod\\n    def _gp_inv(cls, y, alpha):\\n        return y ** (1 / (alpha - 1))\\n\\n    @classmethod\\n    def _p(cls, X, alpha):\\n        return cls._gp_inv(torch.clamp(X, min=0), alpha)\\n\\n    @classmethod\\n    def forward(cls, ctx, X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):\\n\\n        if not isinstance(alpha, torch.Tensor):\\n            alpha = torch.tensor(alpha, dtype=X.dtype, device=X.device)\\n\\n        alpha_shape = list(X.shape)\\n        alpha_shape[dim] = 1\\n        alpha = alpha.expand(*alpha_shape)\\n\\n        ctx.alpha = alpha\\n        ctx.dim = dim\\n        d = X.shape[dim]\\n\\n        X = X * (alpha - 1)\\n\\n        max_val, _ = X.max(dim=dim, keepdim=True)\\n\\n        tau_lo = max_val - cls._gp(1, alpha)\\n        tau_hi = max_val - cls._gp(1 / d, alpha)\\n\\n        f_lo = cls._p(X - tau_lo, alpha).sum(dim) - 1\\n\\n        dm = tau_hi - tau_lo\\n\\n        for it in range(n_iter):\\n\\n            dm /= 2\\n            tau_m = tau_lo + dm\\n            p_m = cls._p(X - tau_m, alpha)\\n            f_m = p_m.sum(dim) - 1\\n\\n            mask = (f_m * f_lo >= 0).unsqueeze(dim)\\n            tau_lo = torch.where(mask, tau_m, tau_lo)\\n\\n        if ensure_sum_one:\\n            p_m /= p_m.sum(dim=dim).unsqueeze(dim=dim)\\n\\n        ctx.save_for_backward(p_m)\\n\\n        return p_m\\n\\n    @classmethod\\n    def backward(cls, ctx, dY):\\n        (Y,) = ctx.saved_tensors\\n\\n        gppr = torch.where(Y > 0, Y ** (2 - ctx.alpha), Y.new_zeros(1))\\n\\n        dX = dY * gppr\\n        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\\n        q = q.unsqueeze(ctx.dim)\\n        dX -= q * gppr\\n\\n        d_alpha = None\\n        if ctx.needs_input_grad[1]:\\n\\n            # alpha gradient computation\\n            # d_alpha = (partial_y / partial_alpha) * dY\\n            # NOTE: ensure alpha is not close to 1\\n            # since there is an indetermination\\n            # batch_size, _ = dY.shape\\n\\n            # shannon terms\\n            S = torch.where(Y > 0, Y * torch.log(Y), Y.new_zeros(1))\\n            # shannon entropy\\n            ent = S.sum(ctx.dim).unsqueeze(ctx.dim)\\n            Y_skewed = gppr / gppr.sum(ctx.dim).unsqueeze(ctx.dim)\\n\\n            d_alpha = dY * (Y - Y_skewed) / ((ctx.alpha - 1) ** 2)\\n            d_alpha -= dY * (S - Y_skewed * ent) / (ctx.alpha - 1)\\n            d_alpha = d_alpha.sum(ctx.dim).unsqueeze(ctx.dim)\\n\\n        return dX, d_alpha, None, None, None\\n\\n\\ndef entmax_bisect(X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):\\n    \\\"\\\"\\\"alpha-entmax: normalizing sparse transform (a la softmax).\\n    Solves the optimization problem:\\n        max_p <x, p> - H_a(p)    s.t.    p >= 0, sum(p) == 1.\\n    where H_a(p) is the Tsallis alpha-entropy with custom alpha >= 1,\\n    using a bisection (root finding, binary search) algorithm.\\n    This function is differentiable with respect to both X and alpha.\\n    Parameters\\n    ----------\\n    X : torch.Tensor\\n        The input tensor.\\n    alpha : float or torch.Tensor\\n        Tensor of alpha parameters (> 1) to use. If scalar\\n        or python float, the same value is used for all rows, otherwise,\\n        it must have shape (or be expandable to)\\n        alpha.shape[j] == (X.shape[j] if j != dim else 1)\\n        A value of alpha=2 corresponds to sparsemax, and alpha=1 corresponds to\\n        softmax (but computing it this way is likely unstable).\\n    dim : int\\n        The dimension along which to apply alpha-entmax.\\n    n_iter : int\\n        Number of bisection iterations. For float32, 24 iterations should\\n        suffice for machine precision.\\n    ensure_sum_one : bool,\\n        Whether to divide the result by its sum. If false, the result might\\n        sum to close but not exactly 1, which might cause downstream problems.\\n    Returns\\n    -------\\n    P : torch tensor, same shape as X\\n        The projection result, such that P.sum(dim=dim) == 1 elementwise.\\n    \\\"\\\"\\\"\\n    return EntmaxBisectFunction.apply(X, alpha, dim, n_iter, ensure_sum_one)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class AlphaChooser(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, head_count):\n",
    "        super(AlphaChooser, self).__init__()\n",
    "        self.pre_alpha = nn.Parameter(torch.randn(head_count))\n",
    "\n",
    "    def forward(self):\n",
    "        alpha = 1 + torch.sigmoid(self.pre_alpha)\n",
    "        return torch.clamp(alpha, min=1.01, max=2)\n",
    "\n",
    "\n",
    "class EntmaxAlpha(nn.Module):\n",
    "\n",
    "    def __init__(self, head_count, dim=0):\n",
    "        super(EntmaxAlpha, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.alpha_chooser = nn.Parameter(AlphaChooser(head_count)())\n",
    "        self.alpha = self.alpha_chooser\n",
    "\n",
    "    def forward(self, att_scores):\n",
    "        batch_size, head_count, query_len, key_len = att_scores.size()\n",
    "\n",
    "        expanded_alpha = self.alpha.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "        # [1,nb_heads,1,1]\n",
    "        expanded_alpha = expanded_alpha.expand((batch_size, -1, query_len, 1))\n",
    "        # [bs, nb_heads, query_len,1]\n",
    "        p_star = entmax_bisect(att_scores, expanded_alpha)\n",
    "        return p_star\n",
    "\n",
    "\n",
    "class EntmaxBisectFunction(Function):\n",
    "    @classmethod\n",
    "    def _gp(cls, x, alpha):\n",
    "        return x ** (alpha - 1)\n",
    "\n",
    "    @classmethod\n",
    "    def _gp_inv(cls, y, alpha):\n",
    "        return y ** (1 / (alpha - 1))\n",
    "\n",
    "    @classmethod\n",
    "    def _p(cls, X, alpha):\n",
    "        return cls._gp_inv(torch.clamp(X, min=0), alpha)\n",
    "\n",
    "    @classmethod\n",
    "    def forward(cls,\n",
    "                ctx,\n",
    "                X,\n",
    "                alpha=1.5,\n",
    "                dim=-1,\n",
    "                n_iter=50,\n",
    "                ensure_sum_one=True):\n",
    "\n",
    "        if not isinstance(alpha, torch.Tensor):\n",
    "            alpha = torch.tensor(alpha, dtype=X.dtype, device=X.device)\n",
    "\n",
    "        alpha_shape = list(X.shape)\n",
    "        alpha_shape[dim] = 1\n",
    "        alpha = alpha.expand(*alpha_shape)\n",
    "\n",
    "        ctx.alpha = alpha\n",
    "        ctx.dim = dim\n",
    "        d = X.shape[dim]\n",
    "\n",
    "        X = X * (alpha - 1)\n",
    "\n",
    "        max_val, _ = X.max(dim=dim, keepdim=True)\n",
    "\n",
    "        tau_lo = max_val - cls._gp(1, alpha)\n",
    "        tau_hi = max_val - cls._gp(1 / d, alpha)\n",
    "\n",
    "        f_lo = cls._p(X - tau_lo, alpha).sum(dim) - 1\n",
    "\n",
    "        dm = tau_hi - tau_lo\n",
    "\n",
    "        for it in range(n_iter):\n",
    "\n",
    "            dm /= 2\n",
    "            tau_m = tau_lo + dm\n",
    "            p_m = cls._p(X - tau_m, alpha)\n",
    "            f_m = p_m.sum(dim) - 1\n",
    "\n",
    "            mask = (f_m * f_lo >= 0).unsqueeze(dim)\n",
    "            tau_lo = torch.where(mask, tau_m, tau_lo)\n",
    "\n",
    "        if ensure_sum_one:\n",
    "            p_m /= p_m.sum(dim=dim).unsqueeze(dim=dim)\n",
    "\n",
    "        ctx.save_for_backward(p_m)\n",
    "\n",
    "        return p_m\n",
    "\n",
    "    @classmethod\n",
    "    def backward(cls, ctx, dY):\n",
    "        Y, = ctx.saved_tensors\n",
    "\n",
    "        gppr = torch.where(Y > 0, Y ** (2 - ctx.alpha), Y.new_zeros(1))\n",
    "\n",
    "        dX = dY * gppr\n",
    "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\n",
    "        q = q.unsqueeze(ctx.dim)\n",
    "        dX -= q * gppr\n",
    "\n",
    "        d_alpha = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "\n",
    "            # alpha gradient computation\n",
    "            # d_alpha = (partial_y / partial_alpha) * dY\n",
    "            # NOTE: ensure alpha is not close to 1\n",
    "            # since there is an indetermination\n",
    "            # batch_size, _ = dY.shape\n",
    "\n",
    "            # shannon terms\n",
    "            S = torch.where(Y > 0, Y * torch.log(Y), Y.new_zeros(1))\n",
    "            # shannon entropy\n",
    "            ent = S.sum(ctx.dim).unsqueeze(ctx.dim)\n",
    "            Y_skewed = gppr / gppr.sum(ctx.dim).unsqueeze(ctx.dim)\n",
    "\n",
    "            d_alpha = dY * (Y - Y_skewed) / ((ctx.alpha - 1) ** 2)\n",
    "            d_alpha -= dY * (S - Y_skewed * ent) / (ctx.alpha - 1)\n",
    "            d_alpha = d_alpha.sum(ctx.dim).unsqueeze(ctx.dim)\n",
    "\n",
    "        return dX, d_alpha, None, None, None\n",
    "\n",
    "\n",
    "def entmax_bisect(X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):\n",
    "    \"\"\"alpha-entmax: normalizing sparse transform (a la softmax).\n",
    "    Solves the optimization problem:\n",
    "        max_p <x, p> - H_a(p)    s.t.    p >= 0, sum(p) == 1.\n",
    "    where H_a(p) is the Tsallis alpha-entropy with custom alpha >= 1,\n",
    "    using a bisection (root finding, binary search) algorithm.\n",
    "    This function is differentiable with respect to both X and alpha.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.Tensor\n",
    "        The input tensor.\n",
    "    alpha : float or torch.Tensor\n",
    "        Tensor of alpha parameters (> 1) to use. If scalar\n",
    "        or python float, the same value is used for all rows, otherwise,\n",
    "        it must have shape (or be expandable to)\n",
    "        alpha.shape[j] == (X.shape[j] if j != dim else 1)\n",
    "        A value of alpha=2 corresponds to sparsemax, and alpha=1 corresponds to\n",
    "        softmax (but computing it this way is likely unstable).\n",
    "    dim : int\n",
    "        The dimension along which to apply alpha-entmax.\n",
    "    n_iter : int\n",
    "        Number of bisection iterations. For float32, 24 iterations should\n",
    "        suffice for machine precision.\n",
    "    ensure_sum_one : bool,\n",
    "        Whether to divide the result by its sum. If false, the result might\n",
    "        sum to close but not exactly 1, which might cause downstream problems.\n",
    "    Returns\n",
    "    -------\n",
    "    P : torch tensor, same shape as X\n",
    "        The projection result, such that P.sum(dim=dim) == 1 elementwise.\n",
    "    \"\"\"\n",
    "    return EntmaxBisectFunction.apply(X, alpha, dim, n_iter, ensure_sum_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"num_attention_heads = 12\";\n",
       "                var nbb_formatted_code = \"num_attention_heads = 12\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_attention_heads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"entmax_alpha = EntmaxAlpha(num_attention_heads)\";\n",
       "                var nbb_formatted_code = \"entmax_alpha = EntmaxAlpha(num_attention_heads)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "entmax_alpha = EntmaxAlpha(num_attention_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"attention_scores = torch.rand(128, 12, 26, 36)\";\n",
       "                var nbb_formatted_code = \"attention_scores = torch.rand(128, 12, 26, 36)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_scores = torch.rand(128, 12, 26, 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"attention_scores = entmax_alpha(attention_scores)\\n# This is meant to be replaced with softmax\";\n",
       "                var nbb_formatted_code = \"attention_scores = entmax_alpha(attention_scores)\\n# This is meant to be replaced with softmax\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_scores = entmax_alpha(attention_scores)\n",
    "# This is meant to be replaced with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch7",
   "language": "python",
   "name": "torch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
