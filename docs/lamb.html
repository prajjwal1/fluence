---

title: fluence.optim.lamb

keywords: fastai
sidebar: home_sidebar

summary: "Implements Lamb optimizer"
description: "Implements Lamb optimizer"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: lamb.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="4d982c88-3c8a-49b5-99c7-b83ff11c3b4d"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#4d982c88-3c8a-49b5-99c7-b83ff11c3b4d');

            setTimeout(function() {
                var nbb_cell_id = 7;
                var nbb_unformatted_code = "# export\nimport torch\nfrom torch.optim import Optimizer";
                var nbb_formatted_code = "# export\nimport torch\nfrom torch.optim import Optimizer";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="28fb79b9-bba6-4bbe-97e0-0439d4df61e3"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#28fb79b9-bba6-4bbe-97e0-0439d4df61e3');

            setTimeout(function() {
                var nbb_cell_id = 3;
                var nbb_unformatted_code = "# export\n\n\nclass Lamb(Optimizer):\n    r\"\"\"Implements Lamb algorithm.\n    It has been proposed in `Large Batch Optimization for Deep Learning:\n    Training BERT in 76 minutes`.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        adam (bool, optional): always use trust ratio = 1,\n        which turns this into Adam. Useful for comparison purposes.\n    .. _Large Batch Optimization for Deep Learning: Training BERT in\n        76 minutes:\n        https://arxiv.org/abs/1904.00962\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr=1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-6,\n        weight_decay=0,\n        adam=False,\n        min_trust=None,\n    ):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at \\\n                              index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at \\\n                              index 1: {}\".format(betas[1]))\n        if min_trust and not 0.0 <= min_trust < 1.0:\n            raise ValueError(\"Minimum trust range from 0 to \\\n                              1: {}\".format(min_trust))\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.adam = adam\n        self.min_trust = min_trust\n        super(Lamb, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \"Lamb does not support sparse gradients, \\\n                         consider SparseAdam instad.\"\n                    )\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                state[\"step\"] += 1\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                step_size = group[\"lr\"]\n\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n\n                adam_step = exp_avg / exp_avg_sq.sqrt().add(group[\"eps\"])\n                if group[\"weight_decay\"] != 0:\n                    adam_step.add_(group[\"weight_decay\"], p.data)\n\n                adam_norm = adam_step.pow(2).sum().sqrt()\n                if weight_norm == 0 or adam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm / adam_norm\n                if self.min_trust:\n                    trust_ratio = max(trust_ratio, self.min_trust)\n                state[\"weight_norm\"] = weight_norm\n                state[\"adam_norm\"] = adam_norm\n                state[\"trust_ratio\"] = trust_ratio\n                if self.adam:\n                    trust_ratio = 1\n\n                p.data.add_(-step_size * trust_ratio, adam_step)\n\n        return loss";
                var nbb_formatted_code = "# export\n\n\nclass Lamb(Optimizer):\n    r\"\"\"Implements Lamb algorithm.\n    It has been proposed in `Large Batch Optimization for Deep Learning:\n    Training BERT in 76 minutes`.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        adam (bool, optional): always use trust ratio = 1,\n        which turns this into Adam. Useful for comparison purposes.\n    .. _Large Batch Optimization for Deep Learning: Training BERT in\n        76 minutes:\n        https://arxiv.org/abs/1904.00962\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr=1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-6,\n        weight_decay=0,\n        adam=False,\n        min_trust=None,\n    ):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                \"Invalid beta parameter at \\\n                              index 0: {}\".format(\n                    betas[0]\n                )\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                \"Invalid beta parameter at \\\n                              index 1: {}\".format(\n                    betas[1]\n                )\n            )\n        if min_trust and not 0.0 <= min_trust < 1.0:\n            raise ValueError(\n                \"Minimum trust range from 0 to \\\n                              1: {}\".format(\n                    min_trust\n                )\n            )\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.adam = adam\n        self.min_trust = min_trust\n        super(Lamb, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \"Lamb does not support sparse gradients, \\\n                         consider SparseAdam instad.\"\n                    )\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                state[\"step\"] += 1\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                step_size = group[\"lr\"]\n\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n\n                adam_step = exp_avg / exp_avg_sq.sqrt().add(group[\"eps\"])\n                if group[\"weight_decay\"] != 0:\n                    adam_step.add_(group[\"weight_decay\"], p.data)\n\n                adam_norm = adam_step.pow(2).sum().sqrt()\n                if weight_norm == 0 or adam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm / adam_norm\n                if self.min_trust:\n                    trust_ratio = max(trust_ratio, self.min_trust)\n                state[\"weight_norm\"] = weight_norm\n                state[\"adam_norm\"] = adam_norm\n                state[\"trust_ratio\"] = trust_ratio\n                if self.adam:\n                    trust_ratio = 1\n\n                p.data.add_(-step_size * trust_ratio, adam_step)\n\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Lamb" class="doc_header"><code>class</code> <code>Lamb</code><a href="https://github.com/prajjwal1/fluence/tree/master/fluence/optim/lamb.py#L12" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Lamb</code>(<strong><code>params</code></strong>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>betas</code></strong>=<em><code>(0.9, 0.999)</code></em>, <strong><code>eps</code></strong>=<em><code>1e-06</code></em>, <strong><code>weight_decay</code></strong>=<em><code>0</code></em>, <strong><code>adam</code></strong>=<em><code>False</code></em>, <strong><code>min_trust</code></strong>=<em><code>None</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Implements Lamb algorithm.
It has been proposed in <code>Large Batch Optimization for Deep Learning:
Training BERT in 76 minutes</code>.
Arguments:
    params (iterable): iterable of parameters to optimize or dicts defining
        parameter groups
    lr (float, optional): learning rate (default: 1e-3)
    betas (Tuple[float, float], optional): coefficients used for computing
        running averages of gradient and its square (default: (0.9, 0.999))
    eps (float, optional): term added to the denominator to improve
        numerical stability (default: 1e-8)
    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
    adam (bool, optional): always use trust ratio = 1,
    which turns this into Adam. Useful for comparison purposes.
.. _Large Batch Optimization for Deep Learning: Training BERT in
    76 minutes:
    <a href="https://arxiv.org/abs/1904.00962">https://arxiv.org/abs/1904.00962</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">AlexNet</span><span class="p">()</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">Lamb</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="4feb2992-fc75-475d-8244-44793b0e019b"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#4feb2992-fc75-475d-8244-44793b0e019b');

            setTimeout(function() {
                var nbb_cell_id = 8;
                var nbb_unformatted_code = "import torchvision\nmodel = torchvision.models.AlexNet()\noptim = Lamb(model.parameters())\noutput = model(torch.rand(128, 3, 64, 64))\noptim.step()";
                var nbb_formatted_code = "import torchvision\n\nmodel = torchvision.models.AlexNet()\noptim = Lamb(model.parameters())\noutput = model(torch.rand(128, 3, 64, 64))\noptim.step()";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

