---

title: lamb optimizer

keywords: fastai
sidebar: home_sidebar

summary: "Implements Lamb optimizer"
description: "Implements Lamb optimizer"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: optim.lamb.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Lamb" class="doc_header"><code>class</code> <code>Lamb</code><a href="https://github.com/prajjwal1/fluence/tree/master/fluence/optim/lamb.py#L12" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Lamb</code>(<strong><code>params</code></strong>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>betas</code></strong>=<em><code>(0.9, 0.999)</code></em>, <strong><code>eps</code></strong>=<em><code>1e-06</code></em>, <strong><code>weight_decay</code></strong>=<em><code>0</code></em>, <strong><code>adam</code></strong>=<em><code>False</code></em>, <strong><code>min_trust</code></strong>=<em><code>None</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Implements Lamb algorithm.
It has been proposed in <code>Large Batch Optimization for Deep Learning:
Training BERT in 76 minutes</code>.
Arguments:
    params (iterable): iterable of parameters to optimize or dicts defining
        parameter groups
    lr (float, optional): learning rate (default: 1e-3)
    betas (Tuple[float, float], optional): coefficients used for computing
        running averages of gradient and its square (default: (0.9, 0.999))
    eps (float, optional): term added to the denominator to improve
        numerical stability (default: 1e-8)
    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
    adam (bool, optional): always use trust ratio = 1,
    which turns this into Adam. Useful for comparison purposes.
.. _Large Batch Optimization for Deep Learning: Training BERT in
    76 minutes:
    <a href="https://arxiv.org/abs/1904.00962">https://arxiv.org/abs/1904.00962</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

