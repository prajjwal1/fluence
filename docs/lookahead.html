---

title: fluence.optim.lookahead

keywords: fastai
sidebar: home_sidebar

summary: "Implements Lookahead optimizer"
description: "Implements Lookahead optimizer"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: lookahead.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> pycodestyle_magic
<span class="o">%</span><span class="k">pycodestyle_on</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="87601bf7-8f1a-4347-b7d3-e1711d7f3037"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#87601bf7-8f1a-4347-b7d3-e1711d7f3037');

            setTimeout(function() {
                var nbb_cell_id = 2;
                var nbb_unformatted_code = "%load_ext pycodestyle_magic\n%pycodestyle_on\n%load_ext nb_black";
                var nbb_formatted_code = "%load_ext pycodestyle_magic\n%pycodestyle_on\n%load_ext nb_black";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="ebdb1dc0-a6f0-405c-bc33-9b90cc6c51c7"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#ebdb1dc0-a6f0-405c-bc33-9b90cc6c51c7');

            setTimeout(function() {
                var nbb_cell_id = 11;
                var nbb_unformatted_code = "# export\nfrom collections import defaultdict\n\nimport torch\nfrom torch.optim.optimizer import Optimizer";
                var nbb_formatted_code = "# export\nfrom collections import defaultdict\n\nimport torch\nfrom torch.optim.optimizer import Optimizer";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>2:1: E302 expected 2 blank lines, found 0
78:1: W391 blank line at end of file
</pre>
</div>
</div>

<div class="output_area">




<div id="df44b6ee-3119-4dca-b310-776efd583086"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#df44b6ee-3119-4dca-b310-776efd583086');

            setTimeout(function() {
                var nbb_cell_id = 5;
                var nbb_unformatted_code = "# export\nclass Lookahead(Optimizer):\n    def __init__(self, base_optimizer, alpha=0.5, k=6):\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\"Slow update rate is not valid: {alpha}\")\n        if not 1 <= k:\n            raise ValueError(f\"Invalid lookahead steps: {k}\")\n        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n        self.base_optimizer = base_optimizer\n        self.param_groups = self.base_optimizer.param_groups\n        self.defaults = base_optimizer.defaults\n        self.defaults.update(defaults)\n        self.state = defaultdict(dict)\n        for name, default in defaults.items():\n            for group in self.param_groups:\n                group.setdefault(name, default)\n\n    def update_slow(self, group):\n        for fast_p in group[\"params\"]:\n            if fast_p.grad is None:\n                continue\n            param_state = self.state[fast_p]\n            if \"slow_buffer\" not in param_state:\n                param_state[\"slow_buffer\"] = torch.empty_like(fast_p.data)\n                param_state[\"slow_buffer\"].copy_(fast_p.data)\n            slow = param_state[\"slow_buffer\"]\n            slow.add_(group[\"lookahead_alpha\"], fast_p.data - slow)\n            fast_p.data.copy_(slow)\n\n    def sync_lookahead(self):\n        for group in self.param_groups:\n            self.update_slow(group)\n\n    def step(self, closure=None):\n        loss = self.base_optimizer.step(closure)\n        for group in self.param_groups:\n            group[\"lookahead_step\"] += 1\n            if group[\"lookahead_step\"] % group[\"lookahead_k\"] == 0:\n                self.update_slow(group)\n        return loss\n\n    def state_dict(self):\n        fast_state_dict = self.base_optimizer.state_dict()\n        slow_state = {\n            (id(k) if isinstance(k, torch.Tensor) else k): v\n            for k, v in self.state.items()\n        }\n        fast_state = fast_state_dict[\"state\"]\n        param_groups = fast_state_dict[\"param_groups\"]\n        return {\n            \"state\": fast_state,\n            \"slow_state\": slow_state,\n            \"param_groups\": param_groups,\n        }\n\n    def load_state_dict(self, state_dict):\n        fast_state_dict = {\n            \"state\": state_dict[\"state\"],\n            \"param_groups\": state_dict[\"param_groups\"],\n        }\n        self.base_optimizer.load_state_dict(fast_state_dict)\n        slow_state_new = False\n        if \"slow_state\" not in state_dict:\n            print(\"Loading state_dict from optimizer without \\\n                     Lookahead applied.\")\n            state_dict[\"slow_state\"] = defaultdict(dict)\n            slow_state_new = True\n        slow_state_dict = {\n            \"state\": state_dict[\"slow_state\"],\n            \"param_groups\": state_dict[\"param_groups\"],\n        }\n        super(Lookahead, self).load_state_dict(slow_state_dict)\n        self.param_groups = self.base_optimizer.param_groups\n        if slow_state_new:\n            for name, default in self.defaults.items():\n                for group in self.param_groups:\n                    group.setdefault(name, default)";
                var nbb_formatted_code = "# export\nclass Lookahead(Optimizer):\n    def __init__(self, base_optimizer, alpha=0.5, k=6):\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\"Slow update rate is not valid: {alpha}\")\n        if not 1 <= k:\n            raise ValueError(f\"Invalid lookahead steps: {k}\")\n        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n        self.base_optimizer = base_optimizer\n        self.param_groups = self.base_optimizer.param_groups\n        self.defaults = base_optimizer.defaults\n        self.defaults.update(defaults)\n        self.state = defaultdict(dict)\n        for name, default in defaults.items():\n            for group in self.param_groups:\n                group.setdefault(name, default)\n\n    def update_slow(self, group):\n        for fast_p in group[\"params\"]:\n            if fast_p.grad is None:\n                continue\n            param_state = self.state[fast_p]\n            if \"slow_buffer\" not in param_state:\n                param_state[\"slow_buffer\"] = torch.empty_like(fast_p.data)\n                param_state[\"slow_buffer\"].copy_(fast_p.data)\n            slow = param_state[\"slow_buffer\"]\n            slow.add_(group[\"lookahead_alpha\"], fast_p.data - slow)\n            fast_p.data.copy_(slow)\n\n    def sync_lookahead(self):\n        for group in self.param_groups:\n            self.update_slow(group)\n\n    def step(self, closure=None):\n        loss = self.base_optimizer.step(closure)\n        for group in self.param_groups:\n            group[\"lookahead_step\"] += 1\n            if group[\"lookahead_step\"] % group[\"lookahead_k\"] == 0:\n                self.update_slow(group)\n        return loss\n\n    def state_dict(self):\n        fast_state_dict = self.base_optimizer.state_dict()\n        slow_state = {\n            (id(k) if isinstance(k, torch.Tensor) else k): v\n            for k, v in self.state.items()\n        }\n        fast_state = fast_state_dict[\"state\"]\n        param_groups = fast_state_dict[\"param_groups\"]\n        return {\n            \"state\": fast_state,\n            \"slow_state\": slow_state,\n            \"param_groups\": param_groups,\n        }\n\n    def load_state_dict(self, state_dict):\n        fast_state_dict = {\n            \"state\": state_dict[\"state\"],\n            \"param_groups\": state_dict[\"param_groups\"],\n        }\n        self.base_optimizer.load_state_dict(fast_state_dict)\n        slow_state_new = False\n        if \"slow_state\" not in state_dict:\n            print(\n                \"Loading state_dict from optimizer without \\\n                     Lookahead applied.\"\n            )\n            state_dict[\"slow_state\"] = defaultdict(dict)\n            slow_state_new = True\n        slow_state_dict = {\n            \"state\": state_dict[\"slow_state\"],\n            \"param_groups\": state_dict[\"param_groups\"],\n        }\n        super(Lookahead, self).load_state_dict(slow_state_dict)\n        self.param_groups = self.base_optimizer.param_groups\n        if slow_state_new:\n            for name, default in self.defaults.items():\n                for group in self.param_groups:\n                    group.setdefault(name, default)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Lookahead" class="doc_header"><code>class</code> <code>Lookahead</code><a href="https://github.com/prajjwal1/fluence/tree/master/fluence/optim/lookahead.py#L12" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Lookahead</code>(<strong><code>base_optimizer</code></strong>, <strong><code>alpha</code></strong>=<em><code>0.5</code></em>, <strong><code>k</code></strong>=<em><code>6</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Base class for all optimizers.</p>
<p>.. warning::
    Parameters need to be specified as collections that have a deterministic
    ordering that is consistent between runs. Examples of objects that don't
    satisfy those properties are sets and iterators over values of dictionaries.</p>
<p>Arguments:
    params (iterable): an iterable of :class:<code>torch.Tensor</code> s or
        :class:<code>dict</code> s. Specifies what Tensors should be optimized.
    defaults: (dict): a dict containing default values of optimization
        options (used when a parameter group doesn't specify them).</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">AlexNet</span><span class="p">()</span>
<span class="n">base_optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">Lookahead</span><span class="p">(</span><span class="n">base_optim</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="0adae0ab-a301-4906-9bf5-e8156daa948c"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#0adae0ab-a301-4906-9bf5-e8156daa948c');

            setTimeout(function() {
                var nbb_cell_id = 12;
                var nbb_unformatted_code = "import torchvision\nmodel = torchvision.models.AlexNet()\nbase_optim = torch.optim.Adam(model.parameters())\noptim = Lookahead(base_optim, k=5, alpha=0.8)\noutput = model(torch.rand(128, 3, 64, 64))\noptim.step()";
                var nbb_formatted_code = "import torchvision\n\nmodel = torchvision.models.AlexNet()\nbase_optim = torch.optim.Adam(model.parameters())\noptim = Lookahead(base_optim, k=5, alpha=0.8)\noutput = model(torch.rand(128, 3, 64, 64))\noptim.step()";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

