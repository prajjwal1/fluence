---

title: Tests/Model

keywords: fastai
sidebar: home_sidebar

summary: "Runs tests for fluence.models"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: model_tests.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    {% raw %}
        
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>I0219 22:46:20.934055 139882232854336 file_utils.py:35] PyTorch version 1.4.0+cpu available.
/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
  _np_qint8 = np.dtype([(&#34;qint8&#34;, np.int8, 1)])
/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
  _np_quint8 = np.dtype([(&#34;quint8&#34;, np.uint8, 1)])
/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
  _np_qint16 = np.dtype([(&#34;qint16&#34;, np.int16, 1)])
/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
  _np_quint16 = np.dtype([(&#34;quint16&#34;, np.uint16, 1)])
/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
  _np_qint32 = np.dtype([(&#34;qint32&#34;, np.int32, 1)])
/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
  np_resource = np.dtype([(&#34;resource&#34;, np.ubyte, 1)])
/glob/intel-python/python3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
I0219 22:46:25.670626 139882232854336 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/u37216/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0219 22:46:26.029149 139882232854336 lxmert_utils.py:217] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/u37216/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
I0219 22:46:26.032839 139882232854336 lxmert_utils.py:225] extracting archive file /home/u37216/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /home/u37216/tmp/tmp67s_yh5y
I0219 22:46:33.948943 139882232854336 lxmert_utils.py:232] Model config {
  &#34;attention_probs_dropout_prob&#34;: 0.1,
  &#34;finetuning_task&#34;: null,
  &#34;hidden_act&#34;: &#34;gelu&#34;,
  &#34;hidden_dropout_prob&#34;: 0.1,
  &#34;hidden_size&#34;: 768,
  &#34;id2label&#34;: {
    &#34;0&#34;: &#34;LABEL_0&#34;,
    &#34;1&#34;: &#34;LABEL_1&#34;
  },
  &#34;initializer_range&#34;: 0.02,
  &#34;intermediate_size&#34;: 3072,
  &#34;is_decoder&#34;: false,
  &#34;label2id&#34;: {
    &#34;LABEL_0&#34;: 0,
    &#34;LABEL_1&#34;: 1
  },
  &#34;layer_norm_eps&#34;: 1e-12,
  &#34;max_position_embeddings&#34;: 512,
  &#34;num_attention_heads&#34;: 12,
  &#34;num_hidden_layers&#34;: 12,
  &#34;num_labels&#34;: 2,
  &#34;output_attentions&#34;: false,
  &#34;output_hidden_states&#34;: false,
  &#34;output_past&#34;: true,
  &#34;pruned_heads&#34;: {},
  &#34;torchscript&#34;: false,
  &#34;type_vocab_size&#34;: 2,
  &#34;use_bfloat16&#34;: false,
  &#34;vocab_size&#34;: 30522
}

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.
Sparse Enabled
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>I0219 22:27:55.681105 140188148434752 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/u37216/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0219 22:27:56.089632 140188148434752 lxmert_utils.py:217] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/u37216/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
I0219 22:27:56.093214 140188148434752 lxmert_utils.py:225] extracting archive file /home/u37216/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /home/u37216/tmp/tmpgwlzyr9q
I0219 22:28:02.912562 140188148434752 lxmert_utils.py:232] Model config {
  &#34;attention_probs_dropout_prob&#34;: 0.1,
  &#34;finetuning_task&#34;: null,
  &#34;hidden_act&#34;: &#34;gelu&#34;,
  &#34;hidden_dropout_prob&#34;: 0.1,
  &#34;hidden_size&#34;: 768,
  &#34;id2label&#34;: {
    &#34;0&#34;: &#34;LABEL_0&#34;,
    &#34;1&#34;: &#34;LABEL_1&#34;
  },
  &#34;initializer_range&#34;: 0.02,
  &#34;intermediate_size&#34;: 3072,
  &#34;is_decoder&#34;: false,
  &#34;label2id&#34;: {
    &#34;LABEL_0&#34;: 0,
    &#34;LABEL_1&#34;: 1
  },
  &#34;layer_norm_eps&#34;: 1e-12,
  &#34;max_position_embeddings&#34;: 512,
  &#34;num_attention_heads&#34;: 12,
  &#34;num_hidden_layers&#34;: 12,
  &#34;num_labels&#34;: 2,
  &#34;output_attentions&#34;: false,
  &#34;output_hidden_states&#34;: false,
  &#34;output_past&#34;: true,
  &#34;pruned_heads&#34;: {},
  &#34;torchscript&#34;: false,
  &#34;type_vocab_size&#34;: 2,
  &#34;use_bfloat16&#34;: false,
  &#34;vocab_size&#34;: 30522
}

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.
Sparse Enabled
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}
</div>
 

