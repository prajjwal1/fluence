---

title: Title

keywords: fastai
sidebar: home_sidebar

summary: "summary"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: entmax.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    {% raw %}
        
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AlphaChooser" class="doc_header"><code>class</code> <code>AlphaChooser</code><a href="https://github.com/prajjwal1/fluence/tree/master/fluence/adaptive/entmax.py#L13" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AlphaChooser</code>(<strong><code>head_count</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="EntmaxAlpha" class="doc_header"><code>class</code> <code>EntmaxAlpha</code><a href="https://github.com/prajjwal1/fluence/tree/master/fluence/adaptive/entmax.py#L23" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>EntmaxAlpha</code>(<strong><code>head_count</code></strong>, <strong><code>dim</code></strong>=<em><code>0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="EntmaxBisectFunction" class="doc_header"><code>class</code> <code>EntmaxBisectFunction</code><a href="https://github.com/prajjwal1/fluence/tree/master/fluence/adaptive/entmax.py#L39" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>EntmaxBisectFunction</code>() :: <code>Function</code></p>
</blockquote>
<p>Records operation history and defines formulas for differentiating ops.</p>
<p>Every operation performed on :class:<code>Tensor</code> s creates a new function
object, that performs the computation, and records that it happened.
The history is retained in the form of a DAG of functions, with edges
denoting data dependencies (<code>input &lt;- output</code>). Then, when backward is
called, the graph is processed in the topological ordering, by calling
:func:<code>backward</code> methods of each :class:<code>Function</code> object, and passing
returned gradients on to next :class:<code>Function</code> s.</p>
<p>Normally, the only way users interact with functions is by creating
subclasses and defining new operations. This is a recommended way of
extending torch.autograd.</p>
<p>Each function object is meant to be used only once (in the forward pass).</p>
<p>Examples::</p>

<pre><code>&gt;&gt;&gt; class Exp(Function):
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def forward(ctx, i):
&gt;&gt;&gt;         result = i.exp()
&gt;&gt;&gt;         ctx.save_for_backward(result)
&gt;&gt;&gt;         return result
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def backward(ctx, grad_output):
&gt;&gt;&gt;         result, = ctx.saved_tensors
&gt;&gt;&gt;         return grad_output * result</code></pre>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="entmax_bisect" class="doc_header"><code>entmax_bisect</code><a href="https://github.com/prajjwal1/fluence/tree/master/fluence/adaptive/entmax.py#L126" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>entmax_bisect</code>(<strong><code>X</code></strong>, <strong><code>alpha</code></strong>=<em><code>1.5</code></em>, <strong><code>dim</code></strong>=<em><code>-1</code></em>, <strong><code>n_iter</code></strong>=<em><code>50</code></em>, <strong><code>ensure_sum_one</code></strong>=<em><code>True</code></em>)</p>
</blockquote>
<p>alpha-entmax: normalizing sparse transform (a la softmax).
Solves the optimization problem:
    max_p &lt;x, p&gt; - H_a(p)    s.t.    p &gt;= 0, sum(p) == 1.
where H_a(p) is the Tsallis alpha-entropy with custom alpha &gt;= 1,
using a bisection (root finding, binary search) algorithm.
This function is differentiable with respect to both X and alpha.</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters">&#182;</a></h2><p>X : torch.Tensor
    The input tensor.
alpha : float or torch.Tensor
    Tensor of alpha parameters (&gt; 1) to use. If scalar
    or python float, the same value is used for all rows, otherwise,
    it must have shape (or be expandable to)
    alpha.shape[j] == (X.shape[j] if j != dim else 1)
    A value of alpha=2 corresponds to sparsemax, and alpha=1 corresponds to
    softmax (but computing it this way is likely unstable).
dim : int
    The dimension along which to apply alpha-entmax.
n_iter : int
    Number of bisection iterations. For float32, 24 iterations should
    suffice for machine precision.
ensure_sum_one : bool,
    Whether to divide the result by its sum. If false, the result might
    sum to close but not exactly 1, which might cause downstream problems.</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns">&#182;</a></h2><p>P : torch tensor, same shape as X
    The projection result, such that P.sum(dim=dim) == 1 elementwise.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="mi">12</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">entmax_alpha</span> <span class="o">=</span> <span class="n">EntmaxAlpha</span><span class="p">(</span><span class="n">num_attention_heads</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">26</span><span class="p">,</span><span class="mi">36</span><span class="p">)</span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attention_scores</span> <span class="o">=</span> <span class="n">entmax_alpha</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span> <span class="c1"># This is meant to be replaced with softmax</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}
</div>
 

