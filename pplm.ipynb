{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 144;\n",
       "                var nbb_unformatted_code = \"accumulated_hidden \";\n",
       "                var nbb_formatted_code = \"accumulated_hidden\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0504 23:21:58.036933 139800908659584 file_utils.py:41] PyTorch version 1.4.0+cpu available.\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import argparse\\nimport json\\nfrom operator import add\\nfrom typing import List, Optional, Tuple, Union\\n\\nimport numpy as np\\nimport torch\\nfrom torch import nn\\nimport torch.nn.functional as F\\nfrom torch.autograd import Variable\\nfrom tqdm import trange\\n\\nfrom transformers import GPT2Tokenizer\\nfrom transformers.modeling_gpt2 import GPT2LMHeadModel\\nfrom transformers.file_utils import cached_path\";\n",
       "                var nbb_formatted_code = \"import argparse\\nimport json\\nfrom operator import add\\nfrom typing import List, Optional, Tuple, Union\\n\\nimport numpy as np\\nimport torch\\nfrom torch import nn\\nimport torch.nn.functional as F\\nfrom torch.autograd import Variable\\nfrom tqdm import trange\\n\\nfrom transformers import GPT2Tokenizer\\nfrom transformers.modeling_gpt2 import GPT2LMHeadModel\\nfrom transformers.file_utils import cached_path\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "from operator import add\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import trange\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers.modeling_gpt2 import GPT2LMHeadModel\n",
    "from transformers.file_utils import cached_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"PPLM_BOW = 1\\nPPLM_DISCRIM = 2\\nPPLM_BOW_DISCRIM = 3\\nSMALL_CONST = 1e-15\\nBIG_CONST = 1e10\\n\\nBAG_OF_WORDS_ARCHIVE_MAP = {\\n    \\\"legal\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/legal.txt\\\",\\n    \\\"military\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/military.txt\\\",\\n    \\\"politics\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/politics.txt\\\",\\n    \\\"religion\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/religion.txt\\\",\\n    \\\"science\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/science.txt\\\",\\n    \\\"space\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/space.txt\\\",\\n    \\\"technology\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/technology.txt\\\",\\n}\\n\\nDISCRIMINATOR_MODELS_PARAMS = {\\n    \\\"clickbait\\\": {\\n        \\\"url\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/clickbait_classifier_head.pt\\\",\\n        \\\"class_size\\\": 2,\\n        \\\"embed_size\\\": 1024,\\n        \\\"class_vocab\\\": {\\\"non_clickbait\\\": 0, \\\"clickbait\\\": 1},\\n        \\\"default_class\\\": 1,\\n        \\\"pretrained_model\\\": \\\"gpt2-medium\\\",\\n    },\\n    \\\"sentiment\\\": {\\n        \\\"url\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/SST_classifier_head.pt\\\",\\n        \\\"class_size\\\": 5,\\n        \\\"embed_size\\\": 1024,\\n        \\\"class_vocab\\\": {\\\"very_positive\\\": 2, \\\"very_negative\\\": 3},\\n        \\\"default_class\\\": 3,\\n        \\\"pretrained_model\\\": \\\"gpt2-medium\\\",\\n    },\\n}\";\n",
       "                var nbb_formatted_code = \"PPLM_BOW = 1\\nPPLM_DISCRIM = 2\\nPPLM_BOW_DISCRIM = 3\\nSMALL_CONST = 1e-15\\nBIG_CONST = 1e10\\n\\nBAG_OF_WORDS_ARCHIVE_MAP = {\\n    \\\"legal\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/legal.txt\\\",\\n    \\\"military\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/military.txt\\\",\\n    \\\"politics\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/politics.txt\\\",\\n    \\\"religion\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/religion.txt\\\",\\n    \\\"science\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/science.txt\\\",\\n    \\\"space\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/space.txt\\\",\\n    \\\"technology\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/technology.txt\\\",\\n}\\n\\nDISCRIMINATOR_MODELS_PARAMS = {\\n    \\\"clickbait\\\": {\\n        \\\"url\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/clickbait_classifier_head.pt\\\",\\n        \\\"class_size\\\": 2,\\n        \\\"embed_size\\\": 1024,\\n        \\\"class_vocab\\\": {\\\"non_clickbait\\\": 0, \\\"clickbait\\\": 1},\\n        \\\"default_class\\\": 1,\\n        \\\"pretrained_model\\\": \\\"gpt2-medium\\\",\\n    },\\n    \\\"sentiment\\\": {\\n        \\\"url\\\": \\\"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/SST_classifier_head.pt\\\",\\n        \\\"class_size\\\": 5,\\n        \\\"embed_size\\\": 1024,\\n        \\\"class_vocab\\\": {\\\"very_positive\\\": 2, \\\"very_negative\\\": 3},\\n        \\\"default_class\\\": 3,\\n        \\\"pretrained_model\\\": \\\"gpt2-medium\\\",\\n    },\\n}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PPLM_BOW = 1\n",
    "PPLM_DISCRIM = 2\n",
    "PPLM_BOW_DISCRIM = 3\n",
    "SMALL_CONST = 1e-15\n",
    "BIG_CONST = 1e10\n",
    "\n",
    "BAG_OF_WORDS_ARCHIVE_MAP = {\n",
    "    \"legal\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/legal.txt\",\n",
    "    \"military\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/military.txt\",\n",
    "    \"politics\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/politics.txt\",\n",
    "    \"religion\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/religion.txt\",\n",
    "    \"science\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/science.txt\",\n",
    "    \"space\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/space.txt\",\n",
    "    \"technology\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/technology.txt\",\n",
    "}\n",
    "\n",
    "DISCRIMINATOR_MODELS_PARAMS = {\n",
    "    \"clickbait\": {\n",
    "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/clickbait_classifier_head.pt\",\n",
    "        \"class_size\": 2,\n",
    "        \"embed_size\": 1024,\n",
    "        \"class_vocab\": {\"non_clickbait\": 0, \"clickbait\": 1},\n",
    "        \"default_class\": 1,\n",
    "        \"pretrained_model\": \"gpt2-medium\",\n",
    "    },\n",
    "    \"sentiment\": {\n",
    "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/SST_classifier_head.pt\",\n",
    "        \"class_size\": 5,\n",
    "        \"embed_size\": 1024,\n",
    "        \"class_vocab\": {\"very_positive\": 2, \"very_negative\": 3},\n",
    "        \"default_class\": 3,\n",
    "        \"pretrained_model\": \"gpt2-medium\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"def top_k_filter(logits, k, probs=False):\\n    \\\"\\\"\\\"\\n    Masks everything but the k top entries as -infinity (1e10).\\n    Used to mask logits such that e^-infinity -> 0 won't contribute to the\\n    sum of the denominator.\\n    \\\"\\\"\\\"\\n    if k == 0:\\n        return logits\\n    else:\\n        values = torch.topk(logits, k)[0]\\n        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\\n        if probs:\\n            return torch.where(\\n                logits < batch_mins, torch.ones_like(logits) * 0.0, logits\\n            )\\n        return torch.where(\\n            logits < batch_mins, torch.ones_like(logits) * -BIG_CONST, logits\\n        )\";\n",
       "                var nbb_formatted_code = \"def top_k_filter(logits, k, probs=False):\\n    \\\"\\\"\\\"\\n    Masks everything but the k top entries as -infinity (1e10).\\n    Used to mask logits such that e^-infinity -> 0 won't contribute to the\\n    sum of the denominator.\\n    \\\"\\\"\\\"\\n    if k == 0:\\n        return logits\\n    else:\\n        values = torch.topk(logits, k)[0]\\n        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\\n        if probs:\\n            return torch.where(\\n                logits < batch_mins, torch.ones_like(logits) * 0.0, logits\\n            )\\n        return torch.where(\\n            logits < batch_mins, torch.ones_like(logits) * -BIG_CONST, logits\\n        )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def top_k_filter(logits, k, probs=False):\n",
    "    \"\"\"\n",
    "    Masks everything but the k top entries as -infinity (1e10).\n",
    "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
    "    sum of the denominator.\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    else:\n",
    "        values = torch.topk(logits, k)[0]\n",
    "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
    "        if probs:\n",
    "            return torch.where(\n",
    "                logits < batch_mins, torch.ones_like(logits) * 0.0, logits\n",
    "            )\n",
    "        return torch.where(\n",
    "            logits < batch_mins, torch.ones_like(logits) * -BIG_CONST, logits\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def perturb_past(\\n    past,\\n    model,\\n    last,\\n    unpert_past=None,\\n    unpert_logits=None,\\n    accumulated_hidden=None,\\n    grad_norms=None,\\n    stepsize=0.01,\\n    one_hot_bows_vectors=None,\\n    classifier=None,\\n    class_label=None,\\n    loss_type=0,\\n    num_iterations=3,\\n    horizon_length=1,\\n    window_length=0,\\n    decay=False,\\n    gamma=1.5,\\n    kl_scale=0.01,\\n    device=\\\"cuda\\\",\\n):\\n    # Generate inital perturbed past\\n    grad_accumulator = [(np.zeros(p.shape).astype(\\\"float32\\\")) for p in past]\\n\\n    if accumulated_hidden is None:\\n        accumulated_hidden = 0\\n\\n    if decay:\\n        decay_mask = torch.arange(0.0, 1.0 + SMALL_CONST, 1.0 / (window_length))[1:]\\n    else:\\n        decay_mask = 1.0\\n\\n    # TODO fix this comment (SUMANTH)\\n    # Generate a mask is gradient perturbated is based on a past window\\n    _, _, _, curr_length, _ = past[0].shape\\n\\n    if curr_length > window_length and window_length > 0:\\n        ones_key_val_shape = (\\n            tuple(past[0].shape[:-2])\\n            + tuple([window_length])\\n            + tuple(past[0].shape[-1:])\\n        )\\n\\n        zeros_key_val_shape = (\\n            tuple(past[0].shape[:-2])\\n            + tuple([curr_length - window_length])\\n            + tuple(past[0].shape[-1:])\\n        )\\n\\n        ones_mask = torch.ones(ones_key_val_shape)\\n        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\\n        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\\n\\n        window_mask = torch.cat(\\n            (ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2\\n        ).to(device)\\n    else:\\n        window_mask = torch.ones_like(past[0]).to(device)\\n\\n    # accumulate perturbations for num_iterations\\n    loss_per_iter = []\\n    new_accumulated_hidden = None\\n    for i in range(num_iterations):\\n        print(\\\"Iteration \\\", i + 1)\\n        curr_perturbation = [\\n            to_var(torch.from_numpy(p_), requires_grad=True, device=device)\\n            for p_ in grad_accumulator\\n        ]\\n\\n        # Compute hidden using perturbed past\\n        perturbed_past = list(map(add, past, curr_perturbation))\\n        _, _, _, curr_length, _ = curr_perturbation[0].shape\\n        all_logits, _, all_hidden = model(last, past=perturbed_past)\\n        hidden = all_hidden[-1]\\n        new_accumulated_hidden = accumulated_hidden + torch.sum(hidden, dim=1).detach()\\n        # TODO: Check the layer-norm consistency of this with trained discriminator (Sumanth)\\n        logits = all_logits[:, -1, :]\\n        probs = F.softmax(logits, dim=-1)\\n\\n        loss = 0.0\\n        loss_list = []\\n        if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\\n            for one_hot_bow in one_hot_bows_vectors:\\n                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\\n                bow_loss = -torch.log(torch.sum(bow_logits))\\n                loss += bow_loss\\n                loss_list.append(bow_loss)\\n            print(\\\" pplm_bow_loss:\\\", loss.data.cpu().numpy())\\n\\n        if loss_type == 2 or loss_type == 3:\\n            ce_loss = torch.nn.CrossEntropyLoss()\\n            # TODO why we need to do this assignment and not just using unpert_past? (Sumanth)\\n            curr_unpert_past = unpert_past\\n            curr_probs = torch.unsqueeze(probs, dim=1)\\n            wte = model.resize_token_embeddings()\\n            for _ in range(horizon_length):\\n                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\\n                _, curr_unpert_past, curr_all_hidden = model(\\n                    past=curr_unpert_past, inputs_embeds=inputs_embeds\\n                )\\n                curr_hidden = curr_all_hidden[-1]\\n                new_accumulated_hidden = new_accumulated_hidden + torch.sum(\\n                    curr_hidden, dim=1\\n                )\\n\\n            prediction = classifier(\\n                new_accumulated_hidden / (curr_length + 1 + horizon_length)\\n            )\\n\\n            label = torch.tensor(\\n                prediction.shape[0] * [class_label], device=device, dtype=torch.long\\n            )\\n            discrim_loss = ce_loss(prediction, label)\\n            print(\\\" pplm_discrim_loss:\\\", discrim_loss.data.cpu().numpy())\\n            loss += discrim_loss\\n            loss_list.append(discrim_loss)\\n\\n        kl_loss = 0.0\\n        if kl_scale > 0.0:\\n            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\\n            unpert_probs = (\\n                unpert_probs\\n                + SMALL_CONST\\n                * (unpert_probs <= SMALL_CONST).float().to(device).detach()\\n            )\\n            correction = (\\n                SMALL_CONST * (probs <= SMALL_CONST).float().to(device).detach()\\n            )\\n            corrected_probs = probs + correction.detach()\\n            kl_loss = kl_scale * (\\n                (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\\n            )\\n            print(\\\" kl_loss\\\", kl_loss.data.cpu().numpy())\\n            loss += kl_loss\\n\\n        loss_per_iter.append(loss.data.cpu().numpy())\\n        print(\\\" pplm_loss\\\", (loss - kl_loss).data.cpu().numpy())\\n\\n        # compute gradients\\n        loss.backward()\\n\\n        # calculate gradient norms\\n        if grad_norms is not None and loss_type == PPLM_BOW:\\n            grad_norms = [\\n                torch.max(grad_norms[index], torch.norm(p_.grad * window_mask))\\n                for index, p_ in enumerate(curr_perturbation)\\n            ]\\n        else:\\n            grad_norms = [\\n                (torch.norm(p_.grad * window_mask) + SMALL_CONST)\\n                for index, p_ in enumerate(curr_perturbation)\\n            ]\\n\\n        # normalize gradients\\n        grad = [\\n            -stepsize\\n            * (p_.grad * window_mask / grad_norms[index] ** gamma).data.cpu().numpy()\\n            for index, p_ in enumerate(curr_perturbation)\\n        ]\\n\\n        # accumulate gradient\\n        grad_accumulator = list(map(add, grad, grad_accumulator))\\n\\n        # reset gradients, just to make sure\\n        for p_ in curr_perturbation:\\n            p_.grad.data.zero_()\\n\\n        # removing past from the graph\\n        new_past = []\\n        for p_ in past:\\n            new_past.append(p_.detach())\\n        past = new_past\\n\\n    # apply the accumulated perturbations to the past\\n    grad_accumulator = [\\n        to_var(torch.from_numpy(p_), requires_grad=True, device=device)\\n        for p_ in grad_accumulator\\n    ]\\n    pert_past = list(map(add, past, grad_accumulator))\\n\\n    return pert_past, new_accumulated_hidden, grad_norms, loss_per_iter\";\n",
       "                var nbb_formatted_code = \"def perturb_past(\\n    past,\\n    model,\\n    last,\\n    unpert_past=None,\\n    unpert_logits=None,\\n    accumulated_hidden=None,\\n    grad_norms=None,\\n    stepsize=0.01,\\n    one_hot_bows_vectors=None,\\n    classifier=None,\\n    class_label=None,\\n    loss_type=0,\\n    num_iterations=3,\\n    horizon_length=1,\\n    window_length=0,\\n    decay=False,\\n    gamma=1.5,\\n    kl_scale=0.01,\\n    device=\\\"cuda\\\",\\n):\\n    # Generate inital perturbed past\\n    grad_accumulator = [(np.zeros(p.shape).astype(\\\"float32\\\")) for p in past]\\n\\n    if accumulated_hidden is None:\\n        accumulated_hidden = 0\\n\\n    if decay:\\n        decay_mask = torch.arange(0.0, 1.0 + SMALL_CONST, 1.0 / (window_length))[1:]\\n    else:\\n        decay_mask = 1.0\\n\\n    # TODO fix this comment (SUMANTH)\\n    # Generate a mask is gradient perturbated is based on a past window\\n    _, _, _, curr_length, _ = past[0].shape\\n\\n    if curr_length > window_length and window_length > 0:\\n        ones_key_val_shape = (\\n            tuple(past[0].shape[:-2])\\n            + tuple([window_length])\\n            + tuple(past[0].shape[-1:])\\n        )\\n\\n        zeros_key_val_shape = (\\n            tuple(past[0].shape[:-2])\\n            + tuple([curr_length - window_length])\\n            + tuple(past[0].shape[-1:])\\n        )\\n\\n        ones_mask = torch.ones(ones_key_val_shape)\\n        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\\n        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\\n\\n        window_mask = torch.cat(\\n            (ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2\\n        ).to(device)\\n    else:\\n        window_mask = torch.ones_like(past[0]).to(device)\\n\\n    # accumulate perturbations for num_iterations\\n    loss_per_iter = []\\n    new_accumulated_hidden = None\\n    for i in range(num_iterations):\\n        print(\\\"Iteration \\\", i + 1)\\n        curr_perturbation = [\\n            to_var(torch.from_numpy(p_), requires_grad=True, device=device)\\n            for p_ in grad_accumulator\\n        ]\\n\\n        # Compute hidden using perturbed past\\n        perturbed_past = list(map(add, past, curr_perturbation))\\n        _, _, _, curr_length, _ = curr_perturbation[0].shape\\n        all_logits, _, all_hidden = model(last, past=perturbed_past)\\n        hidden = all_hidden[-1]\\n        new_accumulated_hidden = accumulated_hidden + torch.sum(hidden, dim=1).detach()\\n        # TODO: Check the layer-norm consistency of this with trained discriminator (Sumanth)\\n        logits = all_logits[:, -1, :]\\n        probs = F.softmax(logits, dim=-1)\\n\\n        loss = 0.0\\n        loss_list = []\\n        if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\\n            for one_hot_bow in one_hot_bows_vectors:\\n                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\\n                bow_loss = -torch.log(torch.sum(bow_logits))\\n                loss += bow_loss\\n                loss_list.append(bow_loss)\\n            print(\\\" pplm_bow_loss:\\\", loss.data.cpu().numpy())\\n\\n        if loss_type == 2 or loss_type == 3:\\n            ce_loss = torch.nn.CrossEntropyLoss()\\n            # TODO why we need to do this assignment and not just using unpert_past? (Sumanth)\\n            curr_unpert_past = unpert_past\\n            curr_probs = torch.unsqueeze(probs, dim=1)\\n            wte = model.resize_token_embeddings()\\n            for _ in range(horizon_length):\\n                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\\n                _, curr_unpert_past, curr_all_hidden = model(\\n                    past=curr_unpert_past, inputs_embeds=inputs_embeds\\n                )\\n                curr_hidden = curr_all_hidden[-1]\\n                new_accumulated_hidden = new_accumulated_hidden + torch.sum(\\n                    curr_hidden, dim=1\\n                )\\n\\n            prediction = classifier(\\n                new_accumulated_hidden / (curr_length + 1 + horizon_length)\\n            )\\n\\n            label = torch.tensor(\\n                prediction.shape[0] * [class_label], device=device, dtype=torch.long\\n            )\\n            discrim_loss = ce_loss(prediction, label)\\n            print(\\\" pplm_discrim_loss:\\\", discrim_loss.data.cpu().numpy())\\n            loss += discrim_loss\\n            loss_list.append(discrim_loss)\\n\\n        kl_loss = 0.0\\n        if kl_scale > 0.0:\\n            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\\n            unpert_probs = (\\n                unpert_probs\\n                + SMALL_CONST\\n                * (unpert_probs <= SMALL_CONST).float().to(device).detach()\\n            )\\n            correction = (\\n                SMALL_CONST * (probs <= SMALL_CONST).float().to(device).detach()\\n            )\\n            corrected_probs = probs + correction.detach()\\n            kl_loss = kl_scale * (\\n                (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\\n            )\\n            print(\\\" kl_loss\\\", kl_loss.data.cpu().numpy())\\n            loss += kl_loss\\n\\n        loss_per_iter.append(loss.data.cpu().numpy())\\n        print(\\\" pplm_loss\\\", (loss - kl_loss).data.cpu().numpy())\\n\\n        # compute gradients\\n        loss.backward()\\n\\n        # calculate gradient norms\\n        if grad_norms is not None and loss_type == PPLM_BOW:\\n            grad_norms = [\\n                torch.max(grad_norms[index], torch.norm(p_.grad * window_mask))\\n                for index, p_ in enumerate(curr_perturbation)\\n            ]\\n        else:\\n            grad_norms = [\\n                (torch.norm(p_.grad * window_mask) + SMALL_CONST)\\n                for index, p_ in enumerate(curr_perturbation)\\n            ]\\n\\n        # normalize gradients\\n        grad = [\\n            -stepsize\\n            * (p_.grad * window_mask / grad_norms[index] ** gamma).data.cpu().numpy()\\n            for index, p_ in enumerate(curr_perturbation)\\n        ]\\n\\n        # accumulate gradient\\n        grad_accumulator = list(map(add, grad, grad_accumulator))\\n\\n        # reset gradients, just to make sure\\n        for p_ in curr_perturbation:\\n            p_.grad.data.zero_()\\n\\n        # removing past from the graph\\n        new_past = []\\n        for p_ in past:\\n            new_past.append(p_.detach())\\n        past = new_past\\n\\n    # apply the accumulated perturbations to the past\\n    grad_accumulator = [\\n        to_var(torch.from_numpy(p_), requires_grad=True, device=device)\\n        for p_ in grad_accumulator\\n    ]\\n    pert_past = list(map(add, past, grad_accumulator))\\n\\n    return pert_past, new_accumulated_hidden, grad_norms, loss_per_iter\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def perturb_past(\n",
    "    past,\n",
    "    model,\n",
    "    last,\n",
    "    unpert_past=None,\n",
    "    unpert_logits=None,\n",
    "    accumulated_hidden=None,\n",
    "    grad_norms=None,\n",
    "    stepsize=0.01,\n",
    "    one_hot_bows_vectors=None,\n",
    "    classifier=None,\n",
    "    class_label=None,\n",
    "    loss_type=0,\n",
    "    num_iterations=3,\n",
    "    horizon_length=1,\n",
    "    window_length=0,\n",
    "    decay=False,\n",
    "    gamma=1.5,\n",
    "    kl_scale=0.01,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    # Generate inital perturbed past\n",
    "    grad_accumulator = [(np.zeros(p.shape).astype(\"float32\")) for p in past]\n",
    "\n",
    "    if accumulated_hidden is None:\n",
    "        accumulated_hidden = 0\n",
    "\n",
    "    if decay:\n",
    "        decay_mask = torch.arange(0.0, 1.0 + SMALL_CONST, 1.0 / (window_length))[1:]\n",
    "    else:\n",
    "        decay_mask = 1.0\n",
    "\n",
    "    # TODO fix this comment (SUMANTH)\n",
    "    # Generate a mask is gradient perturbated is based on a past window\n",
    "    _, _, _, curr_length, _ = past[0].shape\n",
    "\n",
    "    if curr_length > window_length and window_length > 0:\n",
    "        ones_key_val_shape = (\n",
    "            tuple(past[0].shape[:-2])\n",
    "            + tuple([window_length])\n",
    "            + tuple(past[0].shape[-1:])\n",
    "        )\n",
    "\n",
    "        zeros_key_val_shape = (\n",
    "            tuple(past[0].shape[:-2])\n",
    "            + tuple([curr_length - window_length])\n",
    "            + tuple(past[0].shape[-1:])\n",
    "        )\n",
    "\n",
    "        ones_mask = torch.ones(ones_key_val_shape)\n",
    "        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n",
    "        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n",
    "\n",
    "        window_mask = torch.cat(\n",
    "            (ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2\n",
    "        ).to(device)\n",
    "    else:\n",
    "        window_mask = torch.ones_like(past[0]).to(device)\n",
    "\n",
    "    # accumulate perturbations for num_iterations\n",
    "    loss_per_iter = []\n",
    "    new_accumulated_hidden = None\n",
    "    for i in range(num_iterations):\n",
    "        print(\"Iteration \", i + 1)\n",
    "        curr_perturbation = [\n",
    "            to_var(torch.from_numpy(p_), requires_grad=True, device=device)\n",
    "            for p_ in grad_accumulator\n",
    "        ]\n",
    "\n",
    "        # Compute hidden using perturbed past\n",
    "        perturbed_past = list(map(add, past, curr_perturbation))\n",
    "        _, _, _, curr_length, _ = curr_perturbation[0].shape\n",
    "        all_logits, _, all_hidden = model(last, past=perturbed_past)\n",
    "        hidden = all_hidden[-1]\n",
    "        new_accumulated_hidden = accumulated_hidden + torch.sum(hidden, dim=1).detach()\n",
    "        # TODO: Check the layer-norm consistency of this with trained discriminator (Sumanth)\n",
    "        logits = all_logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        loss = 0.0\n",
    "        loss_list = []\n",
    "        if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\n",
    "            for one_hot_bow in one_hot_bows_vectors:\n",
    "                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n",
    "                bow_loss = -torch.log(torch.sum(bow_logits))\n",
    "                loss += bow_loss\n",
    "                loss_list.append(bow_loss)\n",
    "            print(\" pplm_bow_loss:\", loss.data.cpu().numpy())\n",
    "\n",
    "        if loss_type == 2 or loss_type == 3:\n",
    "            ce_loss = torch.nn.CrossEntropyLoss()\n",
    "            # TODO why we need to do this assignment and not just using unpert_past? (Sumanth)\n",
    "            curr_unpert_past = unpert_past\n",
    "            curr_probs = torch.unsqueeze(probs, dim=1)\n",
    "            wte = model.resize_token_embeddings()\n",
    "            for _ in range(horizon_length):\n",
    "                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\n",
    "                _, curr_unpert_past, curr_all_hidden = model(\n",
    "                    past=curr_unpert_past, inputs_embeds=inputs_embeds\n",
    "                )\n",
    "                curr_hidden = curr_all_hidden[-1]\n",
    "                new_accumulated_hidden = new_accumulated_hidden + torch.sum(\n",
    "                    curr_hidden, dim=1\n",
    "                )\n",
    "\n",
    "            prediction = classifier(\n",
    "                new_accumulated_hidden / (curr_length + 1 + horizon_length)\n",
    "            )\n",
    "\n",
    "            label = torch.tensor(\n",
    "                prediction.shape[0] * [class_label], device=device, dtype=torch.long\n",
    "            )\n",
    "            discrim_loss = ce_loss(prediction, label)\n",
    "            print(\" pplm_discrim_loss:\", discrim_loss.data.cpu().numpy())\n",
    "            loss += discrim_loss\n",
    "            loss_list.append(discrim_loss)\n",
    "\n",
    "        kl_loss = 0.0\n",
    "        if kl_scale > 0.0:\n",
    "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
    "            unpert_probs = (\n",
    "                unpert_probs\n",
    "                + SMALL_CONST\n",
    "                * (unpert_probs <= SMALL_CONST).float().to(device).detach()\n",
    "            )\n",
    "            correction = (\n",
    "                SMALL_CONST * (probs <= SMALL_CONST).float().to(device).detach()\n",
    "            )\n",
    "            corrected_probs = probs + correction.detach()\n",
    "            kl_loss = kl_scale * (\n",
    "                (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\n",
    "            )\n",
    "            print(\" kl_loss\", kl_loss.data.cpu().numpy())\n",
    "            loss += kl_loss\n",
    "\n",
    "        loss_per_iter.append(loss.data.cpu().numpy())\n",
    "        print(\" pplm_loss\", (loss - kl_loss).data.cpu().numpy())\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # calculate gradient norms\n",
    "        if grad_norms is not None and loss_type == PPLM_BOW:\n",
    "            grad_norms = [\n",
    "                torch.max(grad_norms[index], torch.norm(p_.grad * window_mask))\n",
    "                for index, p_ in enumerate(curr_perturbation)\n",
    "            ]\n",
    "        else:\n",
    "            grad_norms = [\n",
    "                (torch.norm(p_.grad * window_mask) + SMALL_CONST)\n",
    "                for index, p_ in enumerate(curr_perturbation)\n",
    "            ]\n",
    "\n",
    "        # normalize gradients\n",
    "        grad = [\n",
    "            -stepsize\n",
    "            * (p_.grad * window_mask / grad_norms[index] ** gamma).data.cpu().numpy()\n",
    "            for index, p_ in enumerate(curr_perturbation)\n",
    "        ]\n",
    "\n",
    "        # accumulate gradient\n",
    "        grad_accumulator = list(map(add, grad, grad_accumulator))\n",
    "\n",
    "        # reset gradients, just to make sure\n",
    "        for p_ in curr_perturbation:\n",
    "            p_.grad.data.zero_()\n",
    "\n",
    "        # removing past from the graph\n",
    "        new_past = []\n",
    "        for p_ in past:\n",
    "            new_past.append(p_.detach())\n",
    "        past = new_past\n",
    "\n",
    "    # apply the accumulated perturbations to the past\n",
    "    grad_accumulator = [\n",
    "        to_var(torch.from_numpy(p_), requires_grad=True, device=device)\n",
    "        for p_ in grad_accumulator\n",
    "    ]\n",
    "    pert_past = list(map(add, past, grad_accumulator))\n",
    "\n",
    "    return pert_past, new_accumulated_hidden, grad_norms, loss_per_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"class ClassificationHead(torch.nn.Module):\\n    \\\"\\\"\\\"Classification Head for  transformer encoders\\\"\\\"\\\"\\n\\n    def __init__(self, class_size, embed_size):\\n        super().__init__()\\n        self.class_size = class_size\\n        self.embed_size = embed_size\\n        self.mlp = torch.nn.Linear(embed_size, class_size)\\n\\n    def forward(self, hidden_state):\\n        logits = self.mlp(hidden_state)\\n        return logits\";\n",
       "                var nbb_formatted_code = \"class ClassificationHead(torch.nn.Module):\\n    \\\"\\\"\\\"Classification Head for  transformer encoders\\\"\\\"\\\"\\n\\n    def __init__(self, class_size, embed_size):\\n        super().__init__()\\n        self.class_size = class_size\\n        self.embed_size = embed_size\\n        self.mlp = torch.nn.Linear(embed_size, class_size)\\n\\n    def forward(self, hidden_state):\\n        logits = self.mlp(hidden_state)\\n        return logits\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ClassificationHead(torch.nn.Module):\n",
    "    \"\"\"Classification Head for  transformer encoders\"\"\"\n",
    "\n",
    "    def __init__(self, class_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.class_size = class_size\n",
    "        self.embed_size = embed_size\n",
    "        self.mlp = torch.nn.Linear(embed_size, class_size)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        logits = self.mlp(hidden_state)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"def get_classifier(\\n    name: Optional[str], class_label: Union[str, int], device: str\\n) -> Tuple[Optional[ClassificationHead], Optional[int]]:\\n    if name is None:\\n        return None, None\\n\\n    params = DISCRIMINATOR_MODELS_PARAMS[name]\\n    classifier = ClassificationHead(\\n        class_size=params[\\\"class_size\\\"], embed_size=params[\\\"embed_size\\\"]\\n    ).to(device)\\n    if \\\"url\\\" in params:\\n        resolved_archive_file = cached_path(params[\\\"url\\\"])\\n    elif \\\"path\\\" in params:\\n        resolved_archive_file = params[\\\"path\\\"]\\n    else:\\n        raise ValueError(\\n            \\\"Either url or path have to be specified \\\"\\n            \\\"in the discriminator model parameters\\\"\\n        )\\n    classifier.load_state_dict(torch.load(resolved_archive_file, map_location=device))\\n    classifier.eval()\\n\\n    if isinstance(class_label, str):\\n        if class_label in params[\\\"class_vocab\\\"]:\\n            label_id = params[\\\"class_vocab\\\"][class_label]\\n        else:\\n            label_id = params[\\\"default_class\\\"]\\n            print(\\\"class_label {} not in class_vocab\\\".format(class_label))\\n            print(\\\"available values are: {}\\\".format(params[\\\"class_vocab\\\"]))\\n            print(\\\"using default class {}\\\".format(label_id))\\n\\n    elif isinstance(class_label, int):\\n        if class_label in set(params[\\\"class_vocab\\\"].values()):\\n            label_id = class_label\\n        else:\\n            label_id = params[\\\"default_class\\\"]\\n            print(\\\"class_label {} not in class_vocab\\\".format(class_label))\\n            print(\\\"available values are: {}\\\".format(params[\\\"class_vocab\\\"]))\\n            print(\\\"using default class {}\\\".format(label_id))\\n\\n    else:\\n        label_id = params[\\\"default_class\\\"]\\n\\n    return classifier, label_id\";\n",
       "                var nbb_formatted_code = \"def get_classifier(\\n    name: Optional[str], class_label: Union[str, int], device: str\\n) -> Tuple[Optional[ClassificationHead], Optional[int]]:\\n    if name is None:\\n        return None, None\\n\\n    params = DISCRIMINATOR_MODELS_PARAMS[name]\\n    classifier = ClassificationHead(\\n        class_size=params[\\\"class_size\\\"], embed_size=params[\\\"embed_size\\\"]\\n    ).to(device)\\n    if \\\"url\\\" in params:\\n        resolved_archive_file = cached_path(params[\\\"url\\\"])\\n    elif \\\"path\\\" in params:\\n        resolved_archive_file = params[\\\"path\\\"]\\n    else:\\n        raise ValueError(\\n            \\\"Either url or path have to be specified \\\"\\n            \\\"in the discriminator model parameters\\\"\\n        )\\n    classifier.load_state_dict(torch.load(resolved_archive_file, map_location=device))\\n    classifier.eval()\\n\\n    if isinstance(class_label, str):\\n        if class_label in params[\\\"class_vocab\\\"]:\\n            label_id = params[\\\"class_vocab\\\"][class_label]\\n        else:\\n            label_id = params[\\\"default_class\\\"]\\n            print(\\\"class_label {} not in class_vocab\\\".format(class_label))\\n            print(\\\"available values are: {}\\\".format(params[\\\"class_vocab\\\"]))\\n            print(\\\"using default class {}\\\".format(label_id))\\n\\n    elif isinstance(class_label, int):\\n        if class_label in set(params[\\\"class_vocab\\\"].values()):\\n            label_id = class_label\\n        else:\\n            label_id = params[\\\"default_class\\\"]\\n            print(\\\"class_label {} not in class_vocab\\\".format(class_label))\\n            print(\\\"available values are: {}\\\".format(params[\\\"class_vocab\\\"]))\\n            print(\\\"using default class {}\\\".format(label_id))\\n\\n    else:\\n        label_id = params[\\\"default_class\\\"]\\n\\n    return classifier, label_id\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_classifier(\n",
    "    name: Optional[str], class_label: Union[str, int], device: str\n",
    ") -> Tuple[Optional[ClassificationHead], Optional[int]]:\n",
    "    if name is None:\n",
    "        return None, None\n",
    "\n",
    "    params = DISCRIMINATOR_MODELS_PARAMS[name]\n",
    "    classifier = ClassificationHead(\n",
    "        class_size=params[\"class_size\"], embed_size=params[\"embed_size\"]\n",
    "    ).to(device)\n",
    "    if \"url\" in params:\n",
    "        resolved_archive_file = cached_path(params[\"url\"])\n",
    "    elif \"path\" in params:\n",
    "        resolved_archive_file = params[\"path\"]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Either url or path have to be specified \"\n",
    "            \"in the discriminator model parameters\"\n",
    "        )\n",
    "    classifier.load_state_dict(torch.load(resolved_archive_file, map_location=device))\n",
    "    classifier.eval()\n",
    "\n",
    "    if isinstance(class_label, str):\n",
    "        if class_label in params[\"class_vocab\"]:\n",
    "            label_id = params[\"class_vocab\"][class_label]\n",
    "        else:\n",
    "            label_id = params[\"default_class\"]\n",
    "            print(\"class_label {} not in class_vocab\".format(class_label))\n",
    "            print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
    "            print(\"using default class {}\".format(label_id))\n",
    "\n",
    "    elif isinstance(class_label, int):\n",
    "        if class_label in set(params[\"class_vocab\"].values()):\n",
    "            label_id = class_label\n",
    "        else:\n",
    "            label_id = params[\"default_class\"]\n",
    "            print(\"class_label {} not in class_vocab\".format(class_label))\n",
    "            print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
    "            print(\"using default class {}\".format(label_id))\n",
    "\n",
    "    else:\n",
    "        label_id = params[\"default_class\"]\n",
    "\n",
    "    return classifier, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"def get_bag_of_words_indices(\\n    bag_of_words_ids_or_paths: List[str], tokenizer\\n) -> List[List[List[int]]]:\\n    bow_indices = []\\n    for id_or_path in bag_of_words_ids_or_paths:\\n        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\\n            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\\n        else:\\n            filepath = id_or_path\\n        with open(filepath, \\\"r\\\") as f:\\n            words = f.read().strip().split(\\\"\\\\n\\\")\\n        bow_indices.append(\\n            [tokenizer.encode(word.strip(), add_prefix_space=True) for word in words]\\n        )\\n    return bow_indices\";\n",
       "                var nbb_formatted_code = \"def get_bag_of_words_indices(\\n    bag_of_words_ids_or_paths: List[str], tokenizer\\n) -> List[List[List[int]]]:\\n    bow_indices = []\\n    for id_or_path in bag_of_words_ids_or_paths:\\n        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\\n            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\\n        else:\\n            filepath = id_or_path\\n        with open(filepath, \\\"r\\\") as f:\\n            words = f.read().strip().split(\\\"\\\\n\\\")\\n        bow_indices.append(\\n            [tokenizer.encode(word.strip(), add_prefix_space=True) for word in words]\\n        )\\n    return bow_indices\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_bag_of_words_indices(\n",
    "    bag_of_words_ids_or_paths: List[str], tokenizer\n",
    ") -> List[List[List[int]]]:\n",
    "    bow_indices = []\n",
    "    for id_or_path in bag_of_words_ids_or_paths:\n",
    "        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n",
    "            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n",
    "        else:\n",
    "            filepath = id_or_path\n",
    "        with open(filepath, \"r\") as f:\n",
    "            words = f.read().strip().split(\"\\n\")\n",
    "        bow_indices.append(\n",
    "            [tokenizer.encode(word.strip(), add_prefix_space=True) for word in words]\n",
    "        )\n",
    "    return bow_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"def build_bows_one_hot_vectors(bow_indices, tokenizer, device=\\\"cuda\\\"):\\n    if bow_indices is None:\\n        return None\\n\\n    one_hot_bows_vectors = []\\n    for single_bow in bow_indices:\\n        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\\n        single_bow = torch.tensor(single_bow).to(device)\\n        num_words = single_bow.shape[0]\\n        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\\n        one_hot_bow.scatter_(1, single_bow, 1)\\n        one_hot_bows_vectors.append(one_hot_bow)\\n    return one_hot_bows_vectors\";\n",
       "                var nbb_formatted_code = \"def build_bows_one_hot_vectors(bow_indices, tokenizer, device=\\\"cuda\\\"):\\n    if bow_indices is None:\\n        return None\\n\\n    one_hot_bows_vectors = []\\n    for single_bow in bow_indices:\\n        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\\n        single_bow = torch.tensor(single_bow).to(device)\\n        num_words = single_bow.shape[0]\\n        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\\n        one_hot_bow.scatter_(1, single_bow, 1)\\n        one_hot_bows_vectors.append(one_hot_bow)\\n    return one_hot_bows_vectors\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_bows_one_hot_vectors(bow_indices, tokenizer, device=\"cuda\"):\n",
    "    if bow_indices is None:\n",
    "        return None\n",
    "\n",
    "    one_hot_bows_vectors = []\n",
    "    for single_bow in bow_indices:\n",
    "        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n",
    "        single_bow = torch.tensor(single_bow).to(device)\n",
    "        num_words = single_bow.shape[0]\n",
    "        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n",
    "        one_hot_bow.scatter_(1, single_bow, 1)\n",
    "        one_hot_bows_vectors.append(one_hot_bow)\n",
    "    return one_hot_bows_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"def full_text_generation(\\n    model,\\n    tokenizer,\\n    context=None,\\n    num_samples=1,\\n    device=\\\"cuda\\\",\\n    bag_of_words=None,\\n    discrim=None,\\n    class_label=None,\\n    length=100,\\n    stepsize=0.02,\\n    temperature=1.0,\\n    top_k=10,\\n    sample=False,\\n    num_iterations=3,\\n    grad_length=10000,\\n    horizon_length=1,\\n    window_length=0,\\n    decay=False,\\n    gamma=1.5,\\n    gm_scale=0.9,\\n    kl_scale=0.01,\\n    repetition_penalty=1.0,\\n    **kwargs\\n):\\n    classifier, class_id = get_classifier(discrim, class_label, device)\\n\\n    bow_indices = []\\n    if bag_of_words:\\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(\\\";\\\"), tokenizer)\\n\\n    if bag_of_words and classifier:\\n        print(\\\"Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.\\\")\\n        loss_type = PPLM_BOW_DISCRIM\\n\\n    elif bag_of_words:\\n        loss_type = PPLM_BOW\\n        print(\\\"Using PPLM-BoW\\\")\\n\\n    elif classifier is not None:\\n        loss_type = PPLM_DISCRIM\\n        print(\\\"Using PPLM-Discrim\\\")\\n\\n    else:\\n        raise Exception(\\\"Specify either a bag of words or a discriminator\\\")\\n\\n    unpert_gen_tok_text, _, _ = generate_text_pplm(\\n        model=model,\\n        tokenizer=tokenizer,\\n        context=context,\\n        device=device,\\n        length=length,\\n        sample=sample,\\n        perturb=False,\\n        repetition_penalty=repetition_penalty,\\n    )\\n    if device == \\\"cuda\\\":\\n        torch.cuda.empty_cache()\\n\\n    pert_gen_tok_texts = []\\n    discrim_losses = []\\n    losses_in_time = []\\n\\n    for i in range(num_samples):\\n        pert_gen_tok_text, discrim_loss, loss_in_time = generate_text_pplm(\\n            model=model,\\n            tokenizer=tokenizer,\\n            context=context,\\n            device=device,\\n            perturb=True,\\n            bow_indices=bow_indices,\\n            classifier=classifier,\\n            class_label=class_id,\\n            loss_type=loss_type,\\n            length=length,\\n            stepsize=stepsize,\\n            temperature=temperature,\\n            top_k=top_k,\\n            sample=sample,\\n            num_iterations=num_iterations,\\n            grad_length=grad_length,\\n            horizon_length=horizon_length,\\n            window_length=window_length,\\n            decay=decay,\\n            gamma=gamma,\\n            gm_scale=gm_scale,\\n            kl_scale=kl_scale,\\n            repetition_penalty=repetition_penalty,\\n        )\\n        pert_gen_tok_texts.append(pert_gen_tok_text)\\n        if classifier is not None:\\n            discrim_losses.append(discrim_loss.data.cpu().numpy())\\n        losses_in_time.append(loss_in_time)\\n\\n    if device == \\\"cuda\\\":\\n        torch.cuda.empty_cache()\\n\\n    return unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\";\n",
       "                var nbb_formatted_code = \"def full_text_generation(\\n    model,\\n    tokenizer,\\n    context=None,\\n    num_samples=1,\\n    device=\\\"cuda\\\",\\n    bag_of_words=None,\\n    discrim=None,\\n    class_label=None,\\n    length=100,\\n    stepsize=0.02,\\n    temperature=1.0,\\n    top_k=10,\\n    sample=False,\\n    num_iterations=3,\\n    grad_length=10000,\\n    horizon_length=1,\\n    window_length=0,\\n    decay=False,\\n    gamma=1.5,\\n    gm_scale=0.9,\\n    kl_scale=0.01,\\n    repetition_penalty=1.0,\\n    **kwargs\\n):\\n    classifier, class_id = get_classifier(discrim, class_label, device)\\n\\n    bow_indices = []\\n    if bag_of_words:\\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(\\\";\\\"), tokenizer)\\n\\n    if bag_of_words and classifier:\\n        print(\\\"Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.\\\")\\n        loss_type = PPLM_BOW_DISCRIM\\n\\n    elif bag_of_words:\\n        loss_type = PPLM_BOW\\n        print(\\\"Using PPLM-BoW\\\")\\n\\n    elif classifier is not None:\\n        loss_type = PPLM_DISCRIM\\n        print(\\\"Using PPLM-Discrim\\\")\\n\\n    else:\\n        raise Exception(\\\"Specify either a bag of words or a discriminator\\\")\\n\\n    unpert_gen_tok_text, _, _ = generate_text_pplm(\\n        model=model,\\n        tokenizer=tokenizer,\\n        context=context,\\n        device=device,\\n        length=length,\\n        sample=sample,\\n        perturb=False,\\n        repetition_penalty=repetition_penalty,\\n    )\\n    if device == \\\"cuda\\\":\\n        torch.cuda.empty_cache()\\n\\n    pert_gen_tok_texts = []\\n    discrim_losses = []\\n    losses_in_time = []\\n\\n    for i in range(num_samples):\\n        pert_gen_tok_text, discrim_loss, loss_in_time = generate_text_pplm(\\n            model=model,\\n            tokenizer=tokenizer,\\n            context=context,\\n            device=device,\\n            perturb=True,\\n            bow_indices=bow_indices,\\n            classifier=classifier,\\n            class_label=class_id,\\n            loss_type=loss_type,\\n            length=length,\\n            stepsize=stepsize,\\n            temperature=temperature,\\n            top_k=top_k,\\n            sample=sample,\\n            num_iterations=num_iterations,\\n            grad_length=grad_length,\\n            horizon_length=horizon_length,\\n            window_length=window_length,\\n            decay=decay,\\n            gamma=gamma,\\n            gm_scale=gm_scale,\\n            kl_scale=kl_scale,\\n            repetition_penalty=repetition_penalty,\\n        )\\n        pert_gen_tok_texts.append(pert_gen_tok_text)\\n        if classifier is not None:\\n            discrim_losses.append(discrim_loss.data.cpu().numpy())\\n        losses_in_time.append(loss_in_time)\\n\\n    if device == \\\"cuda\\\":\\n        torch.cuda.empty_cache()\\n\\n    return unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def full_text_generation(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context=None,\n",
    "    num_samples=1,\n",
    "    device=\"cuda\",\n",
    "    bag_of_words=None,\n",
    "    discrim=None,\n",
    "    class_label=None,\n",
    "    length=100,\n",
    "    stepsize=0.02,\n",
    "    temperature=1.0,\n",
    "    top_k=10,\n",
    "    sample=False,\n",
    "    num_iterations=3,\n",
    "    grad_length=10000,\n",
    "    horizon_length=1,\n",
    "    window_length=0,\n",
    "    decay=False,\n",
    "    gamma=1.5,\n",
    "    gm_scale=0.9,\n",
    "    kl_scale=0.01,\n",
    "    repetition_penalty=1.0,\n",
    "    **kwargs\n",
    "):\n",
    "    classifier, class_id = get_classifier(discrim, class_label, device)\n",
    "\n",
    "    bow_indices = []\n",
    "    if bag_of_words:\n",
    "        bow_indices = get_bag_of_words_indices(bag_of_words.split(\";\"), tokenizer)\n",
    "\n",
    "    if bag_of_words and classifier:\n",
    "        print(\"Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.\")\n",
    "        loss_type = PPLM_BOW_DISCRIM\n",
    "\n",
    "    elif bag_of_words:\n",
    "        loss_type = PPLM_BOW\n",
    "        print(\"Using PPLM-BoW\")\n",
    "\n",
    "    elif classifier is not None:\n",
    "        loss_type = PPLM_DISCRIM\n",
    "        print(\"Using PPLM-Discrim\")\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Specify either a bag of words or a discriminator\")\n",
    "\n",
    "    unpert_gen_tok_text, _, _ = generate_text_pplm(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        context=context,\n",
    "        device=device,\n",
    "        length=length,\n",
    "        sample=sample,\n",
    "        perturb=False,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "    )\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    pert_gen_tok_texts = []\n",
    "    discrim_losses = []\n",
    "    losses_in_time = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        pert_gen_tok_text, discrim_loss, loss_in_time = generate_text_pplm(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            context=context,\n",
    "            device=device,\n",
    "            perturb=True,\n",
    "            bow_indices=bow_indices,\n",
    "            classifier=classifier,\n",
    "            class_label=class_id,\n",
    "            loss_type=loss_type,\n",
    "            length=length,\n",
    "            stepsize=stepsize,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            sample=sample,\n",
    "            num_iterations=num_iterations,\n",
    "            grad_length=grad_length,\n",
    "            horizon_length=horizon_length,\n",
    "            window_length=window_length,\n",
    "            decay=decay,\n",
    "            gamma=gamma,\n",
    "            gm_scale=gm_scale,\n",
    "            kl_scale=kl_scale,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "        )\n",
    "        pert_gen_tok_texts.append(pert_gen_tok_text)\n",
    "        if classifier is not None:\n",
    "            discrim_losses.append(discrim_loss.data.cpu().numpy())\n",
    "        losses_in_time.append(loss_in_time)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"def generate_text_pplm(\\n    model,\\n    tokenizer,\\n    context=None,\\n    past=None,\\n    device=\\\"cuda\\\",\\n    perturb=True,\\n    bow_indices=None,\\n    classifier=None,\\n    class_label=None,\\n    loss_type=0,\\n    length=100,\\n    stepsize=0.02,\\n    temperature=1.0,\\n    top_k=10,\\n    sample=False,\\n    num_iterations=3,\\n    grad_length=10000,\\n    horizon_length=1,\\n    window_length=0,\\n    decay=False,\\n    gamma=1.5,\\n    gm_scale=0.9,\\n    kl_scale=0.01,\\n    repetition_penalty=1.0,\\n):\\n    output_so_far = None\\n    if context:\\n        context_t = torch.tensor(context, device=device, dtype=torch.long)\\n        while len(context_t.shape) < 2:\\n            context_t = context_t.unsqueeze(0)\\n        output_so_far = context_t\\n\\n    # collect one hot vectors for bags of words\\n    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\\n\\n    grad_norms = None\\n    last = None\\n    unpert_discrim_loss = 0\\n    loss_in_time = []\\n    for i in trange(length, ascii=True):\\n\\n        # Get past/probs for current output, except for last word\\n        # Note that GPT takes 2 inputs: past + current_token\\n\\n        # run model forward to obtain unperturbed\\n        if past is None and output_so_far is not None:\\n            last = output_so_far[:, -1:]\\n            if output_so_far.shape[1] > 1:\\n                _, past, _ = model(output_so_far[:, :-1])\\n\\n        unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\\n        unpert_last_hidden = unpert_all_hidden[-1]\\n\\n        # check if we are abowe grad max length\\n        if i >= grad_length:\\n            current_stepsize = stepsize * 0\\n        else:\\n            current_stepsize = stepsize\\n\\n        # modify the past if necessary\\n        if not perturb or num_iterations == 0:\\n            pert_past = past\\n\\n        else:\\n            accumulated_hidden = unpert_last_hidden[:, :-1, :]\\n            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\\n\\n            if past is not None:\\n                pert_past, _, grad_norms, loss_this_iter = perturb_past(\\n                    past,\\n                    model,\\n                    last,\\n                    unpert_past=unpert_past,\\n                    unpert_logits=unpert_logits,\\n                    accumulated_hidden=accumulated_hidden,\\n                    grad_norms=grad_norms,\\n                    stepsize=current_stepsize,\\n                    one_hot_bows_vectors=one_hot_bows_vectors,\\n                    classifier=classifier,\\n                    class_label=class_label,\\n                    loss_type=loss_type,\\n                    num_iterations=num_iterations,\\n                    horizon_length=horizon_length,\\n                    window_length=window_length,\\n                    decay=decay,\\n                    gamma=gamma,\\n                    kl_scale=kl_scale,\\n                    device=device,\\n                )\\n                loss_in_time.append(loss_this_iter)\\n            else:\\n                pert_past = past\\n\\n        pert_logits, past, pert_all_hidden = model(last, past=pert_past)\\n        pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\\n\\n        for token_idx in set(output_so_far[0].tolist()):\\n            if pert_logits[0, token_idx] < 0:\\n                pert_logits[0, token_idx] *= repetition_penalty\\n            else:\\n                pert_logits[0, token_idx] /= repetition_penalty\\n\\n        pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n        if classifier is not None:\\n            ce_loss = torch.nn.CrossEntropyLoss()\\n            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\\n            label = torch.tensor([class_label], device=device, dtype=torch.long)\\n            unpert_discrim_loss = ce_loss(prediction, label)\\n            print(\\\"unperturbed discrim loss\\\", unpert_discrim_loss.data.cpu().numpy())\\n        else:\\n            unpert_discrim_loss = 0\\n\\n        # Fuse the modified model and original model\\n        if perturb:\\n\\n            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\\n\\n            pert_probs = (pert_probs ** gm_scale) * (\\n                unpert_probs ** (1 - gm_scale)\\n            )  # + SMALL_CONST\\n            pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)  # + SMALL_CONST\\n\\n            # rescale\\n            if torch.sum(pert_probs) <= 1:\\n                pert_probs = pert_probs / torch.sum(pert_probs)\\n\\n        else:\\n            pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\\n            pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n        # sample or greedy\\n        if sample:\\n            last = torch.multinomial(pert_probs, num_samples=1)\\n\\n        else:\\n            _, last = torch.topk(pert_probs, k=1, dim=-1)\\n\\n        # update context/output_so_far appending the new token\\n        output_so_far = (\\n            last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\\n        )\\n\\n        print(tokenizer.decode(output_so_far.tolist()[0]))\\n\\n    return output_so_far, unpert_discrim_loss, loss_in_time\";\n",
       "                var nbb_formatted_code = \"def generate_text_pplm(\\n    model,\\n    tokenizer,\\n    context=None,\\n    past=None,\\n    device=\\\"cuda\\\",\\n    perturb=True,\\n    bow_indices=None,\\n    classifier=None,\\n    class_label=None,\\n    loss_type=0,\\n    length=100,\\n    stepsize=0.02,\\n    temperature=1.0,\\n    top_k=10,\\n    sample=False,\\n    num_iterations=3,\\n    grad_length=10000,\\n    horizon_length=1,\\n    window_length=0,\\n    decay=False,\\n    gamma=1.5,\\n    gm_scale=0.9,\\n    kl_scale=0.01,\\n    repetition_penalty=1.0,\\n):\\n    output_so_far = None\\n    if context:\\n        context_t = torch.tensor(context, device=device, dtype=torch.long)\\n        while len(context_t.shape) < 2:\\n            context_t = context_t.unsqueeze(0)\\n        output_so_far = context_t\\n\\n    # collect one hot vectors for bags of words\\n    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\\n\\n    grad_norms = None\\n    last = None\\n    unpert_discrim_loss = 0\\n    loss_in_time = []\\n    for i in trange(length, ascii=True):\\n\\n        # Get past/probs for current output, except for last word\\n        # Note that GPT takes 2 inputs: past + current_token\\n\\n        # run model forward to obtain unperturbed\\n        if past is None and output_so_far is not None:\\n            last = output_so_far[:, -1:]\\n            if output_so_far.shape[1] > 1:\\n                _, past, _ = model(output_so_far[:, :-1])\\n\\n        unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\\n        unpert_last_hidden = unpert_all_hidden[-1]\\n\\n        # check if we are abowe grad max length\\n        if i >= grad_length:\\n            current_stepsize = stepsize * 0\\n        else:\\n            current_stepsize = stepsize\\n\\n        # modify the past if necessary\\n        if not perturb or num_iterations == 0:\\n            pert_past = past\\n\\n        else:\\n            accumulated_hidden = unpert_last_hidden[:, :-1, :]\\n            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\\n\\n            if past is not None:\\n                pert_past, _, grad_norms, loss_this_iter = perturb_past(\\n                    past,\\n                    model,\\n                    last,\\n                    unpert_past=unpert_past,\\n                    unpert_logits=unpert_logits,\\n                    accumulated_hidden=accumulated_hidden,\\n                    grad_norms=grad_norms,\\n                    stepsize=current_stepsize,\\n                    one_hot_bows_vectors=one_hot_bows_vectors,\\n                    classifier=classifier,\\n                    class_label=class_label,\\n                    loss_type=loss_type,\\n                    num_iterations=num_iterations,\\n                    horizon_length=horizon_length,\\n                    window_length=window_length,\\n                    decay=decay,\\n                    gamma=gamma,\\n                    kl_scale=kl_scale,\\n                    device=device,\\n                )\\n                loss_in_time.append(loss_this_iter)\\n            else:\\n                pert_past = past\\n\\n        pert_logits, past, pert_all_hidden = model(last, past=pert_past)\\n        pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\\n\\n        for token_idx in set(output_so_far[0].tolist()):\\n            if pert_logits[0, token_idx] < 0:\\n                pert_logits[0, token_idx] *= repetition_penalty\\n            else:\\n                pert_logits[0, token_idx] /= repetition_penalty\\n\\n        pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n        if classifier is not None:\\n            ce_loss = torch.nn.CrossEntropyLoss()\\n            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\\n            label = torch.tensor([class_label], device=device, dtype=torch.long)\\n            unpert_discrim_loss = ce_loss(prediction, label)\\n            print(\\\"unperturbed discrim loss\\\", unpert_discrim_loss.data.cpu().numpy())\\n        else:\\n            unpert_discrim_loss = 0\\n\\n        # Fuse the modified model and original model\\n        if perturb:\\n\\n            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\\n\\n            pert_probs = (pert_probs ** gm_scale) * (\\n                unpert_probs ** (1 - gm_scale)\\n            )  # + SMALL_CONST\\n            pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)  # + SMALL_CONST\\n\\n            # rescale\\n            if torch.sum(pert_probs) <= 1:\\n                pert_probs = pert_probs / torch.sum(pert_probs)\\n\\n        else:\\n            pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\\n            pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n        # sample or greedy\\n        if sample:\\n            last = torch.multinomial(pert_probs, num_samples=1)\\n\\n        else:\\n            _, last = torch.topk(pert_probs, k=1, dim=-1)\\n\\n        # update context/output_so_far appending the new token\\n        output_so_far = (\\n            last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\\n        )\\n\\n        print(tokenizer.decode(output_so_far.tolist()[0]))\\n\\n    return output_so_far, unpert_discrim_loss, loss_in_time\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_text_pplm(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context=None,\n",
    "    past=None,\n",
    "    device=\"cuda\",\n",
    "    perturb=True,\n",
    "    bow_indices=None,\n",
    "    classifier=None,\n",
    "    class_label=None,\n",
    "    loss_type=0,\n",
    "    length=100,\n",
    "    stepsize=0.02,\n",
    "    temperature=1.0,\n",
    "    top_k=10,\n",
    "    sample=False,\n",
    "    num_iterations=3,\n",
    "    grad_length=10000,\n",
    "    horizon_length=1,\n",
    "    window_length=0,\n",
    "    decay=False,\n",
    "    gamma=1.5,\n",
    "    gm_scale=0.9,\n",
    "    kl_scale=0.01,\n",
    "    repetition_penalty=1.0,\n",
    "):\n",
    "    output_so_far = None\n",
    "    if context:\n",
    "        context_t = torch.tensor(context, device=device, dtype=torch.long)\n",
    "        while len(context_t.shape) < 2:\n",
    "            context_t = context_t.unsqueeze(0)\n",
    "        output_so_far = context_t\n",
    "\n",
    "    # collect one hot vectors for bags of words\n",
    "    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\n",
    "\n",
    "    grad_norms = None\n",
    "    last = None\n",
    "    unpert_discrim_loss = 0\n",
    "    loss_in_time = []\n",
    "    for i in trange(length, ascii=True):\n",
    "\n",
    "        # Get past/probs for current output, except for last word\n",
    "        # Note that GPT takes 2 inputs: past + current_token\n",
    "\n",
    "        # run model forward to obtain unperturbed\n",
    "        if past is None and output_so_far is not None:\n",
    "            last = output_so_far[:, -1:]\n",
    "            if output_so_far.shape[1] > 1:\n",
    "                _, past, _ = model(output_so_far[:, :-1])\n",
    "\n",
    "        unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\n",
    "        unpert_last_hidden = unpert_all_hidden[-1]\n",
    "\n",
    "        # check if we are abowe grad max length\n",
    "        if i >= grad_length:\n",
    "            current_stepsize = stepsize * 0\n",
    "        else:\n",
    "            current_stepsize = stepsize\n",
    "\n",
    "        # modify the past if necessary\n",
    "        if not perturb or num_iterations == 0:\n",
    "            pert_past = past\n",
    "\n",
    "        else:\n",
    "            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n",
    "            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n",
    "\n",
    "            if past is not None:\n",
    "                pert_past, _, grad_norms, loss_this_iter = perturb_past(\n",
    "                    past,\n",
    "                    model,\n",
    "                    last,\n",
    "                    unpert_past=unpert_past,\n",
    "                    unpert_logits=unpert_logits,\n",
    "                    accumulated_hidden=accumulated_hidden,\n",
    "                    grad_norms=grad_norms,\n",
    "                    stepsize=current_stepsize,\n",
    "                    one_hot_bows_vectors=one_hot_bows_vectors,\n",
    "                    classifier=classifier,\n",
    "                    class_label=class_label,\n",
    "                    loss_type=loss_type,\n",
    "                    num_iterations=num_iterations,\n",
    "                    horizon_length=horizon_length,\n",
    "                    window_length=window_length,\n",
    "                    decay=decay,\n",
    "                    gamma=gamma,\n",
    "                    kl_scale=kl_scale,\n",
    "                    device=device,\n",
    "                )\n",
    "                loss_in_time.append(loss_this_iter)\n",
    "            else:\n",
    "                pert_past = past\n",
    "\n",
    "        pert_logits, past, pert_all_hidden = model(last, past=pert_past)\n",
    "        pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\n",
    "\n",
    "        for token_idx in set(output_so_far[0].tolist()):\n",
    "            if pert_logits[0, token_idx] < 0:\n",
    "                pert_logits[0, token_idx] *= repetition_penalty\n",
    "            else:\n",
    "                pert_logits[0, token_idx] /= repetition_penalty\n",
    "\n",
    "        pert_probs = F.softmax(pert_logits, dim=-1)\n",
    "\n",
    "        if classifier is not None:\n",
    "            ce_loss = torch.nn.CrossEntropyLoss()\n",
    "            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n",
    "            label = torch.tensor([class_label], device=device, dtype=torch.long)\n",
    "            unpert_discrim_loss = ce_loss(prediction, label)\n",
    "            print(\"unperturbed discrim loss\", unpert_discrim_loss.data.cpu().numpy())\n",
    "        else:\n",
    "            unpert_discrim_loss = 0\n",
    "\n",
    "        # Fuse the modified model and original model\n",
    "        if perturb:\n",
    "\n",
    "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
    "\n",
    "            pert_probs = (pert_probs ** gm_scale) * (\n",
    "                unpert_probs ** (1 - gm_scale)\n",
    "            )  # + SMALL_CONST\n",
    "            pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)  # + SMALL_CONST\n",
    "\n",
    "            # rescale\n",
    "            if torch.sum(pert_probs) <= 1:\n",
    "                pert_probs = pert_probs / torch.sum(pert_probs)\n",
    "\n",
    "        else:\n",
    "            pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\n",
    "            pert_probs = F.softmax(pert_logits, dim=-1)\n",
    "\n",
    "        # sample or greedy\n",
    "        if sample:\n",
    "            last = torch.multinomial(pert_probs, num_samples=1)\n",
    "\n",
    "        else:\n",
    "            _, last = torch.topk(pert_probs, k=1, dim=-1)\n",
    "\n",
    "        # update context/output_so_far appending the new token\n",
    "        output_so_far = (\n",
    "            last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\n",
    "        )\n",
    "\n",
    "        print(tokenizer.decode(output_so_far.tolist()[0]))\n",
    "\n",
    "    return output_so_far, unpert_discrim_loss, loss_in_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"def set_generic_model_params(discrim_weights, discrim_meta):\\n    if discrim_weights is None:\\n        raise ValueError(\\n            \\\"When using a generic discriminator, \\\"\\n            \\\"discrim_weights need to be specified\\\"\\n        )\\n    if discrim_meta is None:\\n        raise ValueError(\\n            \\\"When using a generic discriminator, \\\" \\\"discrim_meta need to be specified\\\"\\n        )\\n\\n    with open(discrim_meta, \\\"r\\\") as discrim_meta_file:\\n        meta = json.load(discrim_meta_file)\\n    meta[\\\"path\\\"] = discrim_weights\\n    DISCRIMINATOR_MODELS_PARAMS[\\\"generic\\\"] = meta\";\n",
       "                var nbb_formatted_code = \"def set_generic_model_params(discrim_weights, discrim_meta):\\n    if discrim_weights is None:\\n        raise ValueError(\\n            \\\"When using a generic discriminator, \\\"\\n            \\\"discrim_weights need to be specified\\\"\\n        )\\n    if discrim_meta is None:\\n        raise ValueError(\\n            \\\"When using a generic discriminator, \\\" \\\"discrim_meta need to be specified\\\"\\n        )\\n\\n    with open(discrim_meta, \\\"r\\\") as discrim_meta_file:\\n        meta = json.load(discrim_meta_file)\\n    meta[\\\"path\\\"] = discrim_weights\\n    DISCRIMINATOR_MODELS_PARAMS[\\\"generic\\\"] = meta\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def set_generic_model_params(discrim_weights, discrim_meta):\n",
    "    if discrim_weights is None:\n",
    "        raise ValueError(\n",
    "            \"When using a generic discriminator, \"\n",
    "            \"discrim_weights need to be specified\"\n",
    "        )\n",
    "    if discrim_meta is None:\n",
    "        raise ValueError(\n",
    "            \"When using a generic discriminator, \" \"discrim_meta need to be specified\"\n",
    "        )\n",
    "\n",
    "    with open(discrim_meta, \"r\") as discrim_meta_file:\n",
    "        meta = json.load(discrim_meta_file)\n",
    "    meta[\"path\"] = discrim_weights\n",
    "    DISCRIMINATOR_MODELS_PARAMS[\"generic\"] = meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"def run_pplm_example(\\n    pretrained_model=\\\"gpt2-medium\\\",\\n    cond_text=\\\"\\\",\\n    uncond=False,\\n    num_samples=1,\\n    bag_of_words=None,\\n    discrim=None,\\n    discrim_weights=None,\\n    discrim_meta=None,\\n    class_label=-1,\\n    length=100,\\n    stepsize=0.02,\\n    temperature=1.0,\\n    top_k=10,\\n    sample=False,\\n    num_iterations=3,\\n    grad_length=10000,\\n    horizon_length=1,\\n    window_length=0,\\n    decay=False,\\n    gamma=1.5,\\n    gm_scale=0.9,\\n    kl_scale=0.01,\\n    seed=0,\\n    no_cuda=False,\\n    colorama=False,\\n    repetition_penalty=1.0,\\n):\\n    # set Random seed\\n    torch.manual_seed(seed)\\n    np.random.seed(seed)\\n\\n    # set the device\\n    device = \\\"cuda\\\" if torch.cuda.is_available() and not no_cuda else \\\"cpu\\\"\\n\\n    if discrim == \\\"generic\\\":\\n        set_generic_model_params(discrim_weights, discrim_meta)\\n\\n    if discrim is not None:\\n        pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim][\\\"pretrained_model\\\"]\\n        print(\\n            \\\"discrim = {}, pretrained_model set \\\"\\n            \\\"to discriminator's = {}\\\".format(discrim, pretrained_model)\\n        )\\n\\n    # load pretrained model\\n    model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\\n    model.to(device)\\n    model.eval()\\n\\n    # load tokenizer\\n    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\\n\\n    # Freeze GPT-2 weights\\n    for param in model.parameters():\\n        param.requires_grad = False\\n\\n    # figure out conditioning text\\n    if uncond:\\n        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token])\\n    else:\\n        raw_text = cond_text\\n        while not raw_text:\\n            print(\\\"Did you forget to add `--cond_text`? \\\")\\n            raw_text = input(\\\"Model prompt >>> \\\")\\n        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text)\\n\\n    print(\\\"= Prefix of sentence =\\\")\\n    print(tokenizer.decode(tokenized_cond_text))\\n    print()\\n\\n    # generate unperturbed and perturbed texts\\n\\n    # full_text_generation returns:\\n    # unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\\n    unpert_gen_tok_text, pert_gen_tok_texts, _, _ = full_text_generation(\\n        model=model,\\n        tokenizer=tokenizer,\\n        context=tokenized_cond_text,\\n        device=device,\\n        num_samples=num_samples,\\n        bag_of_words=bag_of_words,\\n        discrim=discrim,\\n        class_label=class_label,\\n        length=length,\\n        stepsize=stepsize,\\n        temperature=temperature,\\n        top_k=top_k,\\n        sample=sample,\\n        num_iterations=num_iterations,\\n        grad_length=grad_length,\\n        horizon_length=horizon_length,\\n        window_length=window_length,\\n        decay=decay,\\n        gamma=gamma,\\n        gm_scale=gm_scale,\\n        kl_scale=kl_scale,\\n        repetition_penalty=repetition_penalty,\\n    )\\n\\n    # untokenize unperturbed text\\n    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\\n\\n    print(\\\"=\\\" * 80)\\n    print(\\\"= Unperturbed generated text =\\\")\\n    print(unpert_gen_text)\\n    print()\\n\\n    generated_texts = []\\n\\n    bow_word_ids = set()\\n    if bag_of_words and colorama:\\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(\\\";\\\"), tokenizer)\\n        for single_bow_list in bow_indices:\\n            # filtering all words in the list composed of more than 1 token\\n            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\\n            # w[0] because we are sure w has only 1 item because previous fitler\\n            bow_word_ids.update(w[0] for w in filtered)\\n\\n    # iterate through the perturbed texts\\n    for i, pert_gen_tok_text in enumerate(pert_gen_tok_texts):\\n        try:\\n            # untokenize unperturbed text\\n            if colorama:\\n                import colorama\\n\\n                pert_gen_text = \\\"\\\"\\n                for word_id in pert_gen_tok_text.tolist()[0]:\\n                    if word_id in bow_word_ids:\\n                        pert_gen_text += \\\"{}{}{}\\\".format(\\n                            colorama.Fore.RED,\\n                            tokenizer.decode([word_id]),\\n                            colorama.Style.RESET_ALL,\\n                        )\\n                    else:\\n                        pert_gen_text += tokenizer.decode([word_id])\\n            else:\\n                pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\\n\\n            print(\\\"= Perturbed generated text {} =\\\".format(i + 1))\\n            print(pert_gen_text)\\n            print()\\n        except Exception as exc:\\n            print(\\\"Ignoring error while generating perturbed text:\\\", exc)\\n\\n        # keep the prefix, perturbed seq, original seq for each index\\n        generated_texts.append(\\n            (tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text)\\n        )\\n\\n    return\";\n",
       "                var nbb_formatted_code = \"def run_pplm_example(\\n    pretrained_model=\\\"gpt2-medium\\\",\\n    cond_text=\\\"\\\",\\n    uncond=False,\\n    num_samples=1,\\n    bag_of_words=None,\\n    discrim=None,\\n    discrim_weights=None,\\n    discrim_meta=None,\\n    class_label=-1,\\n    length=100,\\n    stepsize=0.02,\\n    temperature=1.0,\\n    top_k=10,\\n    sample=False,\\n    num_iterations=3,\\n    grad_length=10000,\\n    horizon_length=1,\\n    window_length=0,\\n    decay=False,\\n    gamma=1.5,\\n    gm_scale=0.9,\\n    kl_scale=0.01,\\n    seed=0,\\n    no_cuda=False,\\n    colorama=False,\\n    repetition_penalty=1.0,\\n):\\n    # set Random seed\\n    torch.manual_seed(seed)\\n    np.random.seed(seed)\\n\\n    # set the device\\n    device = \\\"cuda\\\" if torch.cuda.is_available() and not no_cuda else \\\"cpu\\\"\\n\\n    if discrim == \\\"generic\\\":\\n        set_generic_model_params(discrim_weights, discrim_meta)\\n\\n    if discrim is not None:\\n        pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim][\\\"pretrained_model\\\"]\\n        print(\\n            \\\"discrim = {}, pretrained_model set \\\"\\n            \\\"to discriminator's = {}\\\".format(discrim, pretrained_model)\\n        )\\n\\n    # load pretrained model\\n    model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\\n    model.to(device)\\n    model.eval()\\n\\n    # load tokenizer\\n    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\\n\\n    # Freeze GPT-2 weights\\n    for param in model.parameters():\\n        param.requires_grad = False\\n\\n    # figure out conditioning text\\n    if uncond:\\n        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token])\\n    else:\\n        raw_text = cond_text\\n        while not raw_text:\\n            print(\\\"Did you forget to add `--cond_text`? \\\")\\n            raw_text = input(\\\"Model prompt >>> \\\")\\n        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text)\\n\\n    print(\\\"= Prefix of sentence =\\\")\\n    print(tokenizer.decode(tokenized_cond_text))\\n    print()\\n\\n    # generate unperturbed and perturbed texts\\n\\n    # full_text_generation returns:\\n    # unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\\n    unpert_gen_tok_text, pert_gen_tok_texts, _, _ = full_text_generation(\\n        model=model,\\n        tokenizer=tokenizer,\\n        context=tokenized_cond_text,\\n        device=device,\\n        num_samples=num_samples,\\n        bag_of_words=bag_of_words,\\n        discrim=discrim,\\n        class_label=class_label,\\n        length=length,\\n        stepsize=stepsize,\\n        temperature=temperature,\\n        top_k=top_k,\\n        sample=sample,\\n        num_iterations=num_iterations,\\n        grad_length=grad_length,\\n        horizon_length=horizon_length,\\n        window_length=window_length,\\n        decay=decay,\\n        gamma=gamma,\\n        gm_scale=gm_scale,\\n        kl_scale=kl_scale,\\n        repetition_penalty=repetition_penalty,\\n    )\\n\\n    # untokenize unperturbed text\\n    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\\n\\n    print(\\\"=\\\" * 80)\\n    print(\\\"= Unperturbed generated text =\\\")\\n    print(unpert_gen_text)\\n    print()\\n\\n    generated_texts = []\\n\\n    bow_word_ids = set()\\n    if bag_of_words and colorama:\\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(\\\";\\\"), tokenizer)\\n        for single_bow_list in bow_indices:\\n            # filtering all words in the list composed of more than 1 token\\n            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\\n            # w[0] because we are sure w has only 1 item because previous fitler\\n            bow_word_ids.update(w[0] for w in filtered)\\n\\n    # iterate through the perturbed texts\\n    for i, pert_gen_tok_text in enumerate(pert_gen_tok_texts):\\n        try:\\n            # untokenize unperturbed text\\n            if colorama:\\n                import colorama\\n\\n                pert_gen_text = \\\"\\\"\\n                for word_id in pert_gen_tok_text.tolist()[0]:\\n                    if word_id in bow_word_ids:\\n                        pert_gen_text += \\\"{}{}{}\\\".format(\\n                            colorama.Fore.RED,\\n                            tokenizer.decode([word_id]),\\n                            colorama.Style.RESET_ALL,\\n                        )\\n                    else:\\n                        pert_gen_text += tokenizer.decode([word_id])\\n            else:\\n                pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\\n\\n            print(\\\"= Perturbed generated text {} =\\\".format(i + 1))\\n            print(pert_gen_text)\\n            print()\\n        except Exception as exc:\\n            print(\\\"Ignoring error while generating perturbed text:\\\", exc)\\n\\n        # keep the prefix, perturbed seq, original seq for each index\\n        generated_texts.append(\\n            (tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text)\\n        )\\n\\n    return\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_pplm_example(\n",
    "    pretrained_model=\"gpt2-medium\",\n",
    "    cond_text=\"\",\n",
    "    uncond=False,\n",
    "    num_samples=1,\n",
    "    bag_of_words=None,\n",
    "    discrim=None,\n",
    "    discrim_weights=None,\n",
    "    discrim_meta=None,\n",
    "    class_label=-1,\n",
    "    length=100,\n",
    "    stepsize=0.02,\n",
    "    temperature=1.0,\n",
    "    top_k=10,\n",
    "    sample=False,\n",
    "    num_iterations=3,\n",
    "    grad_length=10000,\n",
    "    horizon_length=1,\n",
    "    window_length=0,\n",
    "    decay=False,\n",
    "    gamma=1.5,\n",
    "    gm_scale=0.9,\n",
    "    kl_scale=0.01,\n",
    "    seed=0,\n",
    "    no_cuda=False,\n",
    "    colorama=False,\n",
    "    repetition_penalty=1.0,\n",
    "):\n",
    "    # set Random seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # set the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\"\n",
    "\n",
    "    if discrim == \"generic\":\n",
    "        set_generic_model_params(discrim_weights, discrim_meta)\n",
    "\n",
    "    if discrim is not None:\n",
    "        pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim][\"pretrained_model\"]\n",
    "        print(\n",
    "            \"discrim = {}, pretrained_model set \"\n",
    "            \"to discriminator's = {}\".format(discrim, pretrained_model)\n",
    "        )\n",
    "\n",
    "    # load pretrained model\n",
    "    model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "    # Freeze GPT-2 weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # figure out conditioning text\n",
    "    if uncond:\n",
    "        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token])\n",
    "    else:\n",
    "        raw_text = cond_text\n",
    "        while not raw_text:\n",
    "            print(\"Did you forget to add `--cond_text`? \")\n",
    "            raw_text = input(\"Model prompt >>> \")\n",
    "        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text)\n",
    "\n",
    "    print(\"= Prefix of sentence =\")\n",
    "    print(tokenizer.decode(tokenized_cond_text))\n",
    "    print()\n",
    "\n",
    "    # generate unperturbed and perturbed texts\n",
    "\n",
    "    # full_text_generation returns:\n",
    "    # unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n",
    "    unpert_gen_tok_text, pert_gen_tok_texts, _, _ = full_text_generation(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        context=tokenized_cond_text,\n",
    "        device=device,\n",
    "        num_samples=num_samples,\n",
    "        bag_of_words=bag_of_words,\n",
    "        discrim=discrim,\n",
    "        class_label=class_label,\n",
    "        length=length,\n",
    "        stepsize=stepsize,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        sample=sample,\n",
    "        num_iterations=num_iterations,\n",
    "        grad_length=grad_length,\n",
    "        horizon_length=horizon_length,\n",
    "        window_length=window_length,\n",
    "        decay=decay,\n",
    "        gamma=gamma,\n",
    "        gm_scale=gm_scale,\n",
    "        kl_scale=kl_scale,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "    )\n",
    "\n",
    "    # untokenize unperturbed text\n",
    "    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"= Unperturbed generated text =\")\n",
    "    print(unpert_gen_text)\n",
    "    print()\n",
    "\n",
    "    generated_texts = []\n",
    "\n",
    "    bow_word_ids = set()\n",
    "    if bag_of_words and colorama:\n",
    "        bow_indices = get_bag_of_words_indices(bag_of_words.split(\";\"), tokenizer)\n",
    "        for single_bow_list in bow_indices:\n",
    "            # filtering all words in the list composed of more than 1 token\n",
    "            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\n",
    "            # w[0] because we are sure w has only 1 item because previous fitler\n",
    "            bow_word_ids.update(w[0] for w in filtered)\n",
    "\n",
    "    # iterate through the perturbed texts\n",
    "    for i, pert_gen_tok_text in enumerate(pert_gen_tok_texts):\n",
    "        try:\n",
    "            # untokenize unperturbed text\n",
    "            if colorama:\n",
    "                import colorama\n",
    "\n",
    "                pert_gen_text = \"\"\n",
    "                for word_id in pert_gen_tok_text.tolist()[0]:\n",
    "                    if word_id in bow_word_ids:\n",
    "                        pert_gen_text += \"{}{}{}\".format(\n",
    "                            colorama.Fore.RED,\n",
    "                            tokenizer.decode([word_id]),\n",
    "                            colorama.Style.RESET_ALL,\n",
    "                        )\n",
    "                    else:\n",
    "                        pert_gen_text += tokenizer.decode([word_id])\n",
    "            else:\n",
    "                pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n",
    "\n",
    "            print(\"= Perturbed generated text {} =\".format(i + 1))\n",
    "            print(pert_gen_text)\n",
    "            print()\n",
    "        except Exception as exc:\n",
    "            print(\"Ignoring error while generating perturbed text:\", exc)\n",
    "\n",
    "        # keep the prefix, perturbed seq, original seq for each index\n",
    "        generated_texts.append(\n",
    "            (tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text)\n",
    "        )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 154;\n",
       "                var nbb_unformatted_code = \"decay_mask\";\n",
       "                var nbb_formatted_code = \"decay_mask\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = {\n",
    "    \"pretrained_model\": \"gpt2-medium\",\n",
    "    \"cond_text\": \"The lake\",\n",
    "    \"uncond\": True,\n",
    "    \"num_samples\": 1,\n",
    "    \"bag_of_words\": \"military\",\n",
    "    \"discrim\": \"sentiment\",\n",
    "    \"discrim_weights\": None,\n",
    "    \"discrim_meta\": None,\n",
    "    \"class_label\": -1,\n",
    "    \"length\": 100,\n",
    "    \"stepsize\": 0.03,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_k\": 10,\n",
    "    \"sample\": True,\n",
    "    \"num_iterations\": 3,\n",
    "    \"grad_length\": 10000,\n",
    "    \"window_length\": 5,\n",
    "    \"horizon_length\": 1,\n",
    "    \"decay\": True,\n",
    "    \"gamma\": 1.5,\n",
    "    \"gm_scale\": 0.9,\n",
    "    \"kl_scale\": 0.01,\n",
    "    \"seed\": 0,\n",
    "    \"colorama\": False,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"device = torch.device(\\\"cuda\\\") if torch.cuda.is_available() else torch.device(\\\"cpu\\\")\";\n",
       "                var nbb_formatted_code = \"device = torch.device(\\\"cuda\\\") if torch.cuda.is_available() else torch.device(\\\"cpu\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"#run_pplm_example(**args)\";\n",
       "                var nbb_formatted_code = \"# run_pplm_example(**args)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run_pplm_example(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"torch.manual_seed(args[\\\"seed\\\"])\\nnp.random.seed(args[\\\"seed\\\"])\";\n",
       "                var nbb_formatted_code = \"torch.manual_seed(args[\\\"seed\\\"])\\nnp.random.seed(args[\\\"seed\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(args[\"seed\"])\n",
    "np.random.seed(args[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discrim = sentiment, pretrained_model set to discriminator's = gpt2-medium\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0504 23:22:34.408077 139800908659584 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-config.json from cache at /home/u37216/.cache/torch/transformers/98aa65385e18b0efd17acd8bf64dcdf21406bb0c99c801c2d3c9f6bfd1f48f29.42ee920fcfb8dd7cf21fdc10f45b4545a7050ec5dd5463e844c310dd9beeae87\n",
      "I0504 23:22:34.410663 139800908659584 configuration_utils.py:319] Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0504 23:22:34.721830 139800908659584 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin from cache at /home/u37216/.cache/torch/transformers/4b337a4f3b7d3e1518f799e238af607498c02938a3390152aaec7d4dabca5a02.8769029be4f66a5ae1055eefdd1d11621b901d510654266b8681719fff492d6e\n",
      "I0504 23:22:54.961550 139800908659584 modeling_utils.py:601] Weights of GPT2LMHeadModel not initialized from pretrained model: ['lm_head.weight']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"if args[\\\"discrim\\\"] == \\\"generic\\\":\\n    set_generic_model_params(discrim_weights, discrim_meta)\\n\\nif args[\\\"discrim\\\"] is not None:\\n    pretrained_model = DISCRIMINATOR_MODELS_PARAMS[args[\\\"discrim\\\"]][\\\"pretrained_model\\\"]\\n    print(\\n        \\\"discrim = {}, pretrained_model set \\\"\\n        \\\"to discriminator's = {}\\\".format(args[\\\"discrim\\\"], pretrained_model)\\n    )\\n\\n# load pretrained model\\nmodel = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\\nmodel.to(device)\\nmodel.eval()\";\n",
       "                var nbb_formatted_code = \"if args[\\\"discrim\\\"] == \\\"generic\\\":\\n    set_generic_model_params(discrim_weights, discrim_meta)\\n\\nif args[\\\"discrim\\\"] is not None:\\n    pretrained_model = DISCRIMINATOR_MODELS_PARAMS[args[\\\"discrim\\\"]][\\\"pretrained_model\\\"]\\n    print(\\n        \\\"discrim = {}, pretrained_model set \\\"\\n        \\\"to discriminator's = {}\\\".format(args[\\\"discrim\\\"], pretrained_model)\\n    )\\n\\n# load pretrained model\\nmodel = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\\nmodel.to(device)\\nmodel.eval()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args[\"discrim\"] == \"generic\":\n",
    "    set_generic_model_params(discrim_weights, discrim_meta)\n",
    "\n",
    "if args[\"discrim\"] is not None:\n",
    "    pretrained_model = DISCRIMINATOR_MODELS_PARAMS[args[\"discrim\"]][\"pretrained_model\"]\n",
    "    print(\n",
    "        \"discrim = {}, pretrained_model set \"\n",
    "        \"to discriminator's = {}\".format(args[\"discrim\"], pretrained_model)\n",
    "    )\n",
    "\n",
    "# load pretrained model\n",
    "model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt2-medium'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"pretrained_model\";\n",
       "                var nbb_formatted_code = \"pretrained_model\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/SST_classifier_head.pt',\n",
       " 'class_size': 5,\n",
       " 'embed_size': 1024,\n",
       " 'class_vocab': {'very_positive': 2, 'very_negative': 3},\n",
       " 'default_class': 3,\n",
       " 'pretrained_model': 'gpt2-medium'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"DISCRIMINATOR_MODELS_PARAMS[\\\"sentiment\\\"]\";\n",
       "                var nbb_formatted_code = \"DISCRIMINATOR_MODELS_PARAMS[\\\"sentiment\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DISCRIMINATOR_MODELS_PARAMS[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0504 23:22:55.666390 139800908659584 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json from cache at /home/u37216/.cache/torch/transformers/f20f05d3ae37c4e3cd56764d48e566ea5adeba153dcee6eb82a18822c9c731ec.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0504 23:22:55.667993 139800908659584 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-merges.txt from cache at /home/u37216/.cache/torch/transformers/6d882670c55563617571fe0c97df88626fb5033927b40fc18a8acf98dafd4946.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\";\n",
       "                var nbb_formatted_code = \"tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_gpt2.GPT2Tokenizer at 0x7f255a64d240>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"tokenizer\";\n",
       "                var nbb_formatted_code = \"tokenizer\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"for param in model.parameters():\\n    param.requires_grad = False\";\n",
       "                var nbb_formatted_code = \"for param in model.parameters():\\n    param.requires_grad = False\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"tokenized_cond_text = tokenizer.encode([tokenizer.bos_token]) #performs tokenization of bos token\";\n",
       "                var nbb_formatted_code = \"tokenized_cond_text = tokenizer.encode(\\n    [tokenizer.bos_token]\\n)  # performs tokenization of bos token\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_cond_text = tokenizer.encode([tokenizer.bos_token]) #performs tokenization of bos token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/SST_classifier_head.pt',\n",
       " 'class_size': 5,\n",
       " 'embed_size': 1024,\n",
       " 'class_vocab': {'very_positive': 2, 'very_negative': 3},\n",
       " 'default_class': 3,\n",
       " 'pretrained_model': 'gpt2-medium'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"params = DISCRIMINATOR_MODELS_PARAMS[args['discrim']]\\nparams\";\n",
       "                var nbb_formatted_code = \"params = DISCRIMINATOR_MODELS_PARAMS[args[\\\"discrim\\\"]]\\nparams\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = DISCRIMINATOR_MODELS_PARAMS[args['discrim']]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 34;\n",
       "                var nbb_unformatted_code = \"classifier = ClassificationHead(\\n        class_size=params[\\\"class_size\\\"], embed_size=params[\\\"embed_size\\\"]\\n    ).to(device)\";\n",
       "                var nbb_formatted_code = \"classifier = ClassificationHead(\\n    class_size=params[\\\"class_size\\\"], embed_size=params[\\\"embed_size\\\"]\\n).to(device)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = ClassificationHead(\n",
    "        class_size=params[\"class_size\"], embed_size=params[\"embed_size\"]\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u37216/.cache/torch/transformers/42357c6dbedfbfd9f1a59709d4553429b37f25cb0aff8e3c7c8b8291c55dbbc9.a0039fb75d9c0c460975276b6875387756815e189a964d9478dc215178f9cb0c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassificationHead(\n",
       "  (mlp): Linear(in_features=1024, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 42;\n",
       "                var nbb_unformatted_code = \"params\";\n",
       "                var nbb_formatted_code = \"params\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"url\" in params:\n",
    "    resolved_archive_file = cached_path(params[\"url\"])\n",
    "elif \"path\" in params:\n",
    "    resolved_archive_file = params[\"path\"]\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Either url or path have to be specified \"\n",
    "        \"in the discriminator model parameters\"\n",
    "    )\n",
    "print(resolved_archive_file)\n",
    "classifier.load_state_dict(torch.load(resolved_archive_file, map_location=device))\n",
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'very_positive': 2, 'very_negative': 3}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 46;\n",
       "                var nbb_unformatted_code = \"args['class_label']\";\n",
       "                var nbb_formatted_code = \"args[\\\"class_label\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params['class_vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 151;\n",
       "                var nbb_unformatted_code = \"args['window_length']\";\n",
       "                var nbb_formatted_code = \"args[\\\"window_length\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args['class_label'] = 'very_positive'\n",
    "args['window_length'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 48;\n",
       "                var nbb_unformatted_code = \"args['class_label'] = 'very_positive'\";\n",
       "                var nbb_formatted_code = \"args[\\\"class_label\\\"] = \\\"very_positive\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier, class_id = get_classifier(args[\"discrim\"], args[\"class_label\"], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"class_id\";\n",
       "                var nbb_formatted_code = \"class_id\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_ids_or_paths = args['bag_of_words'].split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 52;\n",
       "                var nbb_unformatted_code = \"bow_indices = []\\nif args['bag_of_words']:\\n    bow_indices = get_bag_of_words_indices(args['bag_of_words'].split(\\\";\\\"), tokenizer)\\n\\nif bag_of_words and classifier:\\n    print(\\\"Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.\\\")\\n    loss_type = PPLM_BOW_DISCRIM\\n\\nelif bag_of_words:\\n    loss_type = PPLM_BOW\\n    print(\\\"Using PPLM-BoW\\\")\\n\\nelif classifier is not None:\\n    loss_type = PPLM_DISCRIM\\n    print(\\\"Using PPLM-Discrim\\\")\\n\\nelse:\\n    raise Exception(\\\"Specify either a bag of words or a discriminator\\\")\";\n",
       "                var nbb_formatted_code = \"bow_indices = []\\nif args[\\\"bag_of_words\\\"]:\\n    bow_indices = get_bag_of_words_indices(args[\\\"bag_of_words\\\"].split(\\\";\\\"), tokenizer)\\n\\nif bag_of_words and classifier:\\n    print(\\\"Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.\\\")\\n    loss_type = PPLM_BOW_DISCRIM\\n\\nelif bag_of_words:\\n    loss_type = PPLM_BOW\\n    print(\\\"Using PPLM-BoW\\\")\\n\\nelif classifier is not None:\\n    loss_type = PPLM_DISCRIM\\n    print(\\\"Using PPLM-Discrim\\\")\\n\\nelse:\\n    raise Exception(\\\"Specify either a bag of words or a discriminator\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bow_indices = []\n",
    "if args['bag_of_words']:\n",
    "    bow_indices = get_bag_of_words_indices(args['bag_of_words'].split(\";\"), tokenizer)\n",
    "\n",
    "if args['bag_of_words'] and classifier:\n",
    "    print(\"Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.\")\n",
    "    loss_type = PPLM_BOW_DISCRIM\n",
    "\n",
    "elif bag_of_words:\n",
    "    loss_type = PPLM_BOW\n",
    "    print(\"Using PPLM-BoW\")\n",
    "\n",
    "elif classifier is not None:\n",
    "    loss_type = PPLM_DISCRIM\n",
    "    print(\"Using PPLM-Discrim\")\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Specify either a bag of words or a discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 70;\n",
       "                var nbb_unformatted_code = \"output_so_far = None\\nif context:\\n    context_t = torch.tensor(context, device=device, dtype=torch.long)\\n    while len(context_t.shape) < 2:\\n        context_t = context_t.unsqueeze(0)\\n    output_so_far = context_t\\n\\n# collect one hot vectors for bags of words\\none_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\";\n",
       "                var nbb_formatted_code = \"output_so_far = None\\nif context:\\n    context_t = torch.tensor(context, device=device, dtype=torch.long)\\n    while len(context_t.shape) < 2:\\n        context_t = context_t.unsqueeze(0)\\n    output_so_far = context_t\\n\\n# collect one hot vectors for bags of words\\none_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = tokenized_cond_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 71;\n",
       "                var nbb_unformatted_code = \"context = tokenized_cond_text\";\n",
       "                var nbb_formatted_code = \"context = tokenized_cond_text\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_so_far = None\n",
    "if context:\n",
    "    context_t = torch.tensor(context, device=device, dtype=torch.long)\n",
    "    while len(context_t.shape) < 2:\n",
    "        context_t = context_t.unsqueeze(0)\n",
    "    output_so_far = context_t\n",
    "\n",
    "# collect one hot vectors for bags of words\n",
    "one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([149, 50257])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 78;\n",
       "                var nbb_unformatted_code = \"one_hot_bows_vectors[1].shape\";\n",
       "                var nbb_formatted_code = \"one_hot_bows_vectors[1].shape\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_hot_bows_vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 89;\n",
       "                var nbb_unformatted_code = \"len(single_bow)\";\n",
       "                var nbb_formatted_code = \"len(single_bow)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(single_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 111;\n",
       "                var nbb_unformatted_code = \"for i in trange(args['length'], ascii=True):\\n\\n    # Get past/probs for current output, except for last word\\n    # Note that GPT takes 2 inputs: past + current_token\\n\\n    # run model forward to obtain unperturbed\\n    if past is None and output_so_far is not None:\\n        last = output_so_far[:, -1:]\\n        if output_so_far.shape[1] > 1:\\n            _, past, _ = model(output_so_far[:, :-1])\\n\\n    unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\\n    unpert_last_hidden = unpert_all_hidden[-1]\\n\\n    # check if we are abowe grad max length\\n    if i >= args['grad_length']:\\n        current_stepsize = args['stepsize'] * 0\\n    else:\\n        current_stepsize = args['stepsize']\\n\\n    # modify the past if necessary\\n    if not perturb or num_iterations == 0:\\n        pert_past = past\\n\\n    else:\\n        accumulated_hidden = unpert_last_hidden[:, :-1, :]\\n        accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\\n\\n        if past is not None:\\n            pert_past, _, grad_norms, loss_this_iter = perturb_past(\\n                past,\\n                model,\\n                last,\\n                unpert_past=unpert_past,\\n                unpert_logits=unpert_logits,\\n                accumulated_hidden=accumulated_hidden,\\n                grad_norms=grad_norms,\\n                stepsize=current_stepsize,\\n                one_hot_bows_vectors=one_hot_bows_vectors,\\n                classifier=classifier,\\n                class_label=class_label,\\n                loss_type=loss_type,\\n                num_iterations=num_iterations,\\n                horizon_length=horizon_length,\\n                window_length=window_length,\\n                decay=decay,\\n                gamma=gamma,\\n                kl_scale=kl_scale,\\n                device=device,\\n            )\\n            loss_in_time.append(loss_this_iter)\\n        else:\\n            pert_past = past\\n\\n    pert_logits, past, pert_all_hidden = model(last, past=pert_past)\\n    pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\\n\\n    for token_idx in set(output_so_far[0].tolist()):\\n        if pert_logits[0, token_idx] < 0:\\n            pert_logits[0, token_idx] *= repetition_penalty\\n        else:\\n            pert_logits[0, token_idx] /= repetition_penalty\\n\\n    pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n    if classifier is not None:\\n        ce_loss = torch.nn.CrossEntropyLoss()\\n        prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\\n        label = torch.tensor([class_label], device=device, dtype=torch.long)\\n        unpert_discrim_loss = ce_loss(prediction, label)\\n        print(\\\"unperturbed discrim loss\\\", unpert_discrim_loss.data.cpu().numpy())\\n    else:\\n        unpert_discrim_loss = 0\\n\\n    # Fuse the modified model and original model\\n    if perturb:\\n\\n        unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\\n\\n        pert_probs = (pert_probs ** gm_scale) * (unpert_probs ** (1 - gm_scale))  # + SMALL_CONST\\n        pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)  # + SMALL_CONST\\n\\n        # rescale\\n        if torch.sum(pert_probs) <= 1:\\n            pert_probs = pert_probs / torch.sum(pert_probs)\\n\\n    else:\\n        pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\\n        pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n    # sample or greedy\\n    if sample:\\n        last = torch.multinomial(pert_probs, num_samples=1)\\n\\n    else:\\n        _, last = torch.topk(pert_probs, k=1, dim=-1)\\n\\n    # update context/output_so_far appending the new token\\n    output_so_far = last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\\n\\n    print(tokenizer.decode(output_so_far.tolist()[0]))\";\n",
       "                var nbb_formatted_code = \"for i in trange(args[\\\"length\\\"], ascii=True):\\n\\n    # Get past/probs for current output, except for last word\\n    # Note that GPT takes 2 inputs: past + current_token\\n\\n    # run model forward to obtain unperturbed\\n    if past is None and output_so_far is not None:\\n        last = output_so_far[:, -1:]\\n        if output_so_far.shape[1] > 1:\\n            _, past, _ = model(output_so_far[:, :-1])\\n\\n    unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\\n    unpert_last_hidden = unpert_all_hidden[-1]\\n\\n    # check if we are abowe grad max length\\n    if i >= args[\\\"grad_length\\\"]:\\n        current_stepsize = args[\\\"stepsize\\\"] * 0\\n    else:\\n        current_stepsize = args[\\\"stepsize\\\"]\\n\\n    # modify the past if necessary\\n    if not perturb or num_iterations == 0:\\n        pert_past = past\\n\\n    else:\\n        accumulated_hidden = unpert_last_hidden[:, :-1, :]\\n        accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\\n\\n        if past is not None:\\n            pert_past, _, grad_norms, loss_this_iter = perturb_past(\\n                past,\\n                model,\\n                last,\\n                unpert_past=unpert_past,\\n                unpert_logits=unpert_logits,\\n                accumulated_hidden=accumulated_hidden,\\n                grad_norms=grad_norms,\\n                stepsize=current_stepsize,\\n                one_hot_bows_vectors=one_hot_bows_vectors,\\n                classifier=classifier,\\n                class_label=class_label,\\n                loss_type=loss_type,\\n                num_iterations=num_iterations,\\n                horizon_length=horizon_length,\\n                window_length=window_length,\\n                decay=decay,\\n                gamma=gamma,\\n                kl_scale=kl_scale,\\n                device=device,\\n            )\\n            loss_in_time.append(loss_this_iter)\\n        else:\\n            pert_past = past\\n\\n    pert_logits, past, pert_all_hidden = model(last, past=pert_past)\\n    pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\\n\\n    for token_idx in set(output_so_far[0].tolist()):\\n        if pert_logits[0, token_idx] < 0:\\n            pert_logits[0, token_idx] *= repetition_penalty\\n        else:\\n            pert_logits[0, token_idx] /= repetition_penalty\\n\\n    pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n    if classifier is not None:\\n        ce_loss = torch.nn.CrossEntropyLoss()\\n        prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\\n        label = torch.tensor([class_label], device=device, dtype=torch.long)\\n        unpert_discrim_loss = ce_loss(prediction, label)\\n        print(\\\"unperturbed discrim loss\\\", unpert_discrim_loss.data.cpu().numpy())\\n    else:\\n        unpert_discrim_loss = 0\\n\\n    # Fuse the modified model and original model\\n    if perturb:\\n\\n        unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\\n\\n        pert_probs = (pert_probs ** gm_scale) * (\\n            unpert_probs ** (1 - gm_scale)\\n        )  # + SMALL_CONST\\n        pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)  # + SMALL_CONST\\n\\n        # rescale\\n        if torch.sum(pert_probs) <= 1:\\n            pert_probs = pert_probs / torch.sum(pert_probs)\\n\\n    else:\\n        pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\\n        pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n    # sample or greedy\\n    if sample:\\n        last = torch.multinomial(pert_probs, num_samples=1)\\n\\n    else:\\n        _, last = torch.topk(pert_probs, k=1, dim=-1)\\n\\n    # update context/output_so_far appending the new token\\n    output_so_far = (\\n        last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\\n    )\\n\\n    print(tokenizer.decode(output_so_far.tolist()[0]))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad_norms = None\n",
    "last = None\n",
    "unpert_discrim_loss = 0\n",
    "loss_in_time = []\n",
    "past = None\n",
    "perturb = True\n",
    "num_iterations = 3\n",
    "temperature = 1.0\n",
    "repetition_penalty = 1.0\n",
    "class_label = class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 115;\n",
       "                var nbb_unformatted_code = \"for i in trange(args['length'], ascii=True):\\n\\n    # Get past/probs for current output, except for last word\\n    # Note that GPT takes 2 inputs: past + current_token\\n\\n    # run model forward to obtain unperturbed\\n    if past is None and output_so_far is not None:\\n        last = output_so_far[:, -1:]\\n        if output_so_far.shape[1] > 1:\\n            _, past, _ = model(output_so_far[:, :-1])\\n\\n    unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\\n    unpert_last_hidden = unpert_all_hidden[-1]\\n\\n    # check if we are abowe grad max length\\n    if i >= args['grad_length']:\\n        current_stepsize = args['stepsize'] * 0\\n    else:\\n        current_stepsize = args['stepsize']\\n    \\n    break\\n    \\n    # modify the past if necessary\\n    if not perturb or num_iterations == 0:\\n        pert_past = past\\n\\n    else:\\n        accumulated_hidden = unpert_last_hidden[:, :-1, :]\\n        accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\\n\\n        if past is not None:\\n            pert_past, _, grad_norms, loss_this_iter = perturb_past(\\n                past,\\n                model,\\n                last,\\n                unpert_past=unpert_past,\\n                unpert_logits=unpert_logits,\\n                accumulated_hidden=accumulated_hidden,\\n                grad_norms=grad_norms,\\n                stepsize=current_stepsize,\\n                one_hot_bows_vectors=one_hot_bows_vectors,\\n                classifier=classifier,\\n                class_label=class_label,\\n                loss_type=loss_type,\\n                num_iterations=num_iterations,\\n                horizon_length=horizon_length,\\n                window_length=window_length,\\n                decay=decay,\\n                gamma=gamma,\\n                kl_scale=kl_scale,\\n                device=device,\\n            )\\n            loss_in_time.append(loss_this_iter)\\n        else:\\n            pert_past = past\\n\\n    pert_logits, past, pert_all_hidden = model(last, past=pert_past)\\n    pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\\n\\n    for token_idx in set(output_so_far[0].tolist()):\\n        if pert_logits[0, token_idx] < 0:\\n            pert_logits[0, token_idx] *= repetition_penalty\\n        else:\\n            pert_logits[0, token_idx] /= repetition_penalty\\n\\n    pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n    if classifier is not None:\\n        ce_loss = torch.nn.CrossEntropyLoss()\\n        prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\\n        label = torch.tensor([class_label], device=device, dtype=torch.long)\\n        unpert_discrim_loss = ce_loss(prediction, label)\\n        print(\\\"unperturbed discrim loss\\\", unpert_discrim_loss.data.cpu().numpy())\\n    else:\\n        unpert_discrim_loss = 0\\n\\n    # Fuse the modified model and original model\\n    if perturb:\\n\\n        unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\\n\\n        pert_probs = (pert_probs ** gm_scale) * (unpert_probs ** (1 - args['gm_scale']))  # + SMALL_CONST\\n        pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)  # + SMALL_CONST\\n\\n        # rescale\\n        if torch.sum(pert_probs) <= 1:\\n            pert_probs = pert_probs / torch.sum(pert_probs)\\n\\n    else:\\n        pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\\n        pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n    # sample or greedy\\n    if sample:\\n        last = torch.multinomial(pert_probs, num_samples=1)\\n\\n    else:\\n        _, last = torch.topk(pert_probs, k=1, dim=-1)\\n\\n    # update context/output_so_far appending the new token\\n    output_so_far = last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\\n\\n    print(tokenizer.decode(output_so_far.tolist()[0]))\";\n",
       "                var nbb_formatted_code = \"for i in trange(args[\\\"length\\\"], ascii=True):\\n\\n    # Get past/probs for current output, except for last word\\n    # Note that GPT takes 2 inputs: past + current_token\\n\\n    # run model forward to obtain unperturbed\\n    if past is None and output_so_far is not None:\\n        last = output_so_far[:, -1:]\\n        if output_so_far.shape[1] > 1:\\n            _, past, _ = model(output_so_far[:, :-1])\\n\\n    unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\\n    unpert_last_hidden = unpert_all_hidden[-1]\\n\\n    # check if we are abowe grad max length\\n    if i >= args[\\\"grad_length\\\"]:\\n        current_stepsize = args[\\\"stepsize\\\"] * 0\\n    else:\\n        current_stepsize = args[\\\"stepsize\\\"]\\n\\n    break\\n\\n    # modify the past if necessary\\n    if not perturb or num_iterations == 0:\\n        pert_past = past\\n\\n    else:\\n        accumulated_hidden = unpert_last_hidden[:, :-1, :]\\n        accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\\n\\n        if past is not None:\\n            pert_past, _, grad_norms, loss_this_iter = perturb_past(\\n                past,\\n                model,\\n                last,\\n                unpert_past=unpert_past,\\n                unpert_logits=unpert_logits,\\n                accumulated_hidden=accumulated_hidden,\\n                grad_norms=grad_norms,\\n                stepsize=current_stepsize,\\n                one_hot_bows_vectors=one_hot_bows_vectors,\\n                classifier=classifier,\\n                class_label=class_label,\\n                loss_type=loss_type,\\n                num_iterations=num_iterations,\\n                horizon_length=horizon_length,\\n                window_length=window_length,\\n                decay=decay,\\n                gamma=gamma,\\n                kl_scale=kl_scale,\\n                device=device,\\n            )\\n            loss_in_time.append(loss_this_iter)\\n        else:\\n            pert_past = past\\n\\n    pert_logits, past, pert_all_hidden = model(last, past=pert_past)\\n    pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\\n\\n    for token_idx in set(output_so_far[0].tolist()):\\n        if pert_logits[0, token_idx] < 0:\\n            pert_logits[0, token_idx] *= repetition_penalty\\n        else:\\n            pert_logits[0, token_idx] /= repetition_penalty\\n\\n    pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n    if classifier is not None:\\n        ce_loss = torch.nn.CrossEntropyLoss()\\n        prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\\n        label = torch.tensor([class_label], device=device, dtype=torch.long)\\n        unpert_discrim_loss = ce_loss(prediction, label)\\n        print(\\\"unperturbed discrim loss\\\", unpert_discrim_loss.data.cpu().numpy())\\n    else:\\n        unpert_discrim_loss = 0\\n\\n    # Fuse the modified model and original model\\n    if perturb:\\n\\n        unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\\n\\n        pert_probs = (pert_probs ** gm_scale) * (\\n            unpert_probs ** (1 - args[\\\"gm_scale\\\"])\\n        )  # + SMALL_CONST\\n        pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)  # + SMALL_CONST\\n\\n        # rescale\\n        if torch.sum(pert_probs) <= 1:\\n            pert_probs = pert_probs / torch.sum(pert_probs)\\n\\n    else:\\n        pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\\n        pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n    # sample or greedy\\n    if sample:\\n        last = torch.multinomial(pert_probs, num_samples=1)\\n\\n    else:\\n        _, last = torch.topk(pert_probs, k=1, dim=-1)\\n\\n    # update context/output_so_far appending the new token\\n    output_so_far = (\\n        last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\\n    )\\n\\n    print(tokenizer.decode(output_so_far.tolist()[0]))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in trange(args['length'], ascii=True):\n",
    "\n",
    "    # Get past/probs for current output, except for last word\n",
    "    # Note that GPT takes 2 inputs: past + current_token\n",
    "\n",
    "    # run model forward to obtain unperturbed\n",
    "    if past is None and output_so_far is not None:\n",
    "        last = output_so_far[:, -1:]\n",
    "        print(last.shape)\n",
    "        if output_so_far.shape[1] > 1:\n",
    "            _, past, _ = model(output_so_far[:, :-1])\n",
    "\n",
    "    unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\n",
    "    unpert_last_hidden = unpert_all_hidden[-1]\n",
    "\n",
    "    # check if we are abowe grad max length\n",
    "    if i >= args['grad_length']:\n",
    "        current_stepsize = args['stepsize'] * 0\n",
    "    else:\n",
    "        current_stepsize = args['stepsize']\n",
    "    \n",
    "    break\n",
    "    \n",
    "    # modify the past if necessary\n",
    "    if not perturb or num_iterations == 0:\n",
    "        pert_past = past\n",
    "\n",
    "    else:\n",
    "        accumulated_hidden = unpert_last_hidden[:, :-1, :]\n",
    "        accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n",
    "\n",
    "        if past is not None:\n",
    "            pert_past, _, grad_norms, loss_this_iter = perturb_past(\n",
    "                past,\n",
    "                model,\n",
    "                last,\n",
    "                unpert_past=unpert_past,\n",
    "                unpert_logits=unpert_logits,\n",
    "                accumulated_hidden=accumulated_hidden,\n",
    "                grad_norms=grad_norms,\n",
    "                stepsize=current_stepsize,\n",
    "                one_hot_bows_vectors=one_hot_bows_vectors,\n",
    "                classifier=classifier,\n",
    "                class_label=class_label,\n",
    "                loss_type=loss_type,\n",
    "                num_iterations=num_iterations,\n",
    "                horizon_length=horizon_length,\n",
    "                window_length=window_length,\n",
    "                decay=decay,\n",
    "                gamma=gamma,\n",
    "                kl_scale=kl_scale,\n",
    "                device=device,\n",
    "            )\n",
    "            loss_in_time.append(loss_this_iter)\n",
    "        else:\n",
    "            pert_past = past\n",
    "\n",
    "    pert_logits, past, pert_all_hidden = model(last, past=pert_past)\n",
    "    pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\n",
    "\n",
    "    for token_idx in set(output_so_far[0].tolist()):\n",
    "        if pert_logits[0, token_idx] < 0:\n",
    "            pert_logits[0, token_idx] *= repetition_penalty\n",
    "        else:\n",
    "            pert_logits[0, token_idx] /= repetition_penalty\n",
    "\n",
    "    pert_probs = F.softmax(pert_logits, dim=-1)\n",
    "\n",
    "    if classifier is not None:\n",
    "        ce_loss = torch.nn.CrossEntropyLoss()\n",
    "        prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n",
    "        label = torch.tensor([class_label], device=device, dtype=torch.long)\n",
    "        unpert_discrim_loss = ce_loss(prediction, label)\n",
    "        print(\"unperturbed discrim loss\", unpert_discrim_loss.data.cpu().numpy())\n",
    "    else:\n",
    "        unpert_discrim_loss = 0\n",
    "\n",
    "    # Fuse the modified model and original model\n",
    "    if perturb:\n",
    "\n",
    "        unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
    "\n",
    "        pert_probs = (pert_probs ** gm_scale) * (unpert_probs ** (1 - args['gm_scale']))  # + SMALL_CONST\n",
    "        pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)  # + SMALL_CONST\n",
    "\n",
    "        # rescale\n",
    "        if torch.sum(pert_probs) <= 1:\n",
    "            pert_probs = pert_probs / torch.sum(pert_probs)\n",
    "\n",
    "    else:\n",
    "        pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\n",
    "        pert_probs = F.softmax(pert_logits, dim=-1)\n",
    "\n",
    "    # sample or greedy\n",
    "    if sample:\n",
    "        last = torch.multinomial(pert_probs, num_samples=1)\n",
    "\n",
    "    else:\n",
    "        _, last = torch.topk(pert_probs, k=1, dim=-1)\n",
    "\n",
    "    # update context/output_so_far appending the new token\n",
    "    output_so_far = last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\n",
    "\n",
    "    print(tokenizer.decode(output_so_far.tolist()[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 116;\n",
       "                var nbb_unformatted_code = \"for i in trange(args['length'], ascii=True):\\n\\n    # Get past/probs for current output, except for last word\\n    # Note that GPT takes 2 inputs: past + current_token\\n\\n    # run model forward to obtain unperturbed\\n    if past is None and output_so_far is not None:\\n        last = output_so_far[:, -1:]\\n        print(last.shape)\\n        if output_so_far.shape[1] > 1:\\n            _, past, _ = model(output_so_far[:, :-1])\\n\\n    unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\\n    unpert_last_hidden = unpert_all_hidden[-1]\\n\\n    # check if we are abowe grad max length\\n    if i >= args['grad_length']:\\n        current_stepsize = args['stepsize'] * 0\\n    else:\\n        current_stepsize = args['stepsize']\\n    \\n    break\\n    \\n    # modify the past if necessary\\n    if not perturb or num_iterations == 0:\\n        pert_past = past\\n\\n    else:\\n        accumulated_hidden = unpert_last_hidden[:, :-1, :]\\n        accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\\n\\n        if past is not None:\\n            pert_past, _, grad_norms, loss_this_iter = perturb_past(\\n                past,\\n                model,\\n                last,\\n                unpert_past=unpert_past,\\n                unpert_logits=unpert_logits,\\n                accumulated_hidden=accumulated_hidden,\\n                grad_norms=grad_norms,\\n                stepsize=current_stepsize,\\n                one_hot_bows_vectors=one_hot_bows_vectors,\\n                classifier=classifier,\\n                class_label=class_label,\\n                loss_type=loss_type,\\n                num_iterations=num_iterations,\\n                horizon_length=horizon_length,\\n                window_length=window_length,\\n                decay=decay,\\n                gamma=gamma,\\n                kl_scale=kl_scale,\\n                device=device,\\n            )\\n            loss_in_time.append(loss_this_iter)\\n        else:\\n            pert_past = past\\n\\n    pert_logits, past, pert_all_hidden = model(last, past=pert_past)\\n    pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\\n\\n    for token_idx in set(output_so_far[0].tolist()):\\n        if pert_logits[0, token_idx] < 0:\\n            pert_logits[0, token_idx] *= repetition_penalty\\n        else:\\n            pert_logits[0, token_idx] /= repetition_penalty\\n\\n    pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n    if classifier is not None:\\n        ce_loss = torch.nn.CrossEntropyLoss()\\n        prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\\n        label = torch.tensor([class_label], device=device, dtype=torch.long)\\n        unpert_discrim_loss = ce_loss(prediction, label)\\n        print(\\\"unperturbed discrim loss\\\", unpert_discrim_loss.data.cpu().numpy())\\n    else:\\n        unpert_discrim_loss = 0\\n\\n    # Fuse the modified model and original model\\n    if perturb:\\n\\n        unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\\n\\n        pert_probs = (pert_probs ** gm_scale) * (unpert_probs ** (1 - args['gm_scale']))  # + SMALL_CONST\\n        pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)  # + SMALL_CONST\\n\\n        # rescale\\n        if torch.sum(pert_probs) <= 1:\\n            pert_probs = pert_probs / torch.sum(pert_probs)\\n\\n    else:\\n        pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\\n        pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n    # sample or greedy\\n    if sample:\\n        last = torch.multinomial(pert_probs, num_samples=1)\\n\\n    else:\\n        _, last = torch.topk(pert_probs, k=1, dim=-1)\\n\\n    # update context/output_so_far appending the new token\\n    output_so_far = last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\\n\\n    print(tokenizer.decode(output_so_far.tolist()[0]))\";\n",
       "                var nbb_formatted_code = \"for i in trange(args[\\\"length\\\"], ascii=True):\\n\\n    # Get past/probs for current output, except for last word\\n    # Note that GPT takes 2 inputs: past + current_token\\n\\n    # run model forward to obtain unperturbed\\n    if past is None and output_so_far is not None:\\n        last = output_so_far[:, -1:]\\n        print(last.shape)\\n        if output_so_far.shape[1] > 1:\\n            _, past, _ = model(output_so_far[:, :-1])\\n\\n    unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\\n    unpert_last_hidden = unpert_all_hidden[-1]\\n\\n    # check if we are abowe grad max length\\n    if i >= args[\\\"grad_length\\\"]:\\n        current_stepsize = args[\\\"stepsize\\\"] * 0\\n    else:\\n        current_stepsize = args[\\\"stepsize\\\"]\\n\\n    break\\n\\n    # modify the past if necessary\\n    if not perturb or num_iterations == 0:\\n        pert_past = past\\n\\n    else:\\n        accumulated_hidden = unpert_last_hidden[:, :-1, :]\\n        accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\\n\\n        if past is not None:\\n            pert_past, _, grad_norms, loss_this_iter = perturb_past(\\n                past,\\n                model,\\n                last,\\n                unpert_past=unpert_past,\\n                unpert_logits=unpert_logits,\\n                accumulated_hidden=accumulated_hidden,\\n                grad_norms=grad_norms,\\n                stepsize=current_stepsize,\\n                one_hot_bows_vectors=one_hot_bows_vectors,\\n                classifier=classifier,\\n                class_label=class_label,\\n                loss_type=loss_type,\\n                num_iterations=num_iterations,\\n                horizon_length=horizon_length,\\n                window_length=window_length,\\n                decay=decay,\\n                gamma=gamma,\\n                kl_scale=kl_scale,\\n                device=device,\\n            )\\n            loss_in_time.append(loss_this_iter)\\n        else:\\n            pert_past = past\\n\\n    pert_logits, past, pert_all_hidden = model(last, past=pert_past)\\n    pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\\n\\n    for token_idx in set(output_so_far[0].tolist()):\\n        if pert_logits[0, token_idx] < 0:\\n            pert_logits[0, token_idx] *= repetition_penalty\\n        else:\\n            pert_logits[0, token_idx] /= repetition_penalty\\n\\n    pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n    if classifier is not None:\\n        ce_loss = torch.nn.CrossEntropyLoss()\\n        prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\\n        label = torch.tensor([class_label], device=device, dtype=torch.long)\\n        unpert_discrim_loss = ce_loss(prediction, label)\\n        print(\\\"unperturbed discrim loss\\\", unpert_discrim_loss.data.cpu().numpy())\\n    else:\\n        unpert_discrim_loss = 0\\n\\n    # Fuse the modified model and original model\\n    if perturb:\\n\\n        unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\\n\\n        pert_probs = (pert_probs ** gm_scale) * (\\n            unpert_probs ** (1 - args[\\\"gm_scale\\\"])\\n        )  # + SMALL_CONST\\n        pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)  # + SMALL_CONST\\n\\n        # rescale\\n        if torch.sum(pert_probs) <= 1:\\n            pert_probs = pert_probs / torch.sum(pert_probs)\\n\\n    else:\\n        pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\\n        pert_probs = F.softmax(pert_logits, dim=-1)\\n\\n    # sample or greedy\\n    if sample:\\n        last = torch.multinomial(pert_probs, num_samples=1)\\n\\n    else:\\n        _, last = torch.topk(pert_probs, k=1, dim=-1)\\n\\n    # update context/output_so_far appending the new token\\n    output_so_far = (\\n        last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\\n    )\\n\\n    print(tokenizer.decode(output_so_far.tolist()[0]))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if past is None and output_so_far is not None:\n",
    "    last = output_so_far[:, -1:]\n",
    "    print(last.shape)\n",
    "    if output_so_far.shape[1] > 1:\n",
    "        _, past, _ = model(output_so_far[:, :-1])\n",
    "\n",
    "unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far) # can take tensor([[50256]]) as input\n",
    "# unpert_logits: torch.Size([1, 1, 50257]), \n",
    "# unpert_past[0]: torch.Size([2, 1, 16, 1, 64])  #length: 24\n",
    "# unpert_all_hidden[0]: torch.Size([1, 1, 1024]) #length: 25\n",
    "unpert_last_hidden = unpert_all_hidden[-1] #torch.Size([1, 1, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 135;\n",
       "                var nbb_unformatted_code = \"if i >= args['grad_length']:\\n    current_stepsize = stepsize * 0\\nelse:\\n    current_stepsize = stepsize\\n\\n# modify the past if necessary\\nif not perturb or num_iterations == 0:\\n    pert_past = past\";\n",
       "                var nbb_formatted_code = \"if i >= args[\\\"grad_length\\\"]:\\n    current_stepsize = stepsize * 0\\nelse:\\n    current_stepsize = stepsize\\n\\n# modify the past if necessary\\nif not perturb or num_iterations == 0:\\n    pert_past = past\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if i >= args['grad_length']:\n",
    "    current_stepsize = args['stepsize'] * 0\n",
    "else:\n",
    "    current_stepsize = args['stepsize']\n",
    "\n",
    "# modify the past if necessary\n",
    "if not perturb or num_iterations == 0:\n",
    "    pert_past = past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 136;\n",
       "                var nbb_unformatted_code = \"if i >= args['grad_length']:\\n    current_stepsize = args['stepsize'] * 0\\nelse:\\n    current_stepsize = args['stepsize']\\n\\n# modify the past if necessary\\nif not perturb or num_iterations == 0:\\n    pert_past = past\";\n",
       "                var nbb_formatted_code = \"if i >= args[\\\"grad_length\\\"]:\\n    current_stepsize = args[\\\"stepsize\\\"] * 0\\nelse:\\n    current_stepsize = args[\\\"stepsize\\\"]\\n\\n# modify the past if necessary\\nif not perturb or num_iterations == 0:\\n    pert_past = past\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accumulated_hidden = unpert_last_hidden[:, :-1, :]\n",
    "accumulated_hidden = torch.sum(accumulated_hidden, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 143;\n",
       "                var nbb_unformatted_code = \"unpert_last_hidden[:,:-1,:]\";\n",
       "                var nbb_formatted_code = \"unpert_last_hidden[:, :-1, :]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accumulated_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perturb_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 145;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n%config IPCompleter.greedy=True\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n%config IPCompleter.greedy=True\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad_accumulator = [(np.zeros(p.shape).astype(\"float32\")) for p in past]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 152;\n",
       "                var nbb_unformatted_code = \"args['class_label'] = 'very_positive'\\nargs['window_length'] = 5\";\n",
       "                var nbb_formatted_code = \"args[\\\"class_label\\\"] = \\\"very_positive\\\"\\nargs[\\\"window_length\\\"] = 5\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if accumulated_hidden is None:\n",
    "    accumulated_hidden = 0\n",
    "\n",
    "if args['decay']:\n",
    "    decay_mask = torch.arange(0.0, 1.0 + SMALL_CONST, 1.0 / (args['window_length']))[1:]\n",
    "else:\n",
    "    decay_mask = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 155;\n",
       "                var nbb_unformatted_code = \"args = {\\n    \\\"pretrained_model\\\": \\\"gpt2-medium\\\",\\n    \\\"cond_text\\\": \\\"The lake\\\",\\n    \\\"uncond\\\": True,\\n    \\\"num_samples\\\": 1,\\n    \\\"bag_of_words\\\": \\\"military\\\",\\n    \\\"discrim\\\": \\\"sentiment\\\",\\n    \\\"discrim_weights\\\": None,\\n    \\\"discrim_meta\\\": None,\\n    \\\"class_label\\\": -1,\\n    \\\"length\\\": 100,\\n    \\\"stepsize\\\": 0.03,\\n    \\\"temperature\\\": 1.0,\\n    \\\"top_k\\\": 10,\\n    \\\"sample\\\": True,\\n    \\\"num_iterations\\\": 3,\\n    \\\"grad_length\\\": 10000,\\n    \\\"window_length\\\": 5,\\n    \\\"horizon_length\\\": 1,\\n    \\\"decay\\\": True,\\n    \\\"gamma\\\": 1.5,\\n    \\\"gm_scale\\\": 0.9,\\n    \\\"kl_scale\\\": 0.01,\\n    \\\"seed\\\": 0,\\n    \\\"colorama\\\": False,\\n    \\\"repetition_penalty\\\": 1.0,\\n}\";\n",
       "                var nbb_formatted_code = \"args = {\\n    \\\"pretrained_model\\\": \\\"gpt2-medium\\\",\\n    \\\"cond_text\\\": \\\"The lake\\\",\\n    \\\"uncond\\\": True,\\n    \\\"num_samples\\\": 1,\\n    \\\"bag_of_words\\\": \\\"military\\\",\\n    \\\"discrim\\\": \\\"sentiment\\\",\\n    \\\"discrim_weights\\\": None,\\n    \\\"discrim_meta\\\": None,\\n    \\\"class_label\\\": -1,\\n    \\\"length\\\": 100,\\n    \\\"stepsize\\\": 0.03,\\n    \\\"temperature\\\": 1.0,\\n    \\\"top_k\\\": 10,\\n    \\\"sample\\\": True,\\n    \\\"num_iterations\\\": 3,\\n    \\\"grad_length\\\": 10000,\\n    \\\"window_length\\\": 5,\\n    \\\"horizon_length\\\": 1,\\n    \\\"decay\\\": True,\\n    \\\"gamma\\\": 1.5,\\n    \\\"gm_scale\\\": 0.9,\\n    \\\"kl_scale\\\": 0.01,\\n    \\\"seed\\\": 0,\\n    \\\"colorama\\\": False,\\n    \\\"repetition_penalty\\\": 1.0,\\n}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, _, _, curr_length, _ = past[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 167;\n",
       "                var nbb_unformatted_code = \"tuple(past[0].shape[:-2]) + tuple([curr_length - window_length]) + tuple(past[0].shape[-1:])\";\n",
       "                var nbb_formatted_code = \"tuple(past[0].shape[:-2]) + tuple([curr_length - window_length]) + tuple(\\n    past[0].shape[-1:]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if curr_length > args['window_length'] and args['window_length'] > 0:\n",
    "    ones_key_val_shape = tuple(past[0].shape[:-2]) + tuple([args['window_length']]) + tuple(past[0].shape[-1:])\n",
    "    #(2, 1, 16, 5, 64)\n",
    "    zeros_key_val_shape = (\n",
    "        tuple(past[0].shape[:-2]) + tuple([curr_length - args['window_length']]) + tuple(past[0].shape[-1:])\n",
    "    )\n",
    "    #(2, 1, 16, -4, 64)\n",
    "    ones_mask = torch.ones(ones_key_val_shape)\n",
    "    ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n",
    "    ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n",
    "\n",
    "    window_mask = torch.cat((ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2).to(device)\n",
    "else:\n",
    "    window_mask = torch.ones_like(past[0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 16, -4, 64)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 168;\n",
       "                var nbb_unformatted_code = \"if curr_length > args['window_length'] and args['window_length'] > 0:\\n    ones_key_val_shape = tuple(past[0].shape[:-2]) + tuple([args['window_length']]) + tuple(past[0].shape[-1:])\\n    #(2, 1, 16, 5, 64)\\n    zeros_key_val_shape = (\\n        tuple(past[0].shape[:-2]) + tuple([curr_length - args['window_length']]) + tuple(past[0].shape[-1:])\\n    )\\n\\n    ones_mask = torch.ones(ones_key_val_shape)\\n    ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\\n    ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\\n\\n    window_mask = torch.cat((ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2).to(device)\\nelse:\\n    window_mask = torch.ones_like(past[0]).to(device)\";\n",
       "                var nbb_formatted_code = \"if curr_length > args[\\\"window_length\\\"] and args[\\\"window_length\\\"] > 0:\\n    ones_key_val_shape = (\\n        tuple(past[0].shape[:-2])\\n        + tuple([args[\\\"window_length\\\"]])\\n        + tuple(past[0].shape[-1:])\\n    )\\n    # (2, 1, 16, 5, 64)\\n    zeros_key_val_shape = (\\n        tuple(past[0].shape[:-2])\\n        + tuple([curr_length - args[\\\"window_length\\\"]])\\n        + tuple(past[0].shape[-1:])\\n    )\\n\\n    ones_mask = torch.ones(ones_key_val_shape)\\n    ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\\n    ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\\n\\n    window_mask = torch.cat((ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2).to(\\n        device\\n    )\\nelse:\\n    window_mask = torch.ones_like(past[0]).to(device)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuple(past[0].shape[:-2]) + tuple([curr_length - args['window_length']]) + tuple(past[0].shape[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 175;\n",
       "                var nbb_unformatted_code = \"loss_per_iter = []\\nnew_accumulated_hidden = None\";\n",
       "                var nbb_formatted_code = \"loss_per_iter = []\\nnew_accumulated_hidden = None\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_per_iter = []\n",
    "new_accumulated_hidden = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1\n",
      "pplm_bow_loss: 14.222659\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 260;\n",
       "                var nbb_unformatted_code = \"classifier\";\n",
       "                var nbb_formatted_code = \"classifier\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(num_iterations):\n",
    "    print(\"Iteration \", i + 1)\n",
    "    curr_perturbation = [\n",
    "        torch.from_numpy(p_).requires_grad_(True).to(device) for p_ in grad_accumulator\n",
    "    ] #torch.Size([2, 1, 16, 1, 64])\n",
    "    perturbed_past = list(map(add, past, curr_perturbation))\n",
    "    _, _, _, curr_length, _ = curr_perturbation[0].shape  # 1\n",
    "    all_logits, _, all_hidden = model(last, past=perturbed_past)\n",
    "    # torch.Size([1, 1, 50257]), \n",
    "    # torch.Size([2, 1, 16, 2, 64]), length: 24\n",
    "    # torch.Size([1, 1, 1024])\n",
    "    hidden = all_hidden[-1]\n",
    "    new_accumulated_hidden = accumulated_hidden + torch.sum(hidden, dim=1).detach()\n",
    "    logits = all_logits[:, -1, :]\n",
    "    probs = F.softmax(logits, dim = -1)\n",
    "    \n",
    "    loss = 0.0\n",
    "    loss_list = []\n",
    "    \n",
    "    if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\n",
    "        for one_hot_bow in one_hot_bows_vectors:\n",
    "            bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n",
    "            bow_loss = -torch.log(torch.sum(bow_logits))\n",
    "            loss += bow_loss\n",
    "            loss_list.append(loss)\n",
    "        print(\"pplm_bow_loss:\", loss.data.cpu().numpy())\n",
    "    \n",
    "    if loss_type == 2 or loss_type == 3:\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        curr_unpert_past = unpert_past\n",
    "        curr_probs = torch.unsqueeze(probs, dim=1)\n",
    "        wte = model.resize_token_embeddings() #Embedding(50257, 1024)\n",
    "        for _ in range(args['horizon_length']): \n",
    "            inputs_embeds = torch.matmul(curr_probs, wte.weight.data) #torch.Size([1, 1, 1024])\n",
    "            _, curr_unpert_past, curr_all_hidden = model(past = curr_unpert_past,\n",
    "                                                        inputs_embeds =inputs_embeds)\n",
    "            curr_hidden = curr_all_hidden[-1]\n",
    "            new_accumulated_hidden = new_accumulated_hidden + torch.sum(curr_hidden, dim=1) #torch.Size([1, 1024])\n",
    "        \n",
    "        prediction = classifier(new_accumulated_hidden/(curr_length + 1 + args['horizon_length']) ) #torch.Size([1, 5])\n",
    "        label = torch.tensor(prediction.shape[0]*[class_label], device=device, dtype=torch.long)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 263;\n",
       "                var nbb_unformatted_code = \"prediction.shape\";\n",
       "                var nbb_formatted_code = \"prediction.shape\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch7",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
